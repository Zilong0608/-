{
  "version": "1.0-clean-fullpages",
  "total_unique_questions": 3738,
  "categories": [
    {
      "category": "Deep Learning",
      "subcategories": [
        {
          "subcategory": "Deep Learning-General",
          "questions": [
            {
              "id": "dl_0001",
              "question": "1 什么是深度学习，它与传统机器学习有什么不同？"
            },
            {
              "id": "dl_0002",
              "question": "2 权重初始化如何影响深度学习模型的性能？"
            },
            {
              "id": "dl_0003",
              "question": "3 你能解释感知器和 S 形神经元的区别吗？"
            },
            {
              "id": "dl_0004",
              "question": "4 你能解释反向传播在神经网络中是如何工作的吗？"
            },
            {
              "id": "dl_0005",
              "question": "5 神经网络中常用的激活函数有哪些？"
            },
            {
              "id": "dl_0006",
              "question": "6 如何防止神经网络的过拟合？"
            },
            {
              "id": "dl_0007",
              "question": "7 如何决定神经网络的层数和神经元数？"
            },
            {
              "id": "dl_0008",
              "question": "8 如何处理神经网络中缺失的数据？"
            },
            {
              "id": "dl_0009",
              "question": "9 你能解释一下深度学习中迁移学习的概念吗？"
            },
            {
              "id": "dl_0010",
              "question": "10 你如何评估一个深度学习模型的性能？"
            },
            {
              "id": "dl_0011",
              "question": "11 你能解释卷积神经网络和循环神经网络的区别吗？"
            },
            {
              "id": "dl_0012",
              "question": "12 过拟合和欠拟合的区别是什么？"
            },
            {
              "id": "dl_0013",
              "question": "13 神经网络中 Dropout 的目的是什么？"
            },
            {
              "id": "dl_0014",
              "question": "14 批归一化是如何工作的？"
            },
            {
              "id": "dl_0015",
              "question": "15 激活函数在神经网络中的作用是什么？"
            },
            {
              "id": "dl_0016",
              "question": "16 深度神经网络和浅层神经网络的区别是什么？"
            },
            {
              "id": "dl_0017",
              "question": "17 梯度下降法和随机梯度下降法有什么区别？"
            },
            {
              "id": "dl_0018",
              "question": "18 什么是递归神经网络？"
            },
            {
              "id": "dl_0019",
              "question": "19 深度学习中验证集和测试集的区别是什么？"
            },
            {
              "id": "dl_0020",
              "question": "20 前馈神经网络和循环神经网络的区别是什么？"
            },
            {
              "id": "dl_0021",
              "question": "21 你能解释一下深度学习中权重初始化的概念吗？"
            },
            {
              "id": "dl_0022",
              "question": "22softmax 函数和 sigmoid 函数的区别是什么？"
            },
            {
              "id": "dl_0023",
              "question": "23 你能解释一下深度学习中集成学习的概念吗？"
            },
            {
              "id": "dl_0024",
              "question": "24 你能解释一下深度学习中权重衰减的概念吗？"
            },
            {
              "id": "dl_0025",
              "question": "25 深度学习中 L1 和 L2 正则化的区别是什么？"
            },
            {
              "id": "dl_0026",
              "question": "26 学习率在深度学习中的作用是什么？"
            },
            {
              "id": "dl_0027",
              "question": "27 你能解释一下深度学习中早停法的概念吗？"
            },
            {
              "id": "dl_0028",
              "question": "28 你能解释一下深度学习中数据增强的概念吗？"
            },
            {
              "id": "dl_0029",
              "question": "29 批归一化和层归一化有什么区别？"
            },
            {
              "id": "dl_0030",
              "question": "30 你能解释一下长短期记忆(LSTM)网络的概念吗？"
            },
            {
              "id": "dl_0031",
              "question": "31 深度学习中生成模型和判别模型的区别是什么？"
            },
            {
              "id": "dl_0032",
              "question": "32 你能解释一下生成对抗网络(GANs)的概念吗？"
            },
            {
              "id": "dl_0033",
              "question": "33 深度信念网络和深度神经网络的区别是什么？"
            },
            {
              "id": "dl_0034",
              "question": "34 你能解释一下深度学习中强化学习的概念吗？"
            },
            {
              "id": "dl_0035",
              "question": "35 你能解释一下深度学习中注意机制的概念吗？"
            },
            {
              "id": "dl_0036",
              "question": "为 什 么 需 要 做 特 征 归 一 化 、 标 准 化？"
            },
            {
              "id": "dl_0037",
              "question": "常 用 常 用 的 归 一 化 和 标 准 化 的 方 法 有 哪 些？"
            },
            {
              "id": "dl_0038",
              "question": "「 参 考 资 料 」 ： N， LN， IN， GN 都 是 什 么？"
            },
            {
              "id": "dl_0039",
              "question": "一 化 RNN（ 2015） 有 什 么 区 别？"
            },
            {
              "id": "dl_0040",
              "question": "神 经 网 络 的 深 度 和 宽 度 分 别 指 的 是 什 么？"
            },
            {
              "id": "dl_0041",
              "question": "「参考资料」：深度卷积网络中如何进行上采样？"
            },
            {
              "id": "dl_0042",
              "question": "下 采 样 的 作 用 是 什 么？"
            },
            {
              "id": "dl_0043",
              "question": "通 常 有 哪 些 方 式？"
            },
            {
              "id": "dl_0044",
              "question": "模型的参数量指的是什么？"
            },
            {
              "id": "dl_0045",
              "question": "怎么计算？"
            },
            {
              "id": "dl_0046",
              "question": "1 2 . 模 型 的 FLO P s（ 计 算 量 ） 指 的 是 什 么？"
            },
            {
              "id": "dl_0047",
              "question": "「 参 考 资 料 」 ： CNN 模 型 所 需 的 计 算 力 flops 是 什 么？"
            },
            {
              "id": "dl_0048",
              "question": "有 哪 些 经 典 的 卷 积 类 型？"
            },
            {
              "id": "dl_0049",
              "question": "激 活 函 数 是 什 么？"
            },
            {
              "id": "dl_0050",
              "question": "1 7 . 神 经 网 络 中 1×1 卷 积 有 什 么 作 用？"
            },
            {
              "id": "dl_0051",
              "question": "者 好 处 呢？"
            },
            {
              "id": "dl_0052",
              "question": "随 机 梯 度 下 降 相 比 全 局 梯 度 下 降 好 处 是 什 么？"
            },
            {
              "id": "dl_0053",
              "question": "个网络能正常训练嘛？"
            },
            {
              "id": "dl_0054",
              "question": "增 大 感 受 野 的 方 法？"
            },
            {
              "id": "dl_0055",
              "question": "神 经 网 络 的 正 则 化 方 法？"
            },
            {
              "id": "dl_0056",
              "question": "过 拟 合 的 解 决 方 法？"
            },
            {
              "id": "dl_0057",
              "question": "梯 度 消 失 和 梯 度 爆 炸 的 原 因 是 什 么？"
            },
            {
              "id": "dl_0058",
              "question": "深 度 学 习 为 什 么 在 计 算 机 视 觉 领 域 这 么 好？"
            },
            {
              "id": "dl_0059",
              "question": "2 5 . 为 什 么 神 经 网 络 种 常 用 relu 作 为 激 活 函 数？"
            },
            {
              "id": "dl_0060",
              "question": "卷 积 层 和 全 连 接 层 的 区 别 是 什 么？"
            },
            {
              "id": "dl_0061",
              "question": "2 7 . 什 么 是 正 则 化？"
            },
            {
              "id": "dl_0062",
              "question": "「 参 考 资 料 」 ： l1 正 则 与 l2 正 则 的 特 点 是 什 么 ， 各 有 什 么 优 势？"
            },
            {
              "id": "dl_0063",
              "question": "常 见 的 损 失 函 数 有 哪 些？"
            },
            {
              "id": "dl_0064",
              "question": "你 用 过 哪 些？"
            },
            {
              "id": "dl_0065",
              "question": "2 9 . dr opo ut 为 什 么 能 解 决 过 拟 合？"
            },
            {
              "id": "dl_0066",
              "question": "「 参 考 资 料 」 ： 深 度 学 习 中 的 batch 的 大 小 对 学 习 效 果 有 何 影 响？"
            },
            {
              "id": "dl_0067",
              "question": "3 1 . P yT orch 和 TensorFl ow 的 特 点 分 别 是 什 么？"
            },
            {
              "id": "dl_0068",
              "question": "3 4 . R eLU 函 数 在 0 处 不 可 导 ， 为 什 么 还 能 用？"
            },
            {
              "id": "dl_0069",
              "question": "3 6 . 为 什 么 m ax p o ol ing 要 更 常 用？"
            },
            {
              "id": "dl_0070",
              "question": "a ver ag e p o oling 比 m ax p oo l ing 更 合 适？"
            },
            {
              "id": "dl_0071",
              "question": "为 什 么 要 反 向 传 播？"
            },
            {
              "id": "dl_0072",
              "question": "神 经 网 络 的 优 缺 点？"
            },
            {
              "id": "dl_0073",
              "question": "4 1 . S oft m ax+Cr o ss E nt rop y 如 何 反 向 求 导？"
            },
            {
              "id": "dl_0074",
              "question": "有 什 么 数 据 增 强 的 方 式？"
            },
            {
              "id": "dl_0075",
              "question": "4 3 . 为 什 么 在 模 型 训 练 开 始 会 有 warm u p？"
            },
            {
              "id": "dl_0076",
              "question": "为什么有效；有什么理论解释么？"
            },
            {
              "id": "dl_0077",
              "question": "4 4 . V GG 使 用 3*3 卷 积 核 的 优 势 是 什 么？"
            },
            {
              "id": "dl_0078",
              "question": "个 模 型 无 效？"
            },
            {
              "id": "dl_0079",
              "question": "导 致 模 型 不 收 敛 的 原 因 有 哪 些？"
            },
            {
              "id": "dl_0080",
              "question": "4 7 . R elu 比 Si gm o id 的 效 果 好 在 哪 里？"
            },
            {
              "id": "dl_0081",
              "question": "5 3 . D NN 的 梯 度 是 如 何 更 新 的？"
            },
            {
              "id": "dl_0082",
              "question": "1 各个激活函数的优缺点？"
            },
            {
              "id": "dl_0083",
              "question": "2 为什么 ReLU 常用于神经网络的激活函数？"
            },
            {
              "id": "dl_0084",
              "question": "3 梯度消失和梯度爆炸的解决方案？"
            },
            {
              "id": "dl_0085",
              "question": "梯度爆炸引发的问题？"
            },
            {
              "id": "dl_0086",
              "question": "4 如何确定是否出现梯度爆炸？"
            },
            {
              "id": "dl_0087",
              "question": "5 神经网络中有哪些正则化技术？"
            },
            {
              "id": "dl_0088",
              "question": "6 批量归一化(BN) 如何实现？"
            },
            {
              "id": "dl_0089",
              "question": "7 神经网络中权值共享的理解？"
            },
            {
              "id": "dl_0090",
              "question": "8 对 fine-tuning(微调模型)的理解？"
            },
            {
              "id": "dl_0091",
              "question": "为什么要修改最后几层神经网络权值？"
            },
            {
              "id": "dl_0092",
              "question": "9 什么是 Dropout？"
            },
            {
              "id": "dl_0093",
              "question": "为什么有用？"
            },
            {
              "id": "dl_0094",
              "question": "它是如何工作的？"
            },
            {
              "id": "dl_0095",
              "question": "10 如何选择 dropout 的概率？"
            },
            {
              "id": "dl_0096",
              "question": "11 什么是 Adam？"
            },
            {
              "id": "dl_0097",
              "question": "Adam 和 SGD 之间的主要区别是什么？"
            },
            {
              "id": "dl_0098",
              "question": "12 为什么 Momentum 可以加速训练？"
            },
            {
              "id": "dl_0099",
              "question": "13 什么时候使用 Adam 和 SGD？"
            },
            {
              "id": "dl_0100",
              "question": "15 SGD 每步做什么，为什么能 online learning？"
            },
            {
              "id": "dl_0101",
              "question": "16 学习率太大(太小)时会发生什么？"
            },
            {
              "id": "dl_0102",
              "question": "如何设置学习率？"
            },
            {
              "id": "dl_0103",
              "question": "17 神经网络为什么不用拟牛顿法而是用梯度下降？"
            },
            {
              "id": "dl_0104",
              "question": "18 BN 和 Dropout 在训练和测试时的差别？"
            },
            {
              "id": "dl_0105",
              "question": "19 若网络初始化为 0 的话有什么问题？"
            },
            {
              "id": "dl_0106",
              "question": "20 sigmoid 和 softmax 的区别？"
            },
            {
              "id": "dl_0107",
              "question": "softmax 的公式？"
            },
            {
              "id": "dl_0108",
              "question": "21 改进的 softmax 损失函数有哪些？"
            },
            {
              "id": "dl_0109",
              "question": "22 深度学习调参有哪些技巧？"
            },
            {
              "id": "dl_0110",
              "question": "23 神经网络调参，要往哪些方向想？"
            },
            {
              "id": "dl_0111",
              "question": "24 深度学习训练中是否有必要使用 L1 获得稀疏解？"
            },
            {
              "id": "dl_0112",
              "question": "25 神经网络数据预处理方法有哪些？"
            },
            {
              "id": "dl_0113",
              "question": "26 如何初始化神经网络的权重？"
            },
            {
              "id": "dl_0114",
              "question": "‍神经网络怎样进行参数初始化？"
            },
            {
              "id": "dl_0115",
              "question": "27 为什么构建深度学习模型需要使用 GPU？"
            },
            {
              "id": "dl_0116",
              "question": "28 前馈神经网络(FNN),递归神经网络(RNN)和 CNN 区别？"
            },
            {
              "id": "dl_0117",
              "question": "29 神经网络可以解决哪些问题？"
            },
            {
              "id": "dl_0118",
              "question": "30 如何提高小型网络的精度？"
            },
            {
              "id": "dl_0119",
              "question": "32 什么是鞍点问题？"
            },
            {
              "id": "dl_0120",
              "question": "33 网络设计中，为什么卷积核设计尺寸都是奇数？"
            },
            {
              "id": "dl_0121",
              "question": "3 卷积层有哪些基本参数？"
            },
            {
              "id": "dl_0122",
              "question": "4 如何计算卷积层的输出的大小？"
            },
            {
              "id": "dl_0123",
              "question": "5 如何计算卷积层参数数量？"
            },
            {
              "id": "dl_0124",
              "question": "6 有哪些池化方法？"
            },
            {
              "id": "dl_0125",
              "question": "7 1*1 卷积的作用？"
            },
            {
              "id": "dl_0126",
              "question": "8 卷积层和池化层有什么区别？"
            },
            {
              "id": "dl_0127",
              "question": "9 卷积核是否一定越大越好？"
            },
            {
              "id": "dl_0128",
              "question": "10 卷积在图像中有什么直观作用？"
            },
            {
              "id": "dl_0129",
              "question": "11 CNN 中空洞卷积的作用是什么？"
            },
            {
              "id": "dl_0130",
              "question": "12 怎样才能减少卷积层参数量？"
            },
            {
              "id": "dl_0131",
              "question": "13 在进行卷积操作时，必须同时考虑通道和区域吗？"
            },
            {
              "id": "dl_0132",
              "question": "14 采用宽卷积,窄卷积的好处有什么？"
            },
            {
              "id": "dl_0133",
              "question": "16 如何提高卷积神经网络的泛化能力？"
            },
            {
              "id": "dl_0134",
              "question": "17 卷积神经网络在 NLP 与 CV 领域应用的区别？"
            },
            {
              "id": "dl_0135",
              "question": "18 全连接、局部连接、全卷积与局部卷积的区别？"
            },
            {
              "id": "dl_0136",
              "question": "19 卷积层和全连接层的区别？"
            },
            {
              "id": "dl_0137",
              "question": "20 Max pooling 如何工作？"
            },
            {
              "id": "dl_0138",
              "question": "还有其他池化技术吗？"
            },
            {
              "id": "dl_0139",
              "question": "21 卷积神经网络的优点？"
            },
            {
              "id": "dl_0140",
              "question": "为什么用小卷积核？"
            },
            {
              "id": "dl_0141",
              "question": "22 CNN 拆成 3x1 1x3 的优点？"
            },
            {
              "id": "dl_0142",
              "question": "23 BN、LN、IN、GN 和 SN 的区别？"
            },
            {
              "id": "dl_0143",
              "question": "1 RNNs 训练和传统 ANN 训练异同点？"
            },
            {
              "id": "dl_0144",
              "question": "2 为什么 RNN 训练的时候 Loss 波动很大？"
            },
            {
              "id": "dl_0145",
              "question": "3 RNN 中为什么会出现梯度消失？"
            },
            {
              "id": "dl_0146",
              "question": "4 如何解决 RNN 中的梯度消失问题？"
            },
            {
              "id": "dl_0147",
              "question": "1 LSTM 结构推导，为什么比 RNN 好？"
            },
            {
              "id": "dl_0148",
              "question": "2 为什么 LSTM 模型中既存在 sigmoid 又存在 tanh 两种激活函数，而不是选择统一一种 sigmoid 或者 tanh？"
            },
            {
              "id": "dl_0149",
              "question": "3 LSTM 中为什么经常是两层双向 LSTM？"
            },
            {
              "id": "dl_0150",
              "question": "5 LSTM、RNN、GRU 区别？"
            },
            {
              "id": "dl_0151",
              "question": "6 LSTM 是如何实现长短期记忆功能的？"
            },
            {
              "id": "dl_0152",
              "question": "1 什么是反向传播？"
            },
            {
              "id": "dl_0153",
              "question": "2 反向传播是如何工作的？"
            },
            {
              "id": "dl_0154",
              "question": "3 为什么需要反向传播？"
            },
            {
              "id": "dl_0155",
              "question": "1 神经网络中包含哪些超参数？"
            },
            {
              "id": "dl_0156",
              "question": "2 为什么要进行超参数调优？"
            },
            {
              "id": "dl_0157",
              "question": "4 极端批样本数量下，如何训练网络？"
            },
            {
              "id": "dl_0158",
              "question": "5.2 微调有哪些不同方法？"
            },
            {
              "id": "dl_0159",
              "question": "5.3 微调先冻结底层，训练顶层的原因？"
            },
            {
              "id": "dl_0160",
              "question": "5.4 不同的数据集特性下如何微调？"
            },
            {
              "id": "dl_0161",
              "question": "5.5 目标检测中使用预训练模型的优劣？"
            },
            {
              "id": "dl_0162",
              "question": "5.6 目标检测中如何从零开始训练(train from scratch)？"
            },
            {
              "id": "dl_0163",
              "question": "6 自动化超参数搜索方法有哪些？"
            },
            {
              "id": "dl_0164",
              "question": "为什么需要激活功能？"
            },
            {
              "id": "dl_0165",
              "question": "为什么 dropout 可以解决过拟合？"
            },
            {
              "id": "dl_0166",
              "question": "Dropout 在训练和测试的区别？"
            },
            {
              "id": "dl_0167",
              "question": "12 一阶优化和二阶优化的方法有哪些？"
            },
            {
              "id": "dl_0168",
              "question": "为什么不使用二阶优化？"
            },
            {
              "id": "dl_0169",
              "question": "2.2.1 AlexNet 对比 LeNet 的优势？"
            },
            {
              "id": "dl_0170",
              "question": "2.3.1 VGG 使用 2 个 3*3 卷积的优势在哪里？"
            },
            {
              "id": "dl_0171",
              "question": "2.3.2 每层卷积是否只能用一种尺寸的卷积核？"
            },
            {
              "id": "dl_0172",
              "question": "2.4.1 inception 结构能不能缓解梯度消失？"
            },
            {
              "id": "dl_0173",
              "question": "2.5.1 ResNet 为什么不用 Dropout？"
            },
            {
              "id": "dl_0174",
              "question": "2.5.2 ResNet 网络越来越深，准确率会不会提升？"
            },
            {
              "id": "dl_0175",
              "question": "2.5.3 ResNet v1 与 ResNet v2 的区别？"
            },
            {
              "id": "dl_0176",
              "question": "2.6.1 DenseNet 比 ResNet 好？"
            },
            {
              "id": "dl_0177",
              "question": "比 ResNet 更耗显存？"
            },
            {
              "id": "dl_0178",
              "question": "为什么需要卷积？"
            },
            {
              "id": "dl_0179",
              "question": "不能使用全连接层吗？"
            },
            {
              "id": "dl_0180",
              "question": "为什么降采用使用 max pooling，而分类使用 average pooling？"
            },
            {
              "id": "dl_0181",
              "question": "CNN 是否抗旋转？"
            },
            {
              "id": "dl_0182",
              "question": "如果旋转图像，CNN 的预测会怎样？"
            },
            {
              "id": "dl_0183",
              "question": "什么是数据增强？"
            },
            {
              "id": "dl_0184",
              "question": "为什么需要它们？"
            },
            {
              "id": "dl_0185",
              "question": "你知道哪种增强？"
            },
            {
              "id": "dl_0186",
              "question": "如何选择要使用的增强？"
            },
            {
              "id": "dl_0187",
              "question": "什么是迁移学习？"
            },
            {
              "id": "dl_0188",
              "question": "什么是目标检测？"
            },
            {
              "id": "dl_0189",
              "question": "你知道有哪些框架吗？"
            },
            {
              "id": "dl_0190",
              "question": "什么是对象分割？"
            },
            {
              "id": "dl_0191",
              "question": "tanh？"
            },
            {
              "id": "dl_0192",
              "question": "1各个激活函数的优缺点？"
            },
            {
              "id": "dl_0193",
              "question": "2为什么ReLU常用于神经网络的激活函数？"
            },
            {
              "id": "dl_0194",
              "question": "4如何确定是否出现梯度爆炸？"
            },
            {
              "id": "dl_0195",
              "question": "5神经网络中有哪些正则化技术？"
            },
            {
              "id": "dl_0196",
              "question": "6批量归一化(BN) 如何实现？"
            },
            {
              "id": "dl_0197",
              "question": "7神经网络中权值共享的理解？"
            },
            {
              "id": "dl_0198",
              "question": "8 对fine-tuning(微调模型)的理解？"
            },
            {
              "id": "dl_0199",
              "question": "9 什么是Dropout？"
            },
            {
              "id": "dl_0200",
              "question": "10如何选择dropout 的概率？"
            },
            {
              "id": "dl_0201",
              "question": "11什么是Adam？"
            },
            {
              "id": "dl_0202",
              "question": "Adam和SGD之间的主要区别是什么？"
            },
            {
              "id": "dl_0203",
              "question": "12为什么Momentum可以加速训练？"
            },
            {
              "id": "dl_0204",
              "question": "13什么时候使用Adam和SGD？"
            },
            {
              "id": "dl_0205",
              "question": "15 SGD每步做什么，为什么能online learning？"
            },
            {
              "id": "dl_0206",
              "question": "16学习率太大(太小)时会发生什么？"
            },
            {
              "id": "dl_0207",
              "question": "17神经网络为什么不用拟牛顿法而是用梯度下降？"
            },
            {
              "id": "dl_0208",
              "question": "18 BN和Dropout在训练和测试时的差别？"
            },
            {
              "id": "dl_0209",
              "question": "19若网络初始化为0的话有什么问题？"
            },
            {
              "id": "dl_0210",
              "question": "20 sigmoid和softmax的区别？"
            },
            {
              "id": "dl_0211",
              "question": "softmax的公式？"
            },
            {
              "id": "dl_0212",
              "question": "21改进的softmax损失函数有哪些？"
            },
            {
              "id": "dl_0213",
              "question": "22深度学习调参有哪些技巧？"
            },
            {
              "id": "dl_0214",
              "question": "23神经网络调参，要往哪些方向想？"
            },
            {
              "id": "dl_0215",
              "question": "24深度学习训练中是否有必要使用L1获得稀疏解？"
            },
            {
              "id": "dl_0216",
              "question": "25神经网络数据预处理方法有哪些？"
            },
            {
              "id": "dl_0217",
              "question": "26如何初始化神经网络的权重？"
            },
            {
              "id": "dl_0218",
              "question": "27为什么构建深度学习模型需要使用GPU？"
            },
            {
              "id": "dl_0219",
              "question": "28前馈神经网络(FNN),递归神经网络(RNN)和 CNN 区别？"
            },
            {
              "id": "dl_0220",
              "question": "29神经网络可以解决哪些问题？"
            },
            {
              "id": "dl_0221",
              "question": "30如何提高小型网络的精度？"
            },
            {
              "id": "dl_0222",
              "question": "32什么是鞍点问题？"
            },
            {
              "id": "dl_0223",
              "question": "33网络设计中，为什么卷积核设计尺寸都是奇数？"
            },
            {
              "id": "dl_0224",
              "question": "3卷积层有哪些基本参数？"
            },
            {
              "id": "dl_0225",
              "question": "4如何计算卷积层的输出的大小？"
            },
            {
              "id": "dl_0226",
              "question": "5如何计算卷积层参数数量？"
            },
            {
              "id": "dl_0227",
              "question": "6有哪些池化方法？"
            },
            {
              "id": "dl_0228",
              "question": "7 1*1卷积的作用？"
            },
            {
              "id": "dl_0229",
              "question": "8卷积层和池化层有什么区别？"
            },
            {
              "id": "dl_0230",
              "question": "9卷积核是否一定越大越好？"
            },
            {
              "id": "dl_0231",
              "question": "10卷积在图像中有什么直观作用？"
            },
            {
              "id": "dl_0232",
              "question": "11 CNN中空洞卷积的作用是什么？"
            },
            {
              "id": "dl_0233",
              "question": "12怎样才能减少卷积层参数量？"
            },
            {
              "id": "dl_0234",
              "question": "13在进行卷积操作时，必须同时考虑通道和区域吗？"
            },
            {
              "id": "dl_0235",
              "question": "14采用宽卷积,窄卷积的好处有什么？"
            },
            {
              "id": "dl_0236",
              "question": "16如何提高卷积神经网络的泛化能力？"
            },
            {
              "id": "dl_0237",
              "question": "17卷积神经网络在NLP与CV领域应用的区别？"
            },
            {
              "id": "dl_0238",
              "question": "18全连接、局部连接、全卷积与局部卷积的区别？"
            },
            {
              "id": "dl_0239",
              "question": "19卷积层和全连接层的区别？"
            },
            {
              "id": "dl_0240",
              "question": "20 Max pooling如何工作？"
            },
            {
              "id": "dl_0241",
              "question": "21卷积神经网络的优点？"
            },
            {
              "id": "dl_0242",
              "question": "22 CNN拆成3x1 1x3的优点？"
            },
            {
              "id": "dl_0243",
              "question": "23 BN、LN、IN、GN和SN的区别？"
            },
            {
              "id": "dl_0244",
              "question": "1 RNNs训练和传统ANN训练异同点？"
            },
            {
              "id": "dl_0245",
              "question": "2 为什么RNN 训练的时候Loss波动很大？"
            },
            {
              "id": "dl_0246",
              "question": "3 RNN中为什么会出现梯度消失？"
            },
            {
              "id": "dl_0247",
              "question": "4如何解决RNN中的梯度消失问题？"
            },
            {
              "id": "dl_0248",
              "question": "1 LSTM结构推导，为什么比RNN好？"
            },
            {
              "id": "dl_0249",
              "question": "2为什么LSTM模型中既存在sigmoid又存在tanh两种激活函数，而不是选择统一一种sigmoid或者tanh？"
            },
            {
              "id": "dl_0250",
              "question": "3 LSTM中为什么经常是两层双向LSTM？"
            },
            {
              "id": "dl_0251",
              "question": "5 LSTM、RNN、GRU区别？"
            },
            {
              "id": "dl_0252",
              "question": "6 LSTM是如何实现长短期记忆功能的？"
            },
            {
              "id": "dl_0253",
              "question": "2反向传播是如何工作的？"
            },
            {
              "id": "dl_0254",
              "question": "1神经网络中包含哪些超参数？"
            },
            {
              "id": "dl_0255",
              "question": "2为什么要进行超参数调优？"
            },
            {
              "id": "dl_0256",
              "question": "4极端批样本数量下，如何训练网络？"
            },
            {
              "id": "dl_0257",
              "question": "5.2微调有哪些不同方法？"
            },
            {
              "id": "dl_0258",
              "question": "5.4不同的数据集特性下如何微调？"
            },
            {
              "id": "dl_0259",
              "question": "5.5目标检测中使用预训练模型的优劣？"
            },
            {
              "id": "dl_0260",
              "question": "5.6目标检测中如何从零开始训练(train from scratch)？"
            },
            {
              "id": "dl_0261",
              "question": "6自动化超参数搜索方法有哪些？"
            },
            {
              "id": "dl_0262",
              "question": "为什么dropout可以解决过拟合？"
            },
            {
              "id": "dl_0263",
              "question": "12一阶优化和二阶优化的方法有哪些？"
            },
            {
              "id": "dl_0264",
              "question": "2.2.1 AlexNet 对比LeNet 的优势？"
            },
            {
              "id": "dl_0265",
              "question": "2.3.1 VGG使用2个3*3卷积的优势在哪里？"
            },
            {
              "id": "dl_0266",
              "question": "2.3.2每层卷积是否只能用一种尺寸的卷积核？"
            },
            {
              "id": "dl_0267",
              "question": "2.4.1 inception结构能不能缓解梯度消失？"
            },
            {
              "id": "dl_0268",
              "question": "2.5.1 ResNet为什么不用Dropout？"
            },
            {
              "id": "dl_0269",
              "question": "2.5.2 ResNet网络越来越深，准确率会不会提升？"
            },
            {
              "id": "dl_0270",
              "question": "2.5.3 ResNet v1 与 ResNet v2的区别？"
            },
            {
              "id": "dl_0271",
              "question": "DenseNet直接连接不同层的特征图，而不是像ResNet一样element-wise sum。2.2.6.2为什么 DenseNet 比 ResNet 更耗显存？"
            },
            {
              "id": "dl_0272",
              "question": "为什么降采用使用max pooling，而分类使用average pooling？"
            },
            {
              "id": "dl_0273",
              "question": "CNN是否抗旋转？"
            },
            {
              "id": "dl_0274",
              "question": "如果旋转图像，CNN的预测会怎样？"
            }
          ]
        }
      ]
    },
    {
      "category": "General AI",
      "subcategories": [
        {
          "subcategory": "General AI-General",
          "questions": [
            {
              "id": "gen_0001",
              "question": "1 【演变史】one-hot 存在问题？"
            },
            {
              "id": "gen_0002",
              "question": "2【演变史】wordvec 存在问题？"
            },
            {
              "id": "gen_0003",
              "question": "3【演变史】fastText 存在问题？"
            },
            {
              "id": "gen_0004",
              "question": "4【演变史】elmo 存在问题？"
            },
            {
              "id": "gen_0005",
              "question": "1.1【BERT】Bert 是什么？"
            },
            {
              "id": "gen_0006",
              "question": "1.2【BERT】Bert 三个关键点？"
            },
            {
              "id": "gen_0007",
              "question": "2.1 【BERT】Bert 输入输出表征长啥样？"
            },
            {
              "id": "gen_0008",
              "question": "3.2.1 【BERT】 Bert 为什么需要预训练任务 Masked LM？"
            },
            {
              "id": "gen_0009",
              "question": "3.2.2 【BERT】 Bert 预训练任务 Masked LM 怎么做？"
            },
            {
              "id": "gen_0010",
              "question": "3.2.3 【BERT】 Bert 预训练任务 Masked LM 存在问题？"
            },
            {
              "id": "gen_0011",
              "question": "解释：在微调期间从未看到[MASK]词块？"
            },
            {
              "id": "gen_0012",
              "question": "3.2.4 【BERT】 预训练和微调之间的不匹配的解决方法？"
            },
            {
              "id": "gen_0013",
              "question": "3.3.1 【BERT】Bert 为什么需要预训练任务 Next Sentence Prediction？"
            },
            {
              "id": "gen_0014",
              "question": "3.3.2 【BERT】 Bert 预训练任务 Next Sentence Prediction 怎么做？"
            },
            {
              "id": "gen_0015",
              "question": "4 【BERT】 fine-turning 篇？"
            },
            {
              "id": "gen_0016",
              "question": "4.1 【BERT】为什么 Bert 需要 fine-turning？"
            },
            {
              "id": "gen_0017",
              "question": "4.2 【BERT】 Bert 如何 fine-turning？"
            },
            {
              "id": "gen_0018",
              "question": "5 【BERT】 Bert 损失函数篇？"
            },
            {
              "id": "gen_0019",
              "question": "5.1 【BERT】BERT的两个预训练任务对应的损失函数是什么(用公式形式展示)？"
            },
            {
              "id": "gen_0020",
              "question": "三、 对比篇？"
            },
            {
              "id": "gen_0021",
              "question": "1 【对比】多义词问题是什么？"
            },
            {
              "id": "gen_0022",
              "question": "问题：什么是多义词？"
            },
            {
              "id": "gen_0023",
              "question": "2 【对比】word2vec 为什么解决不了多义词问题？"
            },
            {
              "id": "gen_0024",
              "question": "3 【对比】GPT和BERT有什么不同？"
            },
            {
              "id": "gen_0025",
              "question": "GPT是单向的：然后体现？"
            },
            {
              "id": "gen_0026",
              "question": "4 【对比】为什么 elmo、GPT、Bert能够解决多义词问题？"
            },
            {
              "id": "gen_0027",
              "question": "2 【BERT】Bert 预训练任务？"
            },
            {
              "id": "gen_0028",
              "question": "2.2 【MLM】Bert 预训练任务 Masked LM 怎么做？"
            },
            {
              "id": "gen_0029",
              "question": "2.3【NSP】Bert 预训练任务 Next Sentence Prediction 怎么做？"
            },
            {
              "id": "gen_0030",
              "question": "义知识。那么如果持续地学习各类任务，模型的效果能否进一步提升？"
            },
            {
              "id": "gen_0031",
              "question": "具体的三种类型的无监督训练任务是哪三种呢？"
            },
            {
              "id": "gen_0032",
              "question": "每种里面又包括什么任务呢？"
            },
            {
              "id": "gen_0033",
              "question": "3 GPT 处理的 有监督任务有哪些？"
            },
            {
              "id": "gen_0034",
              "question": "的方格就是不能看到的信息，白色的就是需要attention的信息。如何实现这种控制呢？"
            },
            {
              "id": "gen_0035",
              "question": "输入：下面是一则什么新闻？"
            },
            {
              "id": "gen_0036",
              "question": "输入：阅读理解：特朗普与拜登共同竞选下一任美国总统。根据上述信息回答问题：特朗普是哪国人？"
            },
            {
              "id": "gen_0037",
              "question": "1.1 什么是低秩因式分解？"
            },
            {
              "id": "gen_0038",
              "question": "1.2 什么是跨层参数共享？"
            },
            {
              "id": "gen_0039",
              "question": "1.3 ALBERT 所所用的方法？"
            },
            {
              "id": "gen_0040",
              "question": "论文地址：https://openreview.net/forum？"
            },
            {
              "id": "gen_0041",
              "question": "2.1 什么是蒸馏？"
            },
            {
              "id": "gen_0042",
              "question": "2.2 使用 模型蒸馏 的论文？"
            },
            {
              "id": "gen_0043",
              "question": "3.1 什么是量化？"
            },
            {
              "id": "gen_0044",
              "question": "4.1 什么是剪枝？"
            },
            {
              "id": "gen_0045",
              "question": "四、模型压缩存在问题？"
            },
            {
              "id": "gen_0046",
              "question": "# tokens: [CLS] is this jack ##son ##ville？"
            },
            {
              "id": "gen_0047",
              "question": "可以组合贷吗？"
            },
            {
              "id": "gen_0048",
              "question": "说明：？"
            },
            {
              "id": "gen_0049",
              "question": "6 初期标注数据不足问题？"
            },
            {
              "id": "gen_0050",
              "question": "作为算法工程师，在项目初期，标注数据不足问题，永远是一个老大难问题，那么如何解决该问题呢？"
            },
            {
              "id": "gen_0051",
              "question": "是离群点，过分关注不是错上加错了吗？"
            },
            {
              "id": "gen_0052",
              "question": "为什么会出现该问题：？"
            },
            {
              "id": "gen_0053",
              "question": "我们是否可以采取一个轻量级的模型，比如TextCNN，去逼近BERT的效果呢？"
            },
            {
              "id": "gen_0054",
              "question": "不过也许有的读者会问，为什么不直接蒸馏为一个浅层BERT呢？"
            },
            {
              "id": "gen_0055",
              "question": "是否较大？"
            },
            {
              "id": "gen_0056",
              "question": "1 刷分的奇技淫巧？"
            },
            {
              "id": "gen_0057",
              "question": "参考：在文本分类任务中，有哪些论文中很少提及却对性能有重要影响的tricks？"
            },
            {
              "id": "gen_0058",
              "question": "如何解决NLP分类任务的11个关键问题：类别不平衡&低耗时计算&小样本&鲁棒性&测试检验&长？"
            },
            {
              "id": "gen_0059",
              "question": "1 什么是概率图模型？"
            },
            {
              "id": "gen_0060",
              "question": "2 什么是 随机场？"
            },
            {
              "id": "gen_0061",
              "question": "1 什么是 马尔可夫过程？"
            },
            {
              "id": "gen_0062",
              "question": "2 马尔可夫过程 的核心思想 是什么？"
            },
            {
              "id": "gen_0063",
              "question": "1.1 隐马尔科夫算法 是什么？"
            },
            {
              "id": "gen_0064",
              "question": "1.2 隐马尔科夫算法 中 两个序列 是什么？"
            },
            {
              "id": "gen_0065",
              "question": "1.3 隐马尔科夫算法 中 三个矩阵 是什么？"
            },
            {
              "id": "gen_0066",
              "question": "1.4 隐马尔科夫算法 中 两个假设 是什么？"
            },
            {
              "id": "gen_0067",
              "question": "1.5 隐马尔科夫算法 中 工作流程 是什么？"
            },
            {
              "id": "gen_0068",
              "question": "2.1 隐马尔科夫算法 序列概率计算过程 是什么样的？"
            },
            {
              "id": "gen_0069",
              "question": "如何对一条序列计算其整体的概率。即目标是计算出 $P(O | \\lambda)$ ;？"
            },
            {
              "id": "gen_0070",
              "question": "2.2 隐马尔科夫算法 学习训练过程 是什么样的？"
            },
            {
              "id": "gen_0071",
              "question": "2.3 隐马尔科夫算法 序列标注（解码）过程 是什么样的？"
            },
            {
              "id": "gen_0072",
              "question": "3 HMM模型三个基本问题的联系？"
            },
            {
              "id": "gen_0073",
              "question": "1.1 HMM 存在 什么问题？"
            },
            {
              "id": "gen_0074",
              "question": "2.1 最大熵马尔科夫模型（MEMM） 是什么样？"
            },
            {
              "id": "gen_0075",
              "question": "请务必注意，理解判别模型和定义特征两部分含义，这已经涉及到CRF的雏形了。？"
            },
            {
              "id": "gen_0076",
              "question": "2.2 最大熵马尔科夫模型（MEMM） 如何解决 HMM 问题？"
            },
            {
              "id": "gen_0077",
              "question": "但是得到的最优的状态转换路径是1->1->1->1，为什么呢？"
            },
            {
              "id": "gen_0078",
              "question": "1.1 HMM 和 MEMM 存在什么问题？"
            },
            {
              "id": "gen_0079",
              "question": "2.1 什么是 CRF？"
            },
            {
              "id": "gen_0080",
              "question": "2.2 CRF 的 主要思想是什么？"
            },
            {
              "id": "gen_0081",
              "question": "2.3 CRF 的定义是什么？"
            },
            {
              "id": "gen_0082",
              "question": "2.4 CRF 的 三个基本问题 是什么？"
            },
            {
              "id": "gen_0083",
              "question": "2.5 CRF 的 流程是什么？"
            },
            {
              "id": "gen_0084",
              "question": "3.1 CRF 的 优点在哪里？"
            },
            {
              "id": "gen_0085",
              "question": "3.2 CRF 的 缺点在哪里？"
            },
            {
              "id": "gen_0086",
              "question": "4 CRF 复现？"
            },
            {
              "id": "gen_0087",
              "question": "1 CRF模型 和 HMM 和 MEMM 模型 区别？"
            },
            {
              "id": "gen_0088",
              "question": "2 为什么 CRF模型 会比 HMM 被普遍使用？"
            },
            {
              "id": "gen_0089",
              "question": "如何轻松愉快地理解条件随机场（CRF）？"
            },
            {
              "id": "gen_0090",
              "question": "1 命名实体识别 评价指标 是什么？"
            },
            {
              "id": "gen_0091",
              "question": "1 基于规则的命名实体识别方法是什么？"
            },
            {
              "id": "gen_0092",
              "question": "2 基于无监督学习的命名实体识别方法是什么？"
            },
            {
              "id": "gen_0093",
              "question": "3 基于特征的监督学习的命名实体识别方法是什么？"
            },
            {
              "id": "gen_0094",
              "question": "体识别方法的优点？"
            },
            {
              "id": "gen_0095",
              "question": "2 基于深度学习的命名实体识别方法 的 结构是怎么样？"
            },
            {
              "id": "gen_0096",
              "question": "3 分布式输入层 是什么，有哪些方法？"
            },
            {
              "id": "gen_0097",
              "question": "4.1.1 什么是 BiLSTM-CRF？"
            },
            {
              "id": "gen_0098",
              "question": "4.1.2 为什么要用 BiLSTM？"
            },
            {
              "id": "gen_0099",
              "question": "4.2.1 什么是 Dilated CNN？"
            },
            {
              "id": "gen_0100",
              "question": "4.2.2 为什么会有 Dilated CNN？"
            },
            {
              "id": "gen_0101",
              "question": "4.2.3 Dilated CNN 的优点？"
            },
            {
              "id": "gen_0102",
              "question": "5.1 标签解码器是什么？"
            },
            {
              "id": "gen_0103",
              "question": "5.2 MLP+softmax层 介绍？"
            },
            {
              "id": "gen_0104",
              "question": "5.3 条件随机场CRF层 介绍？"
            },
            {
              "id": "gen_0105",
              "question": "5.4 循环神经网络RNN层 介绍？"
            },
            {
              "id": "gen_0106",
              "question": "5.3 指针网路层 介绍？"
            },
            {
              "id": "gen_0107",
              "question": "1 CNN-CRF vs BiLSTM-CRF vs IDCNN-CRF？"
            },
            {
              "id": "gen_0108",
              "question": "如何解决 CNN-CRF 问题：通过堆叠不同扩张大小的空洞卷积层，有效扩展输入宽度的大小，？"
            },
            {
              "id": "gen_0109",
              "question": "如何解决 BiLSTM-CRF 问题：因为 IDCNN 本身是一个 CNN ，所以可以像 CNN 一样进行并？"
            },
            {
              "id": "gen_0110",
              "question": "2 为什么 DNN 后面要加 CRF？"
            },
            {
              "id": "gen_0111",
              "question": "3 CRF in TensorFlow V.S. CRF in discrete toolkit？"
            },
            {
              "id": "gen_0112",
              "question": "解释：所有导致有的同学认为LSTM+CRF中其实并没有实际意义的CRF。其实按刚才说的，Hi？"
            },
            {
              "id": "gen_0113",
              "question": "1 为什么会有 Elmo？"
            },
            {
              "id": "gen_0114",
              "question": "1 Elmo 的 特点？"
            },
            {
              "id": "gen_0115",
              "question": "2 Elmo 的 思想是什么？"
            },
            {
              "id": "gen_0116",
              "question": "1 Elmo 存在的问题是什么？"
            },
            {
              "id": "gen_0117",
              "question": "1 传统的相似度算法所存在的问题？"
            },
            {
              "id": "gen_0118",
              "question": "1 什么是 Faiss？"
            },
            {
              "id": "gen_0119",
              "question": "2 Faiss 如何使用？"
            },
            {
              "id": "gen_0120",
              "question": "1 Faiss 如何安装？"
            },
            {
              "id": "gen_0121",
              "question": "2 Faiss 的索引Index有哪些？"
            },
            {
              "id": "gen_0122",
              "question": "为什么要创建索引？"
            },
            {
              "id": "gen_0123",
              "question": "3 Faiss 的索引Index都怎么用？"
            },
            {
              "id": "gen_0124",
              "question": "的时候，怎么办呢？"
            },
            {
              "id": "gen_0125",
              "question": "4 Faiss 然后使用 GPU？"
            },
            {
              "id": "gen_0126",
              "question": "query = \"睡前练瑜伽好吗睡觉之前练习40分钟的瑜伽好吗、能起到瘦身的作用吗？"
            },
            {
              "id": "gen_0127",
              "question": "1 问答系统的动机？"
            },
            {
              "id": "gen_0128",
              "question": "场景：假设 你有 一个 标准的问题库，此时 有一个 新 query 进来，你会做什么操作？"
            },
            {
              "id": "gen_0129",
              "question": "2 问答系统 是什么？"
            },
            {
              "id": "gen_0130",
              "question": "1 FAQ 检索式问答系统 是 什么？"
            },
            {
              "id": "gen_0131",
              "question": "2 query 匹配标准 QA 的核心是什么？"
            },
            {
              "id": "gen_0132",
              "question": "1 常用 方案有哪些？"
            },
            {
              "id": "gen_0133",
              "question": "2 为什么 QQ 匹配比较常用？"
            },
            {
              "id": "gen_0134",
              "question": "2.1 QQ 匹配的优点有哪些？"
            },
            {
              "id": "gen_0135",
              "question": "原因 5：QQ 匹配的上线运行速度是什么？"
            },
            {
              "id": "gen_0136",
              "question": "2.2 QQ 匹配的语义空间是什么？"
            },
            {
              "id": "gen_0137",
              "question": "2.3 QQ 匹配的语料的稳定性是什么？"
            },
            {
              "id": "gen_0138",
              "question": "2.4 QQ 匹配的业务回答与算法模型的解耦是什么？"
            },
            {
              "id": "gen_0139",
              "question": "2.5 QQ 匹配的新问题发现与去重是什么？"
            },
            {
              "id": "gen_0140",
              "question": "2.6 QQ 匹配的上线运行速度是什么？"
            },
            {
              "id": "gen_0141",
              "question": "3 QQ 匹配一般处理流程是怎么样？"
            },
            {
              "id": "gen_0142",
              "question": "1 如何发现 FAQ 中标准问题？"
            },
            {
              "id": "gen_0143",
              "question": "2 FAQ 如何做拆分？"
            },
            {
              "id": "gen_0144",
              "question": "3 FAQ 如何做合并？"
            },
            {
              "id": "gen_0145",
              "question": "4 FAQ 标准库如何实时更新？"
            },
            {
              "id": "gen_0146",
              "question": "1 word-level Model 是什么？"
            },
            {
              "id": "gen_0147",
              "question": "2 word-level Model 存在什么问题？"
            },
            {
              "id": "gen_0148",
              "question": "3 Character-Level Model 是什么？"
            },
            {
              "id": "gen_0149",
              "question": "4 Character-Level Model 优点？"
            },
            {
              "id": "gen_0150",
              "question": "5 Character-Level Model 存在问题？"
            },
            {
              "id": "gen_0151",
              "question": "6 Character-Level Model 问题的解决方法？"
            },
            {
              "id": "gen_0152",
              "question": "那么，我们可不可以采取类似于上面的subword的思路来产生更好的word embedding呢？"
            },
            {
              "id": "gen_0153",
              "question": "2 fastText 是什么？"
            },
            {
              "id": "gen_0154",
              "question": "3 fastText 的结构是什么样？"
            },
            {
              "id": "gen_0155",
              "question": "information)？"
            },
            {
              "id": "gen_0156",
              "question": "6 fastText 词内的n-gram信息 的 训练过程？"
            },
            {
              "id": "gen_0157",
              "question": "7 fastText 词内的n-gram信息 存在问题？"
            },
            {
              "id": "gen_0158",
              "question": "1 为什么要用 层次化Softmax回归(Hierarchical Softmax)？"
            },
            {
              "id": "gen_0159",
              "question": "2 层次化Softmax回归(Hierarchical Softmax) 的思想是什么？"
            },
            {
              "id": "gen_0160",
              "question": "3 层次化Softmax回归(Hierarchical Softmax) 的步骤？"
            },
            {
              "id": "gen_0161",
              "question": "四、fastText 存在问题？"
            },
            {
              "id": "gen_0162",
              "question": "如何构造每个逻辑回归单元的输入？"
            },
            {
              "id": "gen_0163",
              "question": "如何建立这棵用于判断的树形结构？"
            },
            {
              "id": "gen_0164",
              "question": "1 什么是 LoRA？"
            },
            {
              "id": "gen_0165",
              "question": "2 LoRA 的思路是什么？"
            },
            {
              "id": "gen_0166",
              "question": "3 LoRA 的特点是什么？"
            },
            {
              "id": "gen_0167",
              "question": "4 简单描述一下 LoRA？"
            },
            {
              "id": "gen_0168",
              "question": "1 QLoRA 的思路是怎么样的？"
            },
            {
              "id": "gen_0169",
              "question": "2 QLoRA 的特点是什么？"
            },
            {
              "id": "gen_0170",
              "question": "1 AdaLoRA 的思路是怎么样的？"
            },
            {
              "id": "gen_0171",
              "question": "四、LoRA权重是否可以合入原模型？"
            },
            {
              "id": "gen_0172",
              "question": "五、ChatGLM-6B LoRA后的权重多大？"
            },
            {
              "id": "gen_0173",
              "question": "六、LoRA 微调优点是什么？"
            },
            {
              "id": "gen_0174",
              "question": "七、LoRA微调方法为啥能加速训练？"
            },
            {
              "id": "gen_0175",
              "question": "八、如何在已有LoRA模型上继续训练？"
            },
            {
              "id": "gen_0176",
              "question": "九、LoRA 缺点是什么？"
            },
            {
              "id": "gen_0177",
              "question": "十、LoRA这种微调方法和全参数比起来有什么劣势吗？"
            },
            {
              "id": "gen_0178",
              "question": "十一、LORA应该作用于Transformer的哪个参数矩阵？"
            },
            {
              "id": "gen_0179",
              "question": "十二、LoRA 微调参数量怎么确定？"
            },
            {
              "id": "gen_0180",
              "question": "十三、Rank 如何选取？"
            },
            {
              "id": "gen_0181",
              "question": "十四、alpha参数 如何选取？"
            },
            {
              "id": "gen_0182",
              "question": "十五、LoRA 高效微调 如何避免过拟合？"
            },
            {
              "id": "gen_0183",
              "question": "十六、微调大模型时, 优化器如何？"
            },
            {
              "id": "gen_0184",
              "question": "十七、哪些因素会影响内存使用？"
            },
            {
              "id": "gen_0185",
              "question": "十八、LoRA权重是否可以合并？"
            },
            {
              "id": "gen_0186",
              "question": "十九、是否可以逐层调整LoRA的最优rank？"
            },
            {
              "id": "gen_0187",
              "question": "二十、Lora的矩阵怎么初始化？"
            },
            {
              "id": "gen_0188",
              "question": "为什么要初始化为全0？"
            },
            {
              "id": "gen_0189",
              "question": "LoRA 微调计算可训练参数的比例 如何确定？"
            },
            {
              "id": "gen_0190",
              "question": "LoRA 微调结果如何保存？"
            },
            {
              "id": "gen_0191",
              "question": "上继续训练呢，还是跟base 模型合并后再套一层lora，或者从头开始训练一个lora？"
            },
            {
              "id": "gen_0192",
              "question": "2 Neo4J 怎么下载？"
            },
            {
              "id": "gen_0193",
              "question": "3 Neo4J 怎么安装？"
            },
            {
              "id": "gen_0194",
              "question": "5 Cypher查询语言是什么？"
            },
            {
              "id": "gen_0195",
              "question": "2 Neo4j 怎么创建节点？"
            },
            {
              "id": "gen_0196",
              "question": "3 Neo4j 怎么创建关系？"
            },
            {
              "id": "gen_0197",
              "question": "4 Neo4j 怎么创建 出生地关系？"
            },
            {
              "id": "gen_0198",
              "question": "5 Neo4j 怎么查询？"
            },
            {
              "id": "gen_0199",
              "question": "6 Neo4j 怎么删除和修改？"
            },
            {
              "id": "gen_0200",
              "question": "三、如何利用 Python 操作 Neo4j 图数据库？"
            },
            {
              "id": "gen_0201",
              "question": "1 neo4j模块：执行CQL ( cypher ) 语句是什么？"
            },
            {
              "id": "gen_0202",
              "question": "2 py2neo模块是什么？"
            },
            {
              "id": "gen_0203",
              "question": "2 检索的方法 的 训练阶段 如何做？"
            },
            {
              "id": "gen_0204",
              "question": "3 检索的方法 的 预测阶段 如何做？"
            },
            {
              "id": "gen_0205",
              "question": "那有什么方法可以解决该问题么？"
            },
            {
              "id": "gen_0206",
              "question": "1 分类任务有哪些类别？"
            },
            {
              "id": "gen_0207",
              "question": "它们都有什么特征？"
            },
            {
              "id": "gen_0208",
              "question": "2 文本分类任务相较于其他领域的分类任务有何不同之处？"
            },
            {
              "id": "gen_0209",
              "question": "3 文本分类任务和文本领域的其他任务相比有何不同之处？"
            },
            {
              "id": "gen_0210",
              "question": "4 文本分类的过程？"
            },
            {
              "id": "gen_0211",
              "question": "1 文本分类任务的数据预处理方法有哪些？"
            },
            {
              "id": "gen_0212",
              "question": "2 你使用过哪些分词方法和工具？"
            },
            {
              "id": "gen_0213",
              "question": "3 中文文本分词的方法？"
            },
            {
              "id": "gen_0214",
              "question": "4 基于字符串匹配的分词方法的原理 是什么？"
            },
            {
              "id": "gen_0215",
              "question": "5 统计语言模型如何应用于分词？"
            },
            {
              "id": "gen_0216",
              "question": "N-gram最大概率分词？"
            },
            {
              "id": "gen_0217",
              "question": "6 基于序列标注的分词方法 是什么？"
            },
            {
              "id": "gen_0218",
              "question": "7 基于(Bi-)LSTM的词性标注 是什么？"
            },
            {
              "id": "gen_0219",
              "question": "8 词干提取和词形还原有什么区别？"
            },
            {
              "id": "gen_0220",
              "question": "1 （一个具体的）文本分类任务可以使用哪些特征？"
            },
            {
              "id": "gen_0221",
              "question": "3 能不能简单介绍下词袋模型？"
            },
            {
              "id": "gen_0222",
              "question": "4.1 什么是n元语法？"
            },
            {
              "id": "gen_0223",
              "question": "为什么要用n-gram？"
            },
            {
              "id": "gen_0224",
              "question": "4.2 n-gram算法的局限性是什么？"
            },
            {
              "id": "gen_0225",
              "question": "5.1 介绍一下主题建模任务？"
            },
            {
              "id": "gen_0226",
              "question": "5.3 TF-IDF算法是做什么的？"
            },
            {
              "id": "gen_0227",
              "question": "5.4 tf-idf高意味着什么？"
            },
            {
              "id": "gen_0228",
              "question": "6.1 如何计算两段文本之间的距离？"
            },
            {
              "id": "gen_0229",
              "question": "6.2 什么是jaccard距离？"
            },
            {
              "id": "gen_0230",
              "question": "6.3 Dice系数和Jaccard系数的区别？"
            },
            {
              "id": "gen_0231",
              "question": "6.4 同样是编辑距离，莱文斯坦距离和汉明距离的区别在哪里？"
            },
            {
              "id": "gen_0232",
              "question": "6.5 写一下计算编辑距离（莱温斯坦距离）的编程题吧？"
            },
            {
              "id": "gen_0233",
              "question": "1.1 fastText的分类过程？"
            },
            {
              "id": "gen_0234",
              "question": "1.2 fastText的优点？"
            },
            {
              "id": "gen_0235",
              "question": "2.1 TextCNN进行文本分类的过程？"
            },
            {
              "id": "gen_0236",
              "question": "2.2 TextCNN可以调整哪些参数？"
            },
            {
              "id": "gen_0237",
              "question": "2.3 使用CNN作为文本分类器时，不同通道channels对应着文本的什么信息？"
            },
            {
              "id": "gen_0238",
              "question": "2.4 TextCNN中卷积核的长与宽代表了什么？"
            },
            {
              "id": "gen_0239",
              "question": "2.5 在TextCNN中的pooling操作与一般CNN的pooling操作有何不同？"
            },
            {
              "id": "gen_0240",
              "question": "2.6 TextCNN的局限性？"
            },
            {
              "id": "gen_0241",
              "question": "3.1 如何解决长文本分类任务？"
            },
            {
              "id": "gen_0242",
              "question": "3.2 简单介绍DPCNN模型相较于TextCNN的改进？"
            },
            {
              "id": "gen_0243",
              "question": "4.1 简要介绍TextRCNN相较于TextCNN的改进？"
            },
            {
              "id": "gen_0244",
              "question": "意力机制如何应用于文本分类领域？"
            },
            {
              "id": "gen_0245",
              "question": "6.1 GNN 图神经网络如何应用于文本分类领域？"
            },
            {
              "id": "gen_0246",
              "question": "7.1 基于Transformer的预训练模型如何应用于文本分类领域？"
            },
            {
              "id": "gen_0247",
              "question": "8.1 你了解哪些预训练模型？"
            },
            {
              "id": "gen_0248",
              "question": "它们的特点是什么？"
            },
            {
              "id": "gen_0249",
              "question": "1.1 二分类问题使用的激活函数sigmoid简介？"
            },
            {
              "id": "gen_0250",
              "question": "1.2 Sigmod的缺点是什么？"
            },
            {
              "id": "gen_0251",
              "question": "2.1 softmax函数是什么？"
            },
            {
              "id": "gen_0252",
              "question": "2.2 softmax函数怎么求导？"
            },
            {
              "id": "gen_0253",
              "question": "3 分类问题使用的损失函数还有有哪些？"
            },
            {
              "id": "gen_0254",
              "question": "1 文本分类任务使用的评估算法和指标有哪些？"
            },
            {
              "id": "gen_0255",
              "question": "2 简单介绍混淆矩阵和kappa？"
            },
            {
              "id": "gen_0256",
              "question": "1 为什么有 one-hot？"
            },
            {
              "id": "gen_0257",
              "question": "2 one-hot 是什么？"
            },
            {
              "id": "gen_0258",
              "question": "3 one-hot 有什么特点？"
            },
            {
              "id": "gen_0259",
              "question": "4 one-hot 存在哪些问题？"
            },
            {
              "id": "gen_0260",
              "question": "1 什么是 TF-IDF？"
            },
            {
              "id": "gen_0261",
              "question": "2 TF-IDF 如何评估词的重要程度？"
            },
            {
              "id": "gen_0262",
              "question": "3 TF-IDF 的思想是什么？"
            },
            {
              "id": "gen_0263",
              "question": "4 TF-IDF 的计算公式是什么？"
            },
            {
              "id": "gen_0264",
              "question": "5 TF-IDF 怎么描述？"
            },
            {
              "id": "gen_0265",
              "question": "6 TF-IDF 的优点是什么？"
            },
            {
              "id": "gen_0266",
              "question": "7 TF-IDF 的缺点是什么？"
            },
            {
              "id": "gen_0267",
              "question": "8 TF-IDF 的应用？"
            },
            {
              "id": "gen_0268",
              "question": "1 Wordvec 指什么？"
            },
            {
              "id": "gen_0269",
              "question": "2 Wordvec 中 CBOW 指什么？"
            },
            {
              "id": "gen_0270",
              "question": "3 Wordvec 中 Skip-gram 指什么？"
            },
            {
              "id": "gen_0271",
              "question": "4 CBOW vs Skip-gram 哪一个好？"
            },
            {
              "id": "gen_0272",
              "question": "那问题来了？"
            },
            {
              "id": "gen_0273",
              "question": "最后 哪个学生 成绩 会更好？"
            },
            {
              "id": "gen_0274",
              "question": "1 Word2vec 中 霍夫曼树 是什么？"
            },
            {
              "id": "gen_0275",
              "question": "2 Word2vec 中 为什么要使用 霍夫曼树？"
            },
            {
              "id": "gen_0276",
              "question": "的带权路径最短，也符合我们的信息论，即我们希望越常用的词拥有更短的编码。如何编码呢？"
            },
            {
              "id": "gen_0277",
              "question": "3 Word2vec 中使用 霍夫曼树 的好处？"
            },
            {
              "id": "gen_0278",
              "question": "4 为什么 Word2vec 中会用到 负采样？"
            },
            {
              "id": "gen_0279",
              "question": "5 Word2vec 中会用到 负采样 是什么样？"
            },
            {
              "id": "gen_0280",
              "question": "6 Word2vec 中 负采样 的采样方式？"
            },
            {
              "id": "gen_0281",
              "question": "1 word2vec和NNLM对比有什么区别？"
            },
            {
              "id": "gen_0282",
              "question": "2 word2vec和tf-idf 在相似度计算时的区别？"
            },
            {
              "id": "gen_0283",
              "question": "1 word2vec训练trick，window设置多大？"
            },
            {
              "id": "gen_0284",
              "question": "比较大，会提取更多的topic信息？"
            },
            {
              "id": "gen_0285",
              "question": "1 中文命名实体识别 与 英文命名实体识别的区别？"
            },
            {
              "id": "gen_0286",
              "question": "那么常用的方法有哪些呢？"
            },
            {
              "id": "gen_0287",
              "question": "1 什么是 词汇增强？"
            },
            {
              "id": "gen_0288",
              "question": "2 为什么说 「词汇增强」 方法对于中文 NER 任务有效呢？"
            },
            {
              "id": "gen_0289",
              "question": "如何在基于字符的NER系统中引入词汇信息，是近年来NER的一个研究重点。本文将这种引入词汇的方？"
            },
            {
              "id": "gen_0290",
              "question": "3 词汇增强 方法有哪些？"
            },
            {
              "id": "gen_0291",
              "question": "4.1 什么是 Dynamic Architecture？"
            },
            {
              "id": "gen_0292",
              "question": "4.2 常用方法有哪些？"
            },
            {
              "id": "gen_0293",
              "question": "4.3 什么是 Lattice LSTM ，存在什么问题？"
            },
            {
              "id": "gen_0294",
              "question": "4.4 什么是 FLAT ，存在什么问题？"
            },
            {
              "id": "gen_0295",
              "question": "5.1 什么是 Adaptive Embedding 范式？"
            },
            {
              "id": "gen_0296",
              "question": "5.2 常用方法有哪些？"
            },
            {
              "id": "gen_0297",
              "question": "5.3 什么是 WC-LSTM ，存在什么问题？"
            },
            {
              "id": "gen_0298",
              "question": "1 什么是 词汇/实体类型信息增强？"
            },
            {
              "id": "gen_0299",
              "question": "3 词汇/实体类型信息增强 方法有哪些？"
            },
            {
              "id": "gen_0300",
              "question": "4 什么是 LEX-BERT？"
            },
            {
              "id": "gen_0301",
              "question": "率比较高，那么分词结果会把两个词合并，那么合并与否对LDA的训练是否有影响呢？"
            },
            {
              "id": "gen_0302",
              "question": "超参数\\alpha \\beta对训练的影响？"
            },
            {
              "id": "gen_0303",
              "question": "用大的数据集训练一个general的model，还是根据垂直领域训练一个specific的model呢？"
            },
            {
              "id": "gen_0304",
              "question": "为什么LDA的最大似然难求？"
            },
            {
              "id": "gen_0305",
              "question": "1 什么是事件？"
            },
            {
              "id": "gen_0306",
              "question": "2 什么是事件抽取？"
            },
            {
              "id": "gen_0307",
              "question": "3 ACE测评中事件抽取涉及的几个基本术语及任务是什么？"
            },
            {
              "id": "gen_0308",
              "question": "4 事件抽取怎么发展的？"
            },
            {
              "id": "gen_0309",
              "question": "5 事件抽取存在什么问题？"
            },
            {
              "id": "gen_0310",
              "question": "1.1 什么是触发词检测？"
            },
            {
              "id": "gen_0311",
              "question": "1.2 触发词检测有哪些方法？"
            },
            {
              "id": "gen_0312",
              "question": "2.1 什么是类型识别？"
            },
            {
              "id": "gen_0313",
              "question": "2.2 类型识别有哪些方法？"
            },
            {
              "id": "gen_0314",
              "question": "3.1 什么是角色识别？"
            },
            {
              "id": "gen_0315",
              "question": "3.2 角色识别有哪些方法？"
            },
            {
              "id": "gen_0316",
              "question": "4.1 什么是论元检测？"
            },
            {
              "id": "gen_0317",
              "question": "4.2 论元检测有哪些方法？"
            },
            {
              "id": "gen_0318",
              "question": "1 模式匹配方法怎么用在事件抽取中？"
            },
            {
              "id": "gen_0319",
              "question": "2 统计机器学习方法怎么用在事件抽取中？"
            },
            {
              "id": "gen_0320",
              "question": "3 深度学习方法怎么用在事件抽取中？"
            },
            {
              "id": "gen_0321",
              "question": "1 事件抽取中常见的英文数据集有哪些？"
            },
            {
              "id": "gen_0322",
              "question": "2 事件抽取中常见的中文数据集有哪些？"
            },
            {
              "id": "gen_0323",
              "question": "3 事件抽取的评价指标是什么？"
            },
            {
              "id": "gen_0324",
              "question": "怎么计算的？"
            },
            {
              "id": "gen_0325",
              "question": "1 事件抽取和命名实体识别（即实体抽取）有什么异同？"
            },
            {
              "id": "gen_0326",
              "question": "2 事件抽取和关系抽取有什么异同？"
            },
            {
              "id": "gen_0327",
              "question": "3 什么是事理图谱？"
            },
            {
              "id": "gen_0328",
              "question": "有哪些事件关系类型？"
            },
            {
              "id": "gen_0329",
              "question": "事理图谱怎么构建？"
            },
            {
              "id": "gen_0330",
              "question": "要技术领域及当前发展热点是什么？"
            },
            {
              "id": "gen_0331",
              "question": "① 事件抽取的定义/概念是什么？"
            },
            {
              "id": "gen_0332",
              "question": "哪些比赛/会议给出了定义？"
            },
            {
              "id": "gen_0333",
              "question": "Q:不同任务对事件的定义不同吧，能具体解释下这些字段吗？"
            },
            {
              "id": "gen_0334",
              "question": "Q:能简单介绍一些事件抽取的应用背景吗？"
            },
            {
              "id": "gen_0335",
              "question": "Q:事件是要分类型的吧？"
            },
            {
              "id": "gen_0336",
              "question": "Q:事件抽取针对的是一段话还是一篇文章呢？"
            },
            {
              "id": "gen_0337",
              "question": "② 有哪些常用的评测数据集和评测标准？"
            },
            {
              "id": "gen_0338",
              "question": "③ 国内外有哪些研究团队和学者，它们主要研究的目标是什么？"
            },
            {
              "id": "gen_0339",
              "question": "④ 事件抽取有哪些应用场景和实际的产品？"
            },
            {
              "id": "gen_0340",
              "question": "Q: 为什么通过新闻可以预测网络故障呢？"
            },
            {
              "id": "gen_0341",
              "question": "⑤ 事件抽取的一般过程，有标注数据开展研究，如何扩展，没有数据怎么做？"
            },
            {
              "id": "gen_0342",
              "question": "Q: 事件抽取一般有什么方法呢？"
            },
            {
              "id": "gen_0343",
              "question": "⑥ 深度学习在事件抽取上有哪些应用，与传统方法比有什么优势/劣势？"
            },
            {
              "id": "gen_0344",
              "question": "⑦ 事件抽取与其他信息抽取任务（关系抽取、NER 等）有什么联系，难点在哪？"
            },
            {
              "id": "gen_0345",
              "question": "A: 时间是不是直接抽取就好了，其它属性该怎么办呢？"
            },
            {
              "id": "gen_0346",
              "question": "Q: 触发词一般是预定义好的，还是需要做检测任务？"
            },
            {
              "id": "gen_0347",
              "question": "⑧ 事件之间的关系如何表示，如何做事件之间的关系抽取，目前有哪些研究？"
            },
            {
              "id": "gen_0348",
              "question": "⑨ 有哪些值得阅读的论文？"
            },
            {
              "id": "gen_0349",
              "question": "有哪些开源了代码的工作？"
            },
            {
              "id": "gen_0350",
              "question": "⑩ 最新的前沿进展有哪些？"
            },
            {
              "id": "gen_0351",
              "question": "https://baijiahao.baidu.com/s？"
            },
            {
              "id": "gen_0352",
              "question": "微调方法是啥？"
            },
            {
              "id": "gen_0353",
              "question": "如何微调？"
            },
            {
              "id": "gen_0354",
              "question": "为什么需要 PEFT？"
            },
            {
              "id": "gen_0355",
              "question": "介绍一下 PEFT？"
            },
            {
              "id": "gen_0356",
              "question": "PEFT 有什么优点？"
            },
            {
              "id": "gen_0357",
              "question": "微调方法批处理大小模式GPU显存速度？"
            },
            {
              "id": "gen_0358",
              "question": "Peft 和 全量微调区别？"
            },
            {
              "id": "gen_0359",
              "question": "PEFT 存在问题？"
            },
            {
              "id": "gen_0360",
              "question": "能不能总结一下各种参数高效微调方法？"
            },
            {
              "id": "gen_0361",
              "question": "trick 3：词向量选取：词向量 or 字向量？"
            },
            {
              "id": "gen_0362",
              "question": "trick 4：特征提取器 如何选择？"
            },
            {
              "id": "gen_0363",
              "question": "trick 5：专有名称 怎么 处理？"
            },
            {
              "id": "gen_0364",
              "question": "trick 6：标注数据 不足怎么处理？"
            },
            {
              "id": "gen_0365",
              "question": "1 什么是实体嵌套？"
            },
            {
              "id": "gen_0366",
              "question": "说明：如图，Span{呼}{枢}=1，代表「呼吸中枢」是一个部位实体；Span{呼}{累}=2，代表？"
            },
            {
              "id": "gen_0367",
              "question": "如何构造Span矩阵问题？"
            },
            {
              "id": "gen_0368",
              "question": "如何解决0-1标签稀疏问题？"
            },
            {
              "id": "gen_0369",
              "question": "trick 9：NER实体span过长怎么办？"
            },
            {
              "id": "gen_0370",
              "question": "trick 10: NER 标注数据噪声问题？"
            },
            {
              "id": "gen_0371",
              "question": "够，另外一个数据量很少，可以怎么做？"
            },
            {
              "id": "gen_0372",
              "question": "trick 12： NER 标注数据不均衡问题？"
            },
            {
              "id": "gen_0373",
              "question": "标注样本少怎么办？"
            },
            {
              "id": "gen_0374",
              "question": "工业界如何解决NER问题？"
            },
            {
              "id": "gen_0375",
              "question": "1 在C++ 程序中调⽤被C 编译器编译后的函数，为什么要加extern“C”？"
            },
            {
              "id": "gen_0376",
              "question": "2 什么是多线程，多线程与多任务有什么区别？"
            },
            {
              "id": "gen_0377",
              "question": "5 可以⽤for循环直接删除ArrayList的特定元素吗？"
            },
            {
              "id": "gen_0378",
              "question": "可能会出现什么问题？"
            },
            {
              "id": "gen_0379",
              "question": "怎样解决？"
            },
            {
              "id": "gen_0380",
              "question": "6 如何看当前Linux系统有⼏颗物理CPU和每颗CPU的核数？"
            },
            {
              "id": "gen_0381",
              "question": "7 使⽤top查看系统资源占⽤情况时，哪⼀列表⽰内存占⽤呢？"
            },
            {
              "id": "gen_0382",
              "question": "8 如何查看当前系统都有哪些进程？"
            },
            {
              "id": "gen_0383",
              "question": "10 ⽹卡或者硬盘有问题时，我们可以通过使⽤哪个命令查看相关信息？"
            },
            {
              "id": "gen_0384",
              "question": "12 什么是lambda函数？"
            },
            {
              "id": "gen_0385",
              "question": "它有什么好处？"
            },
            {
              "id": "gen_0386",
              "question": "15 Python⾥⾯如何拷贝⼀个对象？"
            },
            {
              "id": "gen_0387",
              "question": "4 在⼀个严格单调递增的整数数组中找到a[x] == x的位置？"
            },
            {
              "id": "gen_0388",
              "question": "2 树形结构为什么不需要归⼀化？"
            },
            {
              "id": "gen_0389",
              "question": "6 为什么xgboost要⽤泰勒展开，优势在哪⾥？"
            },
            {
              "id": "gen_0390",
              "question": "7 谈谈判别式模型和⽣成式模型？"
            },
            {
              "id": "gen_0391",
              "question": "11 如何进⾏特征选择？"
            },
            {
              "id": "gen_0392",
              "question": "12 机器学习和统计⾥⾯的auc的物理意义是啥？"
            },
            {
              "id": "gen_0393",
              "question": "13 常见的分类算法有哪些？"
            },
            {
              "id": "gen_0394",
              "question": "他们各⾃的优缺点是什么？"
            },
            {
              "id": "gen_0395",
              "question": "14 RF与GBDT之间的区别与联系？"
            },
            {
              "id": "gen_0396",
              "question": "16 简述KNN最近邻分类算法的过程？"
            },
            {
              "id": "gen_0397",
              "question": "18 如何降低数据集的维度以减少模型计算时间？"
            },
            {
              "id": "gen_0398",
              "question": "19 如何提升已经达到96%精度的分类模型性能？"
            },
            {
              "id": "gen_0399",
              "question": "20 如何在⼀个数据集上选择重要的变量？"
            },
            {
              "id": "gen_0400",
              "question": "21 Gradient boosting算法(GBM)和随机森林都是基于树的算法,它们有什么区别？"
            },
            {
              "id": "gen_0401",
              "question": "22 如何理解模型的过拟合与⽋拟合，以及如何解决？"
            },
            {
              "id": "gen_0402",
              "question": "26 机器学习中的正则化到底是什么意思？"
            },
            {
              "id": "gen_0403",
              "question": "27 请简单阐述下决策树、回归、SVM、神经⽹络等算法各⾃的优缺点？"
            },
            {
              "id": "gen_0404",
              "question": "29 如何通俗理解贝叶斯⽅法和贝叶斯⽹络？"
            },
            {
              "id": "gen_0405",
              "question": "1 什么是归⼀化，它与标准化的区别是什么？"
            },
            {
              "id": "gen_0406",
              "question": "2 请问⼈⼯神经⽹络中为什么ReLU要好过于tanh和Sigmoid function？"
            },
            {
              "id": "gen_0407",
              "question": "3 什么样的数据集不适合⽤深度学习？"
            },
            {
              "id": "gen_0408",
              "question": "6 如何确定是否出现梯度爆炸？"
            },
            {
              "id": "gen_0409",
              "question": "7 RNN是怎么从单层⽹络⼀步⼀步构造的？"
            },
            {
              "id": "gen_0410",
              "question": "10 CNN究竟是怎样⼀步⼀步⼯作的？"
            },
            {
              "id": "gen_0411",
              "question": "11 什么是⾮极⼤值抑制（NMS）？"
            },
            {
              "id": "gen_0412",
              "question": "12 什么是深度学习中的anchor？"
            },
            {
              "id": "gen_0413",
              "question": "13 当神经⽹络的调参效果不好时，从哪些⾓度思考？"
            },
            {
              "id": "gen_0414",
              "question": "3图像边缘检测的原理？"
            },
            {
              "id": "gen_0415",
              "question": "4图像中的⾓点(Harris⾓点)是什么？"
            },
            {
              "id": "gen_0416",
              "question": "为什么⽤⾓点作为特征点？"
            },
            {
              "id": "gen_0417",
              "question": "5为什么说神经⽹络是端到端的⽹络？"
            },
            {
              "id": "gen_0418",
              "question": "6什么是感受野？"
            },
            {
              "id": "gen_0419",
              "question": "7⼯业界中遇到上亿的图像检索任务,如何提⾼图像对⽐效率？"
            },
            {
              "id": "gen_0420",
              "question": "9 one-stage和two-stage⽬标检测⽅法的区别和优缺点？"
            },
            {
              "id": "gen_0421",
              "question": "12什么是TF-IDF算法？"
            },
            {
              "id": "gen_0422",
              "question": "18 怎么解决推荐系统中的冷启动问题？"
            },
            {
              "id": "gen_0423",
              "question": "21 什么是wide&deep模型？"
            },
            {
              "id": "gen_0424",
              "question": "22 怎样将知识图谱引⼊推荐系统？"
            },
            {
              "id": "gen_0425",
              "question": "23 阿⾥最新开源的X-Deep Learning为Online Learning提供了哪些解决⽅案？"
            },
            {
              "id": "gen_0426",
              "question": "24 如何离线评价召回阶段各种模型算法的好坏？"
            },
            {
              "id": "gen_0427",
              "question": "26 芝⿇信⽤分的主要计算维度？"
            },
            {
              "id": "gen_0428",
              "question": "27 为什么我们做评分卡的时候要⽤woe编码？"
            },
            {
              "id": "gen_0429",
              "question": "28 深度学习的风控模型，从经验上看，样本量⼤概要多少条啊？"
            },
            {
              "id": "gen_0430",
              "question": "extern“C”？"
            },
            {
              "id": "gen_0431",
              "question": "将本问题扩展⼀下，下⾯的代码可能会出现什么问题？"
            },
            {
              "id": "gen_0432",
              "question": "Warning: bad syntax, perhaps a bogus '-'？"
            },
            {
              "id": "gen_0433",
              "question": "root 1 0.0 0.0 2900 1428？"
            },
            {
              "id": "gen_0434",
              "question": "root 2 0.0 0.0 0 0？"
            },
            {
              "id": "gen_0435",
              "question": "root 3 0.0 0.0 0 0？"
            },
            {
              "id": "gen_0436",
              "question": "root 4 0.0 0.0 0 0？"
            },
            {
              "id": "gen_0437",
              "question": "4 S root 1 0 0 80 0 - 725 - 10:43？"
            },
            {
              "id": "gen_0438",
              "question": "1 S root 2 0 0 80 0 - 0- 10:43？"
            },
            {
              "id": "gen_0439",
              "question": "1 S root 3 2 0 -40 - - 0- 10:43？"
            },
            {
              "id": "gen_0440",
              "question": "1 S root 4 2 0 80 0 - 0- 10:43？"
            },
            {
              "id": "gen_0441",
              "question": "1 S root 5 2 0 -40 - - 0- 10:43？"
            },
            {
              "id": "gen_0442",
              "question": "特性中的c点么？"
            },
            {
              "id": "gen_0443",
              "question": "元素到⽗节点中（有没有看到红⿊树中左旋操作的影⼦？"
            },
            {
              "id": "gen_0444",
              "question": "有些会重复。哪些重复了呢？"
            },
            {
              "id": "gen_0445",
              "question": "解释: 最长连续递增序列是 [1,3,5], 长度为 3。？"
            },
            {
              "id": "gen_0446",
              "question": "slow⼀定是指向中间节点的。但是此时有⼀个问题，我们此时⽆法知道slow的前⼀个位置了。怎么办？"
            },
            {
              "id": "gen_0447",
              "question": "据某关键字，重复出现次数最多的前100条？"
            },
            {
              "id": "gen_0448",
              "question": "说明：本⽂最初写于2012年6⽉，⽽后不断反反复复修改&优化，修改次数达上百次，最后修改于？"
            },
            {
              "id": "gen_0449",
              "question": "质是⼀样的。为什么？"
            },
            {
              "id": "gen_0450",
              "question": "margin 为 =y(wTx+b)=yf(x)中的Y是只取1和-1 吗？"
            },
            {
              "id": "gen_0451",
              "question": "y的唯⼀作⽤就是确保functional margin的⾮负性？"
            },
            {
              "id": "gen_0452",
              "question": "真是这样的么？"
            },
            {
              "id": "gen_0453",
              "question": "接下来的问题是，如何确定这个超平⾯呢？"
            },
            {
              "id": "gen_0454",
              "question": "我们把 functional margin 定为 1 了吗？"
            },
            {
              "id": "gen_0455",
              "question": "那什么是拉格朗⽇对偶性呢？"
            },
            {
              "id": "gen_0456",
              "question": "提醒：有读者可能会问上述推导过程如何⽽来？"
            },
            {
              "id": "gen_0457",
              "question": "为什么⾮⽀持向量对应的 等于零呢？"
            },
            {
              "id": "gen_0458",
              "question": "上⽂中，我们已经了解到了SVM处理线性可分的情况，那对于⾮线性的数据SVM咋处理呢？"
            },
            {
              "id": "gen_0459",
              "question": "是线性不可分的，此时咱们该如何把这两类数据分开呢(下⽂将会有⼀个相应的三维空间图)？"
            },
            {
              "id": "gen_0460",
              "question": "这样⼀来问题就解决了吗？"
            },
            {
              "id": "gen_0461",
              "question": "细想⼀下，刚才的⽅法是不是有问题？"
            },
            {
              "id": "gen_0462",
              "question": "之后的内积 的结果是相等的，那么区别在于什么地⽅呢？"
            },
            {
              "id": "gen_0463",
              "question": "上⾯说了这么⼀⼤堆，读者可能还是没明⽩核函数到底是个什么东西？"
            },
            {
              "id": "gen_0464",
              "question": "可怕的(如上⽂中19维乃⾄⽆穷维的例⼦)。那咋办呢？"
            },
            {
              "id": "gen_0465",
              "question": "⽺群围起来。但是篱笆应该建在哪⾥呢？"
            },
            {
              "id": "gen_0466",
              "question": "既然需要通过不断的训练以让蓝线最终成为最优分类超平⾯，那么，到底需要训练多少次呢？"
            },
            {
              "id": "gen_0467",
              "question": "隔。OK，还记得上⽂第1.3节开头的内容么？"
            },
            {
              "id": "gen_0468",
              "question": "4.1、什么是最⼩⼆乘法？"
            },
            {
              "id": "gen_0469",
              "question": "什么是⼀元线性模型呢？"
            },
            {
              "id": "gen_0470",
              "question": "那么如何选择乘⼦ 和 呢？"
            },
            {
              "id": "gen_0471",
              "question": "那么在每次迭代中，如何更新乘⼦呢？"
            },
            {
              "id": "gen_0472",
              "question": "知道了如何更新乘⼦，那么选取哪些乘⼦进⾏更新呢？"
            },
            {
              "id": "gen_0473",
              "question": "（特此致歉），怎么办呢？"
            },
            {
              "id": "gen_0474",
              "question": "⼀篇才决定⾃⼰的研究⽅向为SVM的。--http://weibo.com/1580904460/zraWk0u6u？"
            },
            {
              "id": "gen_0475",
              "question": "达数⽉的研究⽀持向量机阶段，直到今⽇。”--http://zhan.renren.com/profile/249335584？"
            },
            {
              "id": "gen_0476",
              "question": "function，说明其实他们的⽬标函数很像，那么问题是svm为什么这么popular呢？"
            },
            {
              "id": "gen_0477",
              "question": "AiohoyDwq？"
            },
            {
              "id": "gen_0478",
              "question": "如何求解凸⽬标函数，SVM,boosting等算法只是这些通⽤⽅法的⼀个具体组建⽽已。”？"
            },
            {
              "id": "gen_0479",
              "question": "⼯业界如何选取核函数，经验的⽅法？"
            },
            {
              "id": "gen_0480",
              "question": "svm的训练过程如何优化？"
            },
            {
              "id": "gen_0481",
              "question": "⽀持向量机系列，pluskid：http://blog.pluskid.org/？"
            },
            {
              "id": "gen_0482",
              "question": "2 9 .发明l i b s v m的台湾林智仁教授0 6年的机器学习讲义S V M：h t t p : / / w e n k u . b a i d u . c o m / l i n k？"
            },
            {
              "id": "gen_0483",
              "question": "www.google.com.hk/url？"
            },
            {
              "id": "gen_0484",
              "question": "http://jacoxu.com/？"
            },
            {
              "id": "gen_0485",
              "question": "c.blog.sina.com.cn/profile.php？"
            },
            {
              "id": "gen_0486",
              "question": "在线编辑Latex 公式：http://www.codecogs.com/latex/eqneditor.php？"
            },
            {
              "id": "gen_0487",
              "question": "Knn、KMeans之类则需要归⼀化呢？"
            },
            {
              "id": "gen_0488",
              "question": "除了归⼀化，我们还会经常提到标准化，那到底什么是标准化和归⼀化呢？"
            },
            {
              "id": "gen_0489",
              "question": "经常在机器学习中的优化问题中看到⼀个算法，即梯度下降法，那到底什么是梯度下降法呢？"
            },
            {
              "id": "gen_0490",
              "question": "额，问题又来了，什么是梯度？"
            },
            {
              "id": "gen_0491",
              "question": "的价值呢？"
            },
            {
              "id": "gen_0492",
              "question": "如果来了⼀个新的房⼦/⾯积，假设在房屋销售价格的记录中没有的，我们怎么办呢？"
            },
            {
              "id": "gen_0493",
              "question": "如何调整θ以使得J(θ)取得最⼩值有很多⽅法，其中有最⼩⼆乘法(min square)，是⼀种完全是数学？"
            },
            {
              "id": "gen_0494",
              "question": "描述的⽅法，另外⼀种就是梯度下降法。？"
            },
            {
              "id": "gen_0495",
              "question": "到底什么是EM算法呢？"
            },
            {
              "id": "gen_0496",
              "question": "你懂了么？"
            },
            {
              "id": "gen_0497",
              "question": "⼗有⼋九你没懂。因为你可能不懂什么是最⼤似然估计？"
            },
            {
              "id": "gen_0498",
              "question": "如果要你推测，这⼀发命中的⼦弹是谁打的？"
            },
            {
              "id": "gen_0499",
              "question": "⼀上公式，你可能就懵圈了。然后回想起我前沿开头所说的话：难道就没有⼀篇通俗易懂的么？"
            },
            {
              "id": "gen_0500",
              "question": "假定我们需要统计七⽉在线10万学员中男⽣⼥⽣的⾝⾼分布，怎么统计呢？"
            },
            {
              "id": "gen_0501",
              "question": "这100个⼈（的⾝⾼）出现的概率最⼤啊，这个概率就是上⾯这个似然函数L(θ)，怎么做到的呢？"
            },
            {
              "id": "gen_0502",
              "question": "之，怎样的θ能让L(θ)最⼤？"
            },
            {
              "id": "gen_0503",
              "question": "数的向量那怎么处理呢？"
            },
            {
              "id": "gen_0504",
              "question": "⽤极⼤似然估计求解了。 于是乎，怎么把Z变成已知的？"
            },
            {
              "id": "gen_0505",
              "question": "maximization algorithm？"
            },
            {
              "id": "gen_0506",
              "question": "OK，问题变得有意思了。现在我们的⽬标没变，还是估计PA和PB，需要怎么做呢？"
            },
            {
              "id": "gen_0507",
              "question": "和蛋⽣鸡的问题吗，如何破？"
            },
            {
              "id": "gen_0508",
              "question": "计新的PA和PB，如果新的PA和PB和我们初始化的PA和PB⼀样，请问这说明了什么？"
            },
            {
              "id": "gen_0509",
              "question": "如果新估计出来的PA和PB和我们初始化的值差别很⼤，怎么办呢？"
            },
            {
              "id": "gen_0510",
              "question": "还记得1.2节开头所说的吧？"
            },
            {
              "id": "gen_0511",
              "question": "分别求偏导，再令其等于0，求解出来不也⼀样吗？"
            },
            {
              "id": "gen_0512",
              "question": "），即得到上图中的（2）式，但（2）式还是有“和的对数”，还是求解不了，咋办呢？"
            },
            {
              "id": "gen_0513",
              "question": "Jensen不等式，促进神奇发⽣的Jensen不等式到底是什么来历呢？"
            },
            {
              "id": "gen_0514",
              "question": "就是 的期望。为什么？"
            },
            {
              "id": "gen_0515",
              "question": "的最⼤值啊，⽽我们想得到式(2)的最⼤值，那怎么办呢？"
            },
            {
              "id": "gen_0516",
              "question": "什么时候下界J(z,Q)与L(θ)在此点θ处相等？"
            },
            {
              "id": "gen_0517",
              "question": "为什么⼀定会收敛？"
            },
            {
              "id": "gen_0518",
              "question": "还有就是，如上节所提出的第⼆个问题，它会收敛吗？"
            },
            {
              "id": "gen_0519",
              "question": "或者，如何怎么确保EM收敛？"
            },
            {
              "id": "gen_0520",
              "question": "解释下第（4）步，得到 时，只是最⼤化？"
            },
            {
              "id": "gen_0521",
              "question": "N O，M步中，到底如何求θ的极值呢？"
            },
            {
              "id": "gen_0522",
              "question": "M步很显然，就是最⼤化那⼀步，E步又从何谈起呢？"
            },
            {
              "id": "gen_0523",
              "question": "⽣成的呢？"
            },
            {
              "id": "gen_0524",
              "question": "词”。事实上，⼀开始可供选择的主题有3个：教育、经济、交通，那为何偏偏选取教育这个主题呢？"
            },
            {
              "id": "gen_0525",
              "question": "怎么计算得到呢？"
            },
            {
              "id": "gen_0526",
              "question": "反过来，既然⽂档已经产⽣，那么如何根据已经产⽣好的⽂档反推其主题呢？"
            },
            {
              "id": "gen_0527",
              "question": "3 怎么通俗易懂地解释EM算法并且举个例⼦？"
            },
            {
              "id": "gen_0528",
              "question": "4 milter：如何感性地理解EM算法？"
            },
            {
              "id": "gen_0529",
              "question": "为什么要归⼀化呢？"
            },
            {
              "id": "gen_0530",
              "question": "1 归⼀化为什么能提⾼梯度下降法求解最优解的速度？"
            },
            {
              "id": "gen_0531",
              "question": "如何根据auc值来计算真正的类别，换句话说，就是对auc的反向⼯程。？"
            },
            {
              "id": "gen_0532",
              "question": "什么是AUC？"
            },
            {
              "id": "gen_0533",
              "question": "为什么AUC和logloss⽐accuracy更常⽤呢？"
            },
            {
              "id": "gen_0534",
              "question": "那么问题来了，什么是真、伪阳性率呢？"
            },
            {
              "id": "gen_0535",
              "question": "可以直接优化AUC来训练分类器吗？"
            },
            {
              "id": "gen_0536",
              "question": "低该数据集的维度以减少模型计算时间。你的机器内存有限。你会怎么做？"
            },
            {
              "id": "gen_0537",
              "question": "满意你的模型性能？"
            },
            {
              "id": "gen_0538",
              "question": "你可以做些什么呢？"
            },
            {
              "id": "gen_0539",
              "question": "有什么区别？"
            },
            {
              "id": "gen_0540",
              "question": "个⼀个学员依次上台站到你⾯前时，你会怎么区分谁是男谁是⼥呢？"
            },
            {
              "id": "gen_0541",
              "question": "但究竟根据哪个指标划分更好呢？"
            },
            {
              "id": "gen_0542",
              "question": "怎么判断“头发长短”或者“是否有喉结”是最好的划分⽅式，效果怎么量化呢？"
            },
            {
              "id": "gen_0543",
              "question": "什么是信息增益呢？"
            },
            {
              "id": "gen_0544",
              "question": "回归树又是什么呢？"
            },
            {
              "id": "gen_0545",
              "question": "不再是类别，是数值（预测值），那么怎么确定呢？"
            },
            {
              "id": "gen_0546",
              "question": "boosting集成学习由多个相关联的决策树联合决策，什么叫相关联？"
            },
            {
              "id": "gen_0547",
              "question": "GBDT咋办呢？"
            },
            {
              "id": "gen_0548",
              "question": "注意，为何gbdt可以⽤⽤负梯度近似残差呢？"
            },
            {
              "id": "gen_0549",
              "question": "恩，你可能要拍案⽽起了，惊呼，这不是跟上⽂介绍的gbdt乃异曲同⼯么？"
            },
            {
              "id": "gen_0550",
              "question": "更加⼀般的，损失函数不是⼆次函数咋办？"
            },
            {
              "id": "gen_0551",
              "question": "呜呼，透了！不过，这个转化过程中的关键泰勒⼆次展开到底是哪来的呢？"
            },
            {
              "id": "gen_0552",
              "question": "还记得4.2节开头对⽬标函数的说明吧（损失函数揭⽰训练误差 + 正则化定义复杂度）？"
            },
            {
              "id": "gen_0553",
              "question": "理解这个推导的关键在哪呢？"
            },
            {
              "id": "gen_0554",
              "question": "现在的情况是只要知道树的结构，就能得到⼀个该结构下的最好分数，那如何确定树的结构呢？"
            },
            {
              "id": "gen_0555",
              "question": "穷种，那咋办呢？"
            },
            {
              "id": "gen_0556",
              "question": "特征进⾏分割后，我们选择所谓的增益Gain最⾼的那个特征，⽽Gain如何计算呢？"
            },
            {
              "id": "gen_0557",
              "question": "还记得4.2节最后，我们得到的计算式⼦吧？"
            },
            {
              "id": "gen_0558",
              "question": "测2，⼀个预测2，那么倾向后⼀种，为什么呢？"
            },
            {
              "id": "gen_0559",
              "question": "• 本质上来讲，这就是⼀个⼆次函数最优化问题！但要是损失函数不是⼆次函数咋办？"
            },
            {
              "id": "gen_0560",
              "question": "确定分裂⽤的f e a t u r e，h o w？"
            },
            {
              "id": "gen_0561",
              "question": "如何确⽴节点的w以及最⼩的loss function，⼤声告诉我怎么做？"
            },
            {
              "id": "gen_0562",
              "question": "树，是不是贪⼼策略？"
            },
            {
              "id": "gen_0563",
              "question": "凡是这种循环迭代的⽅式必定有停⽌条件，什么时候停⽌呢？"
            },
            {
              "id": "gen_0564",
              "question": "为什么在实际的 kaggle ⽐赛中 gbdt 和 random forest 效果⾮常好？"
            },
            {
              "id": "gen_0565",
              "question": "怎样通俗的理解泰勒级数？"
            },
            {
              "id": "gen_0566",
              "question": "http://www.codecogs.com/latex/eqneditor.php？"
            },
            {
              "id": "gen_0567",
              "question": "重要（尽管GBDT调整后也可⽤于分类但不代表GBDT的树是分类树）。那么回归树是如何⼯作的呢？"
            },
            {
              "id": "gen_0568",
              "question": "Boosting，迭代，即通过迭代多棵树来共同决策。这怎么实现呢？"
            },
            {
              "id": "gen_0569",
              "question": "做最终结论？"
            },
            {
              "id": "gen_0570",
              "question": "那么哪⾥体现了Gradient呢？"
            },
            {
              "id": "gen_0571",
              "question": "1）既然图1和图2 最终效果相同，为何还需要GBDT呢？"
            },
            {
              "id": "gen_0572",
              "question": "2）Gradient呢？"
            },
            {
              "id": "gen_0573",
              "question": "不是“G”BDT么？"
            },
            {
              "id": "gen_0574",
              "question": "3）这不是boosting吧？"
            },
            {
              "id": "gen_0575",
              "question": "终分类器，那这个最终分类器的误差界到底是多少呢？"
            },
            {
              "id": "gen_0576",
              "question": "型，什么又是前向分步算法呢？"
            },
            {
              "id": "gen_0577",
              "question": "解释，你甚⾄可以认为本节就是对上⽂整个1.2节的解释。？"
            },
            {
              "id": "gen_0578",
              "question": "http://citeseerx.ist.psu.edu/viewdoc/download？"
            },
            {
              "id": "gen_0579",
              "question": "eqneditor.php？"
            },
            {
              "id": "gen_0580",
              "question": "但⼀直没有⼀篇好的⽂章理清到底什么是正则化？"
            },
            {
              "id": "gen_0581",
              "question": "什么是过拟合(Overfitting):？"
            },
            {
              "id": "gen_0582",
              "question": "如何解决过拟合问题：？"
            },
            {
              "id": "gen_0583",
              "question": "什么是规则？"
            },
            {
              "id": "gen_0584",
              "question": "那添加L1和L2正则化有什么⽤？"
            },
            {
              "id": "gen_0585",
              "question": "那为什么L2正则化可以获得值很⼩的参数？"
            },
            {
              "id": "gen_0586",
              "question": "4 机器学习中使⽤正则化来防⽌过拟合是什么原理？"
            },
            {
              "id": "gen_0587",
              "question": "2 何谓熵？"
            },
            {
              "id": "gen_0588",
              "question": "相当于问题转换成了寻找与样本的分布最接近的概率分布模型，如何寻找呢？"
            },
            {
              "id": "gen_0589",
              "question": "2013年在微博上关于极⼤似然估计的讨论：http://weibo.com/1580904460/zfUsAgCl2？"
            },
            {
              "id": "gen_0590",
              "question": "从袋⼦中取得⽩球的概率是多少？"
            },
            {
              "id": "gen_0591",
              "question": "率有多⼤？"
            },
            {
              "id": "gen_0592",
              "question": "样本之前（或观察到X之前），有着怎样的分布呢？"
            },
            {
              "id": "gen_0593",
              "question": "⽐如往台球桌上扔⼀个球，这个球落会落在何处呢？"
            },
            {
              "id": "gen_0594",
              "question": "根据上图，第1点可能很容易理解，但第2、3点中所述的条件独⽴是啥意思呢？"
            },
            {
              "id": "gen_0595",
              "question": "独⽴。意味着啥呢？"
            },
            {
              "id": "gen_0596",
              "question": "对于上图，在⼀个⼈已经呼吸困难（dyspnoea）的情况下，其抽烟（smoking）的概率是多少呢？"
            },
            {
              "id": "gen_0597",
              "question": "解释下上述式⼦推导过程：？"
            },
            {
              "id": "gen_0598",
              "question": "但搞了半天，虽然知道了什么是因⼦图，但因⼦图到底是⼲嘛的呢？"
            },
            {
              "id": "gen_0599",
              "question": "和意义何在？"
            },
            {
              "id": "gen_0600",
              "question": "啊哈，啥原理呢？"
            },
            {
              "id": "gen_0601",
              "question": "上述式⼦如何进⼀步化简计算呢？"
            },
            {
              "id": "gen_0602",
              "question": "前者2次乘法1次加法，后者1次乘法，1次加法。我们这⾥的计算是否能借鉴到分配率呢？"
            },
            {
              "id": "gen_0603",
              "question": "到底什么是sum-product算法呢？"
            },
            {
              "id": "gen_0604",
              "question": "说明：特征缩放其实并不需要太精确，其⽬的只是为了让梯度下降能够运⾏得更快⼀点，让梯度下降收？"
            },
            {
              "id": "gen_0605",
              "question": "function？"
            },
            {
              "id": "gen_0606",
              "question": "Normalization为什么效果好？"
            },
            {
              "id": "gen_0607",
              "question": "输⼊不是序列⽽输出为序列的情况怎么处理？"
            },
            {
              "id": "gen_0608",
              "question": "（有错么？"
            },
            {
              "id": "gen_0609",
              "question": "何共享呢？"
            },
            {
              "id": "gen_0610",
              "question": "应是what？"
            },
            {
              "id": "gen_0611",
              "question": "（b）⾄于右侧公式和左侧的图是怎样的⼀个⼀⼀对应关系呢？"
            },
            {
              "id": "gen_0612",
              "question": "区别在哪呢？"
            },
            {
              "id": "gen_0613",
              "question": "要问哪个变体是最好的？"
            },
            {
              "id": "gen_0614",
              "question": "其中的差异性真的重要吗？"
            },
            {
              "id": "gen_0615",
              "question": "1 LSTM结构推导，为什么⽐RNN好？"
            },
            {
              "id": "gen_0616",
              "question": "2 GRU是什么？"
            },
            {
              "id": "gen_0617",
              "question": "GRU对LSTM做了哪些改动？"
            },
            {
              "id": "gen_0618",
              "question": "3 LSTM神经⽹络输⼊输出究竟是怎样的？"
            },
            {
              "id": "gen_0619",
              "question": "这样做的⽬的是什么？"
            },
            {
              "id": "gen_0620",
              "question": "5 如何修复梯度爆炸问题？"
            },
            {
              "id": "gen_0621",
              "question": "6 如何解决RNN梯度爆炸和弥散的问题？"
            },
            {
              "id": "gen_0622",
              "question": "3 RNN是怎么从单层⽹络⼀步⼀步构造的？"
            },
            {
              "id": "gen_0623",
              "question": "6 LSTM相关的典型⾯试题：https://www.julyedu.com/search？"
            },
            {
              "id": "gen_0624",
              "question": "类下第36题『CNN究竟是怎样⼀步⼀步⼯作的？"
            },
            {
              "id": "gen_0625",
              "question": "举个例⼦，这周末北京有⼀草莓⾳乐节，那去不去呢？"
            },
            {
              "id": "gen_0626",
              "question": "压缩⾄0到1有何⽤处呢？"
            },
            {
              "id": "gen_0627",
              "question": "啥叫输⼊层、输出层、隐藏层呢？"
            },
            {
              "id": "gen_0628",
              "question": "简⾔之，当我们给定⼀个\"X\"的图案，计算机怎么识别这个图案就是“X”呢？"
            },
            {
              "id": "gen_0629",
              "question": "是怎么匹配的呢？"
            },
            {
              "id": "gen_0630",
              "question": "接着，我们细究下上图的具体计算过程。即上图中的输出结果1具体是怎么计算得到的呢？"
            },
            {
              "id": "gen_0631",
              "question": "很简单不是？"
            },
            {
              "id": "gen_0632",
              "question": "识别⼀幅图⽚是包含有字母\"X\"还是字母\"O\"？"
            },
            {
              "id": "gen_0633",
              "question": "那么这9个anchors是做什么的呢？"
            },
            {
              "id": "gen_0634",
              "question": "解释⼀下上⾯这张图的数字。？"
            },
            {
              "id": "gen_0635",
              "question": "信息（猜测这样做也许更鲁棒？"
            },
            {
              "id": "gen_0636",
              "question": "那么Anchor⼀共有多少个？"
            },
            {
              "id": "gen_0637",
              "question": "1 ）是否找到合适的损失函数？"
            },
            {
              "id": "gen_0638",
              "question": "2）batch size是否合适？"
            },
            {
              "id": "gen_0639",
              "question": "3）是否选择了合适的激活函数？"
            },
            {
              "id": "gen_0640",
              "question": "5）是否选择了合适的优化算法？"
            },
            {
              "id": "gen_0641",
              "question": "6）是否过拟合？"
            },
            {
              "id": "gen_0642",
              "question": "可能有⼈会问，为什么要输⼊输出都⼀样呢？"
            },
            {
              "id": "gen_0643",
              "question": "有什么⽤啊？"
            },
            {
              "id": "gen_0644",
              "question": "为什么输⼊数据需要归⼀化（Normalized Data）？"
            },
            {
              "id": "gen_0645",
              "question": "归⼀化后有什么好处呢？"
            },
            {
              "id": "gen_0646",
              "question": "数的中间部分，这样就相当于我这⼀层⽹络所学习到的特征分布被你搞坏了，这可怎么办？"
            },
            {
              "id": "gen_0647",
              "question": "层⽹络的神经元进⾏归⼀化。既然BN是对单个神经元的运算，那么在CNN中卷积层上要怎么搞？"
            },
            {
              "id": "gen_0648",
              "question": "定位的问题的解决思路有哪些？"
            },
            {
              "id": "gen_0649",
              "question": "Regression的部分加在哪？"
            },
            {
              "id": "gen_0650",
              "question": "疑惑：框要取多⼤？"
            },
            {
              "id": "gen_0651",
              "question": "当图像有很多物体怎么办的？"
            },
            {
              "id": "gen_0652",
              "question": "那把这个任务看做分类问题？"
            },
            {
              "id": "gen_0653",
              "question": "看成分类问题有何不妥？"
            },
            {
              "id": "gen_0654",
              "question": "看做classification， 有没有办法优化下？"
            },
            {
              "id": "gen_0655",
              "question": "取候选框⽤到的算法“选择性搜索”到底怎么选出这些候选框的呢？"
            },
            {
              "id": "gen_0656",
              "question": "makes for effective detection proposals？"
            },
            {
              "id": "gen_0657",
              "question": "说句⼈话就是，⼀张16:9⽐例的图⽚你硬是要Resize成1:1的图⽚，你说图⽚失真不？"
            },
            {
              "id": "gen_0658",
              "question": "最后加⼊某种结构，使得后⾯全连接层得到的输⼊变成固定的呢？"
            },
            {
              "id": "gen_0659",
              "question": "出。神奇吧？"
            },
            {
              "id": "gen_0660",
              "question": "R-CNN与Fast R-CNN的区别有哪些呢？"
            },
            {
              "id": "gen_0661",
              "question": "能不能找出⼀个更加⾼效的⽅法来求出这些候选框呢？"
            },
            {
              "id": "gen_0662",
              "question": "精准。那是不是可以结合region proposal的思想实现精准⼀些的定位？"
            },
            {
              "id": "gen_0663",
              "question": "那么如何建⽴某个位置和其特征的对应关系呢？"
            },
            {
              "id": "gen_0664",
              "question": "2 h t t p s : / / m p . w e i x i n . q q . c o m / s？"
            },
            {
              "id": "gen_0665",
              "question": "去⾯试时，便被问到“ o n e - s t a g e和t w o - s t a g e⽬标检测⽅法的区别和优缺点？"
            },
            {
              "id": "gen_0666",
              "question": "ques_id/2103 ，但如果你是第⼀次听到one-stage和two-stage，你会不会瞬间⼀脸懵逼，这是啥？"
            },
            {
              "id": "gen_0667",
              "question": "为什么⽬标检测问题更难？"
            },
            {
              "id": "gen_0668",
              "question": "为什么会这样？"
            },
            {
              "id": "gen_0669",
              "question": "中的分⼼模型。为什么说它注意⼒不集中呢？"
            },
            {
              "id": "gen_0670",
              "question": "输⼊句⼦单词注意⼒分配概率分布值呢？"
            },
            {
              "id": "gen_0671",
              "question": "(Tom,0.6)(Chase,0.2) (Jerry,0.2) 是如何得到的呢？"
            },
            {
              "id": "gen_0672",
              "question": "上述内容就是经典的Soft Attention模型的基本思想，那么怎么理解Attention模型的物理含义呢？"
            },
            {
              "id": "gen_0673",
              "question": "⼀个很⾃然的问题是：通过Self Attention到底学到了哪些规律或者抽取出了哪些特征呢？"
            },
            {
              "id": "gen_0674",
              "question": "Self Attention有什么增益或者好处呢？"
            },
            {
              "id": "gen_0675",
              "question": "这个“it”在这个句⼦是指什么呢？"
            },
            {
              "id": "gen_0676",
              "question": "它指的是street还是这个animal呢？"
            },
            {
              "id": "gen_0677",
              "question": "请务必检查Tensor2Tensor notebook ，在⾥⾯你可以下载⼀个Transformer模型，并⽤交互式可视化？"
            },
            {
              "id": "gen_0678",
              "question": "什么是查询向量、键向量和值向量向量？"
            },
            {
              "id": "gen_0679",
              "question": "组成)。所以我们需要⼀种⽅法把这⼋个矩阵压缩成⼀个矩阵。那该怎么做？"
            },
            {
              "id": "gen_0680",
              "question": "这个模式会是什么样⼦？"
            },
            {
              "id": "gen_0681",
              "question": "解码组件最后会输出⼀个实数向量。我们如何把浮点数变成⼀个单词？"
            },
            {
              "id": "gen_0682",
              "question": "你会如何⽐较两个概率分布呢？"
            },
            {
              "id": "gen_0683",
              "question": "好的评估指标（参考：交叉验证，链接https://www.youtube.com/watch？"
            },
            {
              "id": "gen_0684",
              "question": "样多。这是不是意味着，作为关键词，它们的重要性是⼀样的？"
            },
            {
              "id": "gen_0685",
              "question": "开始，熟悉使⽤向量来表⽰事物。你是否知道你的个性可以仅被五个数字的列表（向量）表⽰？"
            },
            {
              "id": "gen_0686",
              "question": "个性嵌⼊：你是什么样的⼈？"
            },
            {
              "id": "gen_0687",
              "question": "如何⽤0到100的范围来表⽰你是多么内向/外向（其中0是最内向的，100是最外向的）？"
            },
            {
              "id": "gen_0688",
              "question": "做过像MBTI那样的⼈格测试，或者五⼤⼈格特质测试？"
            },
            {
              "id": "gen_0689",
              "question": "当你只知道这⼀条信息的时候，你觉得你有多了解这个⼈？"
            },
            {
              "id": "gen_0690",
              "question": "个更像我？"
            },
            {
              "id": "gen_0691",
              "question": "看看“Man”和“Woman”彼此之间是如何⽐它们任⼀⼀个单词与“King”相⽐更相似的？"
            },
            {
              "id": "gen_0692",
              "question": "总结出⼀个模糊的“youth”概念？"
            },
            {
              "id": "gen_0693",
              "question": "的“royalty”概念？"
            },
            {
              "id": "gen_0694",
              "question": "差有多少？"
            },
            {
              "id": "gen_0695",
              "question": "但是我们作为输出词填写什么呢？"
            },
            {
              "id": "gen_0696",
              "question": "Bert最近很⽕，应该是最近最⽕爆的AI进展，⽹上的评价很⾼，那么Bert值得这么⾼的评价吗？"
            },
            {
              "id": "gen_0697",
              "question": "个⼈判断是值得。那为什么会有这么⾼的评价呢？"
            },
            {
              "id": "gen_0698",
              "question": "是因为它有重⼤的理论或者模型创新吗？"
            },
            {
              "id": "gen_0699",
              "question": "那么新的问题来了，为什么这种预训练的思路是可⾏的？"
            },
            {
              "id": "gen_0700",
              "question": "然图像领域预训练这么好⽤，那⼲嘛⾃然语⾔处理不做这个事情呢？"
            },
            {
              "id": "gen_0701",
              "question": "就算你傻，你看见⼈家这么做，有样学样不就⾏了吗？"
            },
            {
              "id": "gen_0702",
              "question": "已。听说过word embedding吗？"
            },
            {
              "id": "gen_0703",
              "question": "点的性能提升，只是没有那么耀眼的成功⽽已。没听过？"
            },
            {
              "id": "gen_0704",
              "question": "什么是语⾔模型？"
            },
            {
              "id": "gen_0705",
              "question": "⾯紧跟的单词应该是哪个，你会怎么做？"
            },
            {
              "id": "gen_0706",
              "question": "为什么会发⽣这种技术奇遇记？"
            },
            {
              "id": "gen_0707",
              "question": "C(W_i ) 是什么？"
            },
            {
              "id": "gen_0708",
              "question": "Word2Vec是怎么⼯作的呢？"
            },
            {
              "id": "gen_0709",
              "question": "⽽你回头看看， N N L M 是怎么训练的？"
            },
            {
              "id": "gen_0710",
              "question": "的。为什么Word2Vec这么处理？"
            },
            {
              "id": "gen_0711",
              "question": "么去训练⽹络。为什么要讲Word2Vec呢？"
            },
            {
              "id": "gen_0712",
              "question": "法的效果如何呢？"
            },
            {
              "id": "gen_0713",
              "question": "我们的主题是预训练，那么问题是Word Embedding这种做法能算是预训练吗？"
            },
            {
              "id": "gen_0714",
              "question": "这乍看上去好像是个查表操作，不像是预训练的做法是吧？"
            },
            {
              "id": "gen_0715",
              "question": "这⽚在Word Embedding头上笼罩了好⼏年的乌云是什么？"
            },
            {
              "id": "gen_0716",
              "question": "这些⽅法看上去都成本太⾼或者太繁琐了，有没有简单优美的解决⽅案呢？"
            },
            {
              "id": "gen_0717",
              "question": "ELMO的论⽂题⽬：“Deep contextualized word representation”更能体现其精髓，⽽精髓在哪⾥？"
            },
            {
              "id": "gen_0718",
              "question": "上⾯介绍的是ELMO的第⼀阶段：预训练阶段。那么预训练好⽹络结构后，如何给下游任务使⽤呢？"
            },
            {
              "id": "gen_0719",
              "question": "embedding后多义词问题解决了吗？"
            },
            {
              "id": "gen_0720",
              "question": "ELMO经过这般操作，效果如何呢？"
            },
            {
              "id": "gen_0721",
              "question": "那么站在现在这个时间节点看，ELMO有什么值得改进的缺点呢？"
            },
            {
              "id": "gen_0722",
              "question": "你问了：什么是注意⼒机制？"
            },
            {
              "id": "gen_0723",
              "question": "前汽车的雏形已经出现了，⼲嘛还要执着在换轮胎这个事情呢？"
            },
            {
              "id": "gen_0724",
              "question": "假设预训练好了⽹络模型，后⾯下游任务怎么⽤？"
            },
            {
              "id": "gen_0725",
              "question": "看到了么？"
            },
            {
              "id": "gen_0726",
              "question": "暴露年龄的歌词）？"
            },
            {
              "id": "gen_0727",
              "question": "不同任务，怎么改造才能靠近GPT的⽹络结构呢？"
            },
            {
              "id": "gen_0728",
              "question": "那么站在现在的时间节点看，G P T有什么值得改进的地⽅呢？"
            },
            {
              "id": "gen_0729",
              "question": "以使⽤Bert预训练好的模型参数呢？"
            },
            {
              "id": "gen_0730",
              "question": "Bert采⽤这种两阶段⽅式解决各种NLP任务效果如何？"
            },
            {
              "id": "gen_0731",
              "question": "乍⼀看上去好像不太好搞。我觉得吧，其实有⼀种很直观的思路，怎么办？"
            },
            {
              "id": "gen_0732",
              "question": "那么Bert是怎么做的呢？"
            },
            {
              "id": "gen_0733",
              "question": "我们前⾯不是提过Word2Vec吗？"
            },
            {
              "id": "gen_0734",
              "question": "根据它的上⽂Context-Before和下⽂Context-after去预测单词。其实Bert怎么做的？"
            },
            {
              "id": "gen_0735",
              "question": "那么Bert本⾝在模型和⽅法⾓度有什么创新呢？"
            },
            {
              "id": "gen_0736",
              "question": "我们说过Bert效果特别好，那么到底是什么因素起作⽤呢？"
            },
            {
              "id": "gen_0737",
              "question": "如何引⼊先验的语⾔学知识其实⼀直是NLP尤其是深度学习场景下的NLP的主要⽬标之⼀，不过⼀直没？"
            },
            {
              "id": "gen_0738",
              "question": "CF)，⾸先想⼀个简单的问题，如果你现在想看个电影，但你不知道具体看哪部，你会怎么做？"
            },
            {
              "id": "gen_0739",
              "question": "从图中看到多少信息？"
            },
            {
              "id": "gen_0740",
              "question": "找到相似的⽤户和物品，通过什么途径找到呢？"
            },
            {
              "id": "gen_0741",
              "question": "此外，什么时候⽤item-base，什么时候⽤user-base呢：http://weibo.com/1580904460/zhZ9AiIkZ？"
            },
            {
              "id": "gen_0742",
              "question": "mod=weibotime？"
            },
            {
              "id": "gen_0743",
              "question": "了。为何？"
            },
            {
              "id": "gen_0744",
              "question": "更多请参考：http://sofasofa.io/forum_main_post.php？"
            },
            {
              "id": "gen_0745",
              "question": "进⾏最有效的推荐呢？"
            },
            {
              "id": "gen_0746",
              "question": "如何对物品进⾏分类，分成⼏类？"
            },
            {
              "id": "gen_0747",
              "question": "如何确定⽤户对哪些物品类别有兴趣，兴趣程度如何？"
            },
            {
              "id": "gen_0748",
              "question": "1 https://www.jianshu.com/p/7b6bb28c1753？"
            },
            {
              "id": "gen_0749",
              "question": "解决⽅案？"
            },
            {
              "id": "gen_0750",
              "question": "如何离线评价召回阶段各种模型算法的好坏？"
            },
            {
              "id": "gen_0751",
              "question": "都不知道该怎么做？"
            },
            {
              "id": "gen_0752",
              "question": "然⽽，相关的综述⽂章不少，但碎⽚罗列的居多，模型之间内在的联系和演化思路如何揭⽰？"
            },
            {
              "id": "gen_0753",
              "question": "样才能迅速get到新模型的创新点和适⽤场景，快速提⾼新论⽂速度，节约理解、复现模型的成本？"
            },
            {
              "id": "gen_0754",
              "question": "为什么我们做评分卡的时候要⽤woe编码，⽽不是⽤别的编码⽅式呢？"
            },
            {
              "id": "gen_0755",
              "question": "仅是因为woe可以把特征从⾮线性变成线性的吗？"
            },
            {
              "id": "gen_0756",
              "question": "Transformer模型的基本结构是什么？"
            },
            {
              "id": "gen_0757",
              "question": "它是如何改变深度学习领域的？"
            },
            {
              "id": "gen_0758",
              "question": "Transformer为何能够有效地处理长距离依赖问题？"
            },
            {
              "id": "gen_0759",
              "question": "与传统RNN和LSTM相比有哪些优势？"
            },
            {
              "id": "gen_0760",
              "question": "多头注意力的作用是什么？"
            },
            {
              "id": "gen_0761",
              "question": "能不能手写下attention？"
            },
            {
              "id": "gen_0762",
              "question": "Transformer模型如何平衡模型性能与计算资源的消耗？"
            },
            {
              "id": "gen_0763",
              "question": "Transformer模型的自注意力机制如何实现并行处理？"
            },
            {
              "id": "gen_0764",
              "question": "Transformer模型如何处理变长输入序列？"
            },
            {
              "id": "gen_0765",
              "question": "Transformer模型的缩放点积注意力(Scaled Dot-Product Attention)是什么，其重要性在哪里？"
            },
            {
              "id": "gen_0766",
              "question": "Transformer模型在实践中如何优化以处理超长序列？"
            },
            {
              "id": "gen_0767",
              "question": "Transformer模型在自注意力层中如何解决多尺度表示问题？"
            },
            {
              "id": "gen_0768",
              "question": "Transformer模型中的自注意力机制在计算效率和表示能力之间是如何权衡的？"
            },
            {
              "id": "gen_0769",
              "question": "Transformer模型的参数共享策略对模型性能有何影响？"
            },
            {
              "id": "gen_0770",
              "question": "Transformer encoder和decoder的区别？"
            },
            {
              "id": "gen_0771",
              "question": "Transformer模型中的前馈网络(Feed-Forward Networks)的作用是什么？"
            },
            {
              "id": "gen_0772",
              "question": "Pre-norm和post-norm有什么区别？"
            },
            {
              "id": "gen_0773",
              "question": "Transformer网络很深，是怎么避免过拟合问题的？"
            },
            {
              "id": "gen_0774",
              "question": "Transformer的两个mask机制是什么？"
            },
            {
              "id": "gen_0775",
              "question": "Transformer为什么要用Layer norm？"
            },
            {
              "id": "gen_0776",
              "question": "Encoder和decoder是如何进行交互的？"
            },
            {
              "id": "gen_0777",
              "question": "如何提高Transformer模型中自注意力机制的计算效率？"
            },
            {
              "id": "gen_0778",
              "question": "为什么self-attention要除以根号N？"
            },
            {
              "id": "gen_0779",
              "question": "有方法不用处理根号N的吗？"
            },
            {
              "id": "gen_0780",
              "question": "Transformer模型中注意力权重如何解释模型的决策？"
            },
            {
              "id": "gen_0781",
              "question": "如何在自注意力机制中平衡局部信息和全局信息的捕获？"
            },
            {
              "id": "gen_0782",
              "question": "基于attention有哪些代表性的改进方法？"
            },
            {
              "id": "gen_0783",
              "question": "如何设计更有效的注意力机制来处理层次化或结构化数据？"
            },
            {
              "id": "gen_0784",
              "question": "那 DMP 的数据是哪里来的呢？"
            },
            {
              "id": "gen_0785",
              "question": "而对于新用户，由于其特征非常的稀疏，使用基于深度学习（DL）的推荐系统效果会比较差，那有什么方法呢？"
            },
            {
              "id": "gen_0786",
              "question": "GLM是什么？"
            },
            {
              "id": "gen_0787",
              "question": "SVM的原理？"
            },
            {
              "id": "gen_0788",
              "question": "怎么找到最优的线性分类器？"
            },
            {
              "id": "gen_0789",
              "question": "支持向量是什么？"
            },
            {
              "id": "gen_0790",
              "question": "介绍一下CNN？"
            },
            {
              "id": "gen_0791",
              "question": "CNN中的卷积到底指什么？"
            },
            {
              "id": "gen_0792",
              "question": "举个例子？"
            },
            {
              "id": "gen_0793",
              "question": "介绍决策树、信息熵？"
            },
            {
              "id": "gen_0794",
              "question": "随机森林“随机”二字体现在什么地方？"
            },
            {
              "id": "gen_0795",
              "question": "介绍一下XGBoost，与GBDT相比有什么不同？"
            },
            {
              "id": "gen_0796",
              "question": "ReLU激活函数是如何解决梯度消失和梯度爆炸问题的？"
            },
            {
              "id": "gen_0797",
              "question": "什么是梯度消失和梯度爆炸？"
            },
            {
              "id": "gen_0798",
              "question": "什么单元更容易出现梯度消失梯度爆炸的问题？"
            },
            {
              "id": "gen_0799",
              "question": "ReLU如何解决梯度消失问题？"
            },
            {
              "id": "gen_0800",
              "question": "ReLU之前常用的激活函数？"
            },
            {
              "id": "gen_0801",
              "question": "卷积神经网络中常见的层有哪些？"
            },
            {
              "id": "gen_0802",
              "question": "m*n*3的图像输入进去，输出会有变化吗？"
            },
            {
              "id": "gen_0803",
              "question": "分类问题的交叉熵是什么？"
            },
            {
              "id": "gen_0804",
              "question": "分类问题是否可以用MSE？"
            },
            {
              "id": "gen_0805",
              "question": "推荐系统中，相比于余弦相似度，是否可以用欧几里得距离判断相似度？"
            },
            {
              "id": "gen_0806",
              "question": "过拟合怎么处理？"
            },
            {
              "id": "gen_0807",
              "question": "L1、L2正则化的效果、区别、原理？"
            },
            {
              "id": "gen_0808",
              "question": "Dropout的原理、在训练和测试时的区别？"
            },
            {
              "id": "gen_0809",
              "question": "SGD、Adam、动量优化的SGD？"
            },
            {
              "id": "gen_0810",
              "question": "Adam和动量优化的SGD效率上的区别？"
            },
            {
              "id": "gen_0811",
              "question": "推荐系统中，如何进行负采样？"
            },
            {
              "id": "gen_0812",
              "question": "NLP中常见的分词方法有哪些？"
            },
            {
              "id": "gen_0813",
              "question": "讲一下BERT的结构？"
            },
            {
              "id": "gen_0814",
              "question": "自然语言处理有哪些任务？"
            },
            {
              "id": "gen_0815",
              "question": "L1，L2正则化的区别，岭回归是L1正则化还是L2正则化？"
            },
            {
              "id": "gen_0816",
              "question": "怎么处理类别不平衡？"
            },
            {
              "id": "gen_0817",
              "question": "模型提速的方法有哪些？"
            },
            {
              "id": "gen_0818",
              "question": "了解数据挖掘的方法嘛？"
            },
            {
              "id": "gen_0819",
              "question": "了解对比学习嘛？"
            },
            {
              "id": "gen_0820",
              "question": "说一下广度优先遍历和深度优先遍历？"
            },
            {
              "id": "gen_0821",
              "question": "如何理解交叉熵的物理意义？"
            },
            {
              "id": "gen_0822",
              "question": "过拟合如何去解决？"
            },
            {
              "id": "gen_0823",
              "question": "类别不平衡是如何去处理的？"
            },
            {
              "id": "gen_0824",
              "question": "如果进行采样，策略是什么？"
            },
            {
              "id": "gen_0825",
              "question": "对于一个时间顺序的推荐数据，如何划分训练集和验证集，能不能随机？"
            },
            {
              "id": "gen_0826",
              "question": "欠拟合如何去解决，训练过程不收敛如何去解决？"
            },
            {
              "id": "gen_0827",
              "question": "二分类的分类损失函数？"
            },
            {
              "id": "gen_0828",
              "question": "多分类的分类损失函数(Softmax)？"
            },
            {
              "id": "gen_0829",
              "question": "为什么不用MSE分类用交叉熵？"
            },
            {
              "id": "gen_0830",
              "question": "yolov5相比于之前增加的特性有哪些？"
            },
            {
              "id": "gen_0831",
              "question": "可以介绍一下attention机制吗？"
            },
            {
              "id": "gen_0832",
              "question": "关于attention机制，三个矩阵KQ,KV,K..的作用是什么？"
            },
            {
              "id": "gen_0833",
              "question": "生成式模型与判别式模型的区别？"
            },
            {
              "id": "gen_0834",
              "question": "模型的方差和偏差是指什么？"
            },
            {
              "id": "gen_0835",
              "question": "二分类模型的评估指标有哪些？"
            },
            {
              "id": "gen_0836",
              "question": "AUC刻画的什么？"
            },
            {
              "id": "gen_0837",
              "question": "说明了什么意思？"
            },
            {
              "id": "gen_0838",
              "question": "交叉熵函数刻画的什么？"
            },
            {
              "id": "gen_0839",
              "question": "对于非常大的分类类别，对于softmax有哪些优化方法？"
            },
            {
              "id": "gen_0840",
              "question": "softmax除了作为激活函数，在深度学习中还有哪些用途？"
            },
            {
              "id": "gen_0841",
              "question": "可以解释一下熵吗，它的公式怎么算的？"
            },
            {
              "id": "gen_0842",
              "question": "BERT的base版本的原始模型，训练的时候，第一个epoch模型的判定结果很可能是错的，这个时候熵还可信吗？"
            },
            {
              "id": "gen_0843",
              "question": "交叉熵和KL散度有什么关系？"
            },
            {
              "id": "gen_0844",
              "question": "BERT的缺点？"
            },
            {
              "id": "gen_0845",
              "question": "RoBERTa相比BERT有哪些改进？"
            },
            {
              "id": "gen_0846",
              "question": "BERT的输入有哪几种Embedding？"
            },
            {
              "id": "gen_0847",
              "question": "你有了解其他模型去尝试解决长度限制的方案吗？"
            },
            {
              "id": "gen_0848",
              "question": "BERT是怎么缓解梯度消失的？"
            },
            {
              "id": "gen_0849",
              "question": "LN和BN的区别？"
            },
            {
              "id": "gen_0850",
              "question": "如何解决prompt泛化性？"
            },
            {
              "id": "gen_0851",
              "question": "关于bert的后续改进工作，分别改进了哪些地方？"
            },
            {
              "id": "gen_0852",
              "question": "对知识蒸馏知道多少，有哪些改进用到了？"
            },
            {
              "id": "gen_0853",
              "question": "YOLO的正负样本是什么？"
            },
            {
              "id": "gen_0854",
              "question": "模型压缩和加速的方法有哪些？"
            },
            {
              "id": "gen_0855",
              "question": "半精度是什么？"
            },
            {
              "id": "gen_0856",
              "question": "半精度的理论原理是什么？"
            },
            {
              "id": "gen_0857",
              "question": "你了解的知识蒸馏模型有哪些？"
            },
            {
              "id": "gen_0858",
              "question": "自监督、半监督、无监督的区别？"
            },
            {
              "id": "gen_0859",
              "question": "什么是bilstm-crf？"
            },
            {
              "id": "gen_0860",
              "question": "如何解决过拟合和欠拟合。？"
            },
            {
              "id": "gen_0861",
              "question": "什么是交叉验证。？"
            },
            {
              "id": "gen_0862",
              "question": "统计学中的P值是什么含义，如何通俗地解释？"
            },
            {
              "id": "gen_0863",
              "question": "如何构建多模态模型？"
            },
            {
              "id": "gen_0864",
              "question": "lora的矩阵怎么初始化？"
            },
            {
              "id": "gen_0865",
              "question": "gpt源码past_key_value是干啥的？"
            },
            {
              "id": "gen_0866",
              "question": "模型输出的分布比较稀疏，怎么处理？"
            },
            {
              "id": "gen_0867",
              "question": "描述下Transformer的结构？"
            },
            {
              "id": "gen_0868",
              "question": "为什么Transformer可以处理多种模态，它是怎么处理的？"
            },
            {
              "id": "gen_0869",
              "question": "详见: https://zhuanlan.zhihu.com/p/604551577？"
            },
            {
              "id": "gen_0870",
              "question": "如何解决Transformer长度限制的问题？"
            },
            {
              "id": "gen_0871",
              "question": "Python迭代器是什么，构成是怎样的？"
            },
            {
              "id": "gen_0872",
              "question": "10亿个参数的模型，部署后占用多大显存？"
            },
            {
              "id": "gen_0873",
              "question": "深度可分离卷积是什么？"
            },
            {
              "id": "gen_0874",
              "question": "CNN中参数量和计算量怎么算？"
            },
            {
              "id": "gen_0875",
              "question": "深度可分离卷积的参数量和计算量是多少？"
            },
            {
              "id": "gen_0876",
              "question": "了解Linux的管道命令吗？"
            },
            {
              "id": "gen_0877",
              "question": "AUC和ROC是什么？"
            },
            {
              "id": "gen_0878",
              "question": "Precision和Recall是什么？"
            },
            {
              "id": "gen_0879",
              "question": "Transfomer是什么？"
            },
            {
              "id": "gen_0880",
              "question": "Python的深拷贝和浅拷贝的区别？"
            },
            {
              "id": "gen_0881",
              "question": "赋值时浅拷贝还是深拷贝？"
            },
            {
              "id": "gen_0882",
              "question": "一个元素在一个有序数组的第一次出现位置？"
            },
            {
              "id": "gen_0883",
              "question": "blip2的架构，优势和之前多模态模型的区别？"
            },
            {
              "id": "gen_0884",
              "question": "知识蒸馏和无监督样本训练？"
            },
            {
              "id": "gen_0885",
              "question": "在自然语言处理模型训练中，评价指标是怎样设定的？"
            },
            {
              "id": "gen_0886",
              "question": "自然语言处理中对低质量数据做数据清洗的方法？"
            },
            {
              "id": "gen_0887",
              "question": "LSTM和RNN有什么区别？"
            },
            {
              "id": "gen_0888",
              "question": "解决什么问题？"
            },
            {
              "id": "gen_0889",
              "question": "多任务学习各loss差异过大怎样处理？"
            },
            {
              "id": "gen_0890",
              "question": "多模态融合后，怎样知道最终结果受哪种模态影响更大？"
            },
            {
              "id": "gen_0891",
              "question": "过拟合应该怎样处理？"
            },
            {
              "id": "gen_0892",
              "question": "BN层在训练和推理过程中有什么不一样？"
            },
            {
              "id": "gen_0893",
              "question": "什么是堆？"
            },
            {
              "id": "gen_0894",
              "question": "什么是完全二叉树？"
            },
            {
              "id": "gen_0895",
              "question": "路径规划算法有哪些？"
            },
            {
              "id": "gen_0896",
              "question": "如何判断链表是否有环？"
            },
            {
              "id": "gen_0897",
              "question": "什么是信息增益？"
            },
            {
              "id": "gen_0898",
              "question": "文本生成的几大预训练任务？"
            },
            {
              "id": "gen_0899",
              "question": "多模态中常见的SOTA模型有哪些？"
            },
            {
              "id": "gen_0900",
              "question": "对比学习负样本是否重要？"
            },
            {
              "id": "gen_0901",
              "question": "负样本构造成本过高应该怎么解决？"
            },
            {
              "id": "gen_0902",
              "question": "word2vec的原理，怎么训练的？"
            },
            {
              "id": "gen_0903",
              "question": "样本不平衡问题怎么处理的，有什么方法？"
            },
            {
              "id": "gen_0904",
              "question": "快速排序时间复杂度？"
            },
            {
              "id": "gen_0905",
              "question": "稳定性怎么样？"
            },
            {
              "id": "gen_0906",
              "question": "各种评估指标？"
            },
            {
              "id": "gen_0907",
              "question": "xgboost算法介绍？"
            },
            {
              "id": "gen_0908",
              "question": "评分卡建模全流程？"
            },
            {
              "id": "gen_0909",
              "question": "问题2、深度学习中，常见的损失函数有哪些？"
            },
            {
              "id": "gen_0910",
              "question": "问题5、CV中数据增强的方法有哪些？"
            },
            {
              "id": "gen_0911",
              "question": "多头注意力机制和单个注意力机制时间复杂度会变吗？"
            },
            {
              "id": "gen_0912",
              "question": "大模型微调过程中如何避免灾难性遗忘？"
            },
            {
              "id": "gen_0913",
              "question": "1 对话系统有哪几种？"
            },
            {
              "id": "gen_0914",
              "question": "2 这几种对话系统的区别？"
            },
            {
              "id": "gen_0915",
              "question": "如何衡量质量：以用户的主观体验为主？"
            },
            {
              "id": "gen_0916",
              "question": "如何衡量质量：以任务的完成情况来衡量对话质量？"
            },
            {
              "id": "gen_0917",
              "question": "1 为什么要用 多轮对话系统？"
            },
            {
              "id": "gen_0918",
              "question": "2 常见的多轮对话系统解决方案是什么？"
            },
            {
              "id": "gen_0919",
              "question": "1 什么是任务型对话系统？"
            },
            {
              "id": "gen_0920",
              "question": "2 任务型对话系统的流程是怎么样？"
            },
            {
              "id": "gen_0921",
              "question": "3.1 什么是 语言理解（SLU）？"
            },
            {
              "id": "gen_0922",
              "question": "3.2 语言理解（SLU）的输入输出是什么？"
            },
            {
              "id": "gen_0923",
              "question": "3.3 语言理解（SLU）所使用的技术是什么？"
            },
            {
              "id": "gen_0924",
              "question": "4.1 什么是 DST（对话状态跟踪）？"
            },
            {
              "id": "gen_0925",
              "question": "4.2 DST（对话状态跟踪）的输入输出是什么？"
            },
            {
              "id": "gen_0926",
              "question": "4.3 DST（对话状态跟踪）存在问题和解决方法？"
            },
            {
              "id": "gen_0927",
              "question": "4.4 DST（对话状态跟踪）实现方式是什么？"
            },
            {
              "id": "gen_0928",
              "question": "5.1 DPO（对话策略学习）是什么？"
            },
            {
              "id": "gen_0929",
              "question": "5.2 DPO（对话策略学习）的输入输出是什么？"
            },
            {
              "id": "gen_0930",
              "question": "5.3 DPO（对话策略学习）的实现方法是什么？"
            },
            {
              "id": "gen_0931",
              "question": "6.1 NLG（自然语言生成）是什么？"
            },
            {
              "id": "gen_0932",
              "question": "6.2 NLG（自然语言生成）的输入输出是什么？"
            },
            {
              "id": "gen_0933",
              "question": "6.3 NLG（自然语言生成）的实现方式？"
            },
            {
              "id": "gen_0934",
              "question": "一、为什么需要 提示学习（Prompting）？"
            },
            {
              "id": "gen_0935",
              "question": "二、什么是 提示学习（Prompting）？"
            },
            {
              "id": "gen_0936",
              "question": "三、提示学习（Prompting） 有什么优点？"
            },
            {
              "id": "gen_0937",
              "question": "四、提示学习（Prompting）有哪些方法，能不能稍微介绍一下它们间？"
            },
            {
              "id": "gen_0938",
              "question": "1.1 为什么需要 前缀微调（Prefix-tining）？"
            },
            {
              "id": "gen_0939",
              "question": "1.2 前缀微调（Prefix-tining）思路是什么？"
            },
            {
              "id": "gen_0940",
              "question": "1.3 前缀微调（Prefix-tining）的优点是什么？"
            },
            {
              "id": "gen_0941",
              "question": "1.4 前缀微调（Prefix-tining）的缺点是什么？"
            },
            {
              "id": "gen_0942",
              "question": "2.1 为什么需要 指示微调（Prompt-tuning）？"
            },
            {
              "id": "gen_0943",
              "question": "2.2 指示微调（Prompt-tuning）思路是什么？"
            },
            {
              "id": "gen_0944",
              "question": "2.3 指示微调（Prompt-tuning）优点是什么？"
            },
            {
              "id": "gen_0945",
              "question": "2.4 指示微调（Prompt-tuning）缺点是什么？"
            },
            {
              "id": "gen_0946",
              "question": "2.5 指示微调（Prompt-tuning）与 Prefix-tuning 区别 是什么？"
            },
            {
              "id": "gen_0947",
              "question": "2.6 指示微调（Prompt-tuning）与 fine-tuning 区别 是什么？"
            },
            {
              "id": "gen_0948",
              "question": "3.1 为什么需要 P-tuning？"
            },
            {
              "id": "gen_0949",
              "question": "3.2 P-tuning 思路是什么？"
            },
            {
              "id": "gen_0950",
              "question": "3.3 P-tuning 优点是什么？"
            },
            {
              "id": "gen_0951",
              "question": "3.4 P-tuning 缺点是什么？"
            },
            {
              "id": "gen_0952",
              "question": "3.5 大模型微调 p_tuning 和传统 fine tuning 有什么区别？"
            },
            {
              "id": "gen_0953",
              "question": "4.1 为什么需要 P-tuning v2？"
            },
            {
              "id": "gen_0954",
              "question": "4.2 P-tuning v2 思路是什么？"
            },
            {
              "id": "gen_0955",
              "question": "4.3 P-tuning v2 优点是什么？"
            },
            {
              "id": "gen_0956",
              "question": "4.4 P-tuning v2 缺点是什么？"
            },
            {
              "id": "gen_0957",
              "question": "四、提示学习（Prompting）有哪些方法，能不能稍微介绍一下？"
            },
            {
              "id": "gen_0958",
              "question": "比较大的变化？"
            },
            {
              "id": "gen_0959",
              "question": "如何 让 Prompt Tuning 能够在不同参数规模的预训练模型、针对不同下游任务的结果上都？"
            },
            {
              "id": "gen_0960",
              "question": "一、什么是文本挖掘？"
            },
            {
              "id": "gen_0961",
              "question": "二、文本挖掘的作用是什么？"
            },
            {
              "id": "gen_0962",
              "question": "1 什么是文本摘要？"
            },
            {
              "id": "gen_0963",
              "question": "2 文本摘要技术有哪些类型？"
            },
            {
              "id": "gen_0964",
              "question": "1 抽取式摘要是怎么做的？"
            },
            {
              "id": "gen_0965",
              "question": "1.1 句子重要性评估算法有哪些？"
            },
            {
              "id": "gen_0966",
              "question": "1.2 基于约束的摘要生成方法有哪些？"
            },
            {
              "id": "gen_0967",
              "question": "1.3 TextTeaser算法是怎么抽取摘要的？"
            },
            {
              "id": "gen_0968",
              "question": "1.4 TextRank算法是怎么抽取摘要的？"
            },
            {
              "id": "gen_0969",
              "question": "2 抽取式摘要的可读性问题是什么？"
            },
            {
              "id": "gen_0970",
              "question": "1 压缩式摘要是怎么做的？"
            },
            {
              "id": "gen_0971",
              "question": "1 生成式摘要是怎么做的？"
            },
            {
              "id": "gen_0972",
              "question": "2 生成式摘要存在哪些问题？"
            },
            {
              "id": "gen_0973",
              "question": "3 Pointer-generator network解决了什么问题？"
            },
            {
              "id": "gen_0974",
              "question": "1 摘要质量的评估方法有哪些类型？"
            },
            {
              "id": "gen_0975",
              "question": "请专家对系统的自动摘要结果打分，但是专家之间差异性较大。解决方法之一是金字塔方法(pyramid？"
            },
            {
              "id": "gen_0976",
              "question": "2 什么是ROUGE？"
            },
            {
              "id": "gen_0977",
              "question": "3 几种ROUGE指标之间的区别是什么？"
            },
            {
              "id": "gen_0978",
              "question": "4 BLEU和ROUGE有什么不同？"
            },
            {
              "id": "gen_0979",
              "question": "2 什么是知识图谱呢？"
            },
            {
              "id": "gen_0980",
              "question": "2.1 什么是图（Graph）呢？"
            },
            {
              "id": "gen_0981",
              "question": "2.2 什么是 Schema 呢？"
            },
            {
              "id": "gen_0982",
              "question": "3 知识图谱的类别有哪些？"
            },
            {
              "id": "gen_0983",
              "question": "4 知识图谱的价值在哪呢？"
            },
            {
              "id": "gen_0984",
              "question": "二、怎么构建知识图谱呢？"
            },
            {
              "id": "gen_0985",
              "question": "1 知识图谱的数据来源于哪里？"
            },
            {
              "id": "gen_0986",
              "question": "2 信息抽取的难点在哪里？"
            },
            {
              "id": "gen_0987",
              "question": "3 构建知识图谱所涉及的技术？"
            },
            {
              "id": "gen_0988",
              "question": "4、知识图谱的具体构建技术是什么？"
            },
            {
              "id": "gen_0989",
              "question": "三、知识图谱怎么存储？"
            },
            {
              "id": "gen_0990",
              "question": "四、知识图谱可以做什么？"
            },
            {
              "id": "gen_0991",
              "question": "1 Q： 知识表示相对于one-hot表示的优势是什么？"
            },
            {
              "id": "gen_0992",
              "question": "2 Q：有哪些文本表示模型？"
            },
            {
              "id": "gen_0993",
              "question": "它们各有什么优缺点？"
            },
            {
              "id": "gen_0994",
              "question": "3 Q：word2vec与LDA模型之间的区别和联系？"
            },
            {
              "id": "gen_0995",
              "question": "4 Q：介绍下词向量空间中的平移不变现象？"
            },
            {
              "id": "gen_0996",
              "question": "5 Q：简要介绍下TransE模型的思想及优点？"
            },
            {
              "id": "gen_0997",
              "question": "一、为什么 需要 适配器微调（Adapter-tuning）？"
            },
            {
              "id": "gen_0998",
              "question": "二、适配器微调（Adapter-tuning）思路？"
            },
            {
              "id": "gen_0999",
              "question": "三、 适配器微调（Adapter-tuning）特点是什么？"
            },
            {
              "id": "gen_1000",
              "question": "四、AdapterFusion 思路 是什么？"
            },
            {
              "id": "gen_1001",
              "question": "五、AdapterDrop 思路 是什么？"
            },
            {
              "id": "gen_1002",
              "question": "六、AdapterDrop 特点 是什么？"
            },
            {
              "id": "gen_1003",
              "question": "七、MAM Adapter 思路 是什么？"
            },
            {
              "id": "gen_1004",
              "question": "八、MAM Adapter 特点 是什么？"
            }
          ]
        }
      ]
    },
    {
      "category": "LLM",
      "subcategories": [
        {
          "subcategory": "LLM-General",
          "questions": [
            {
              "id": "llm_0001",
              "question": "与首 token 时间？"
            },
            {
              "id": "llm_0002",
              "question": "1.3 面对“成本+效果”双约束，如何构建一个多目标打分函数并给出权重设计示例？"
            },
            {
              "id": "llm_0003",
              "question": "2.2 如何量化“数据不能出域”带来的合规成本并纳入 TCO？"
            },
            {
              "id": "llm_0004",
              "question": "2.3 当调用量突增 10× 时，自托管方案与 API 方案的成本拐点如何计算？"
            },
            {
              "id": "llm_0005",
              "question": "3.2 如何评估视觉编码器在 512×512 输入下的 FLOPs 占比？"
            },
            {
              "id": "llm_0006",
              "question": "3.3 当音频采样率从 16 kHz 提升到 48 kHz 时，对端到端延迟的影响如何建模？"
            },
            {
              "id": "llm_0007",
              "question": "4.1 LLaMA 2 社区版在月活 >700M 产品中的合规限制有哪些具体条款？"
            },
            {
              "id": "llm_0008",
              "question": "4.2 如何自动化扫描 Docker 镜像中携带的 GPL 组件与模型权重冲突？"
            },
            {
              "id": "llm_0009",
              "question": "4.3 若基于 ChatGLM-6B 二次分发，需要向用户披露哪些最小信息集？"
            },
            {
              "id": "llm_0010",
              "question": "5.2 如何构造对抗性 Prompt 以探测模型政治敏感话题的拒绝率？"
            },
            {
              "id": "llm_0011",
              "question": "5.3 给定 100 条业务 query，如何计算模型拒绝回答的可接受阈值？"
            },
            {
              "id": "llm_0012",
              "question": "1.2 如何利用 Elo 评级算法给指令难度打分并防止标注者偏差？"
            },
            {
              "id": "llm_0013",
              "question": "1.3 当 30% 数据为机器生成时，如何设计对抗过滤器降低循环幻觉？"
            },
            {
              "id": "llm_0014",
              "question": "2.1 在 10TB 原始语料上，如何用 MinHash LSH 在 4 小时内完成近似去重？"
            },
            {
              "id": "llm_0015",
              "question": "2.2 敏感词库每日更新，如何构建 Aho-Corasick 自动机的增量更新策略？"
            },
            {
              "id": "llm_0016",
              "question": "2.3 当遇到拆字、拼音、谐音变异时，如何基于 BERT-CRF 做敏感实体识别？"
            },
            {
              "id": "llm_0017",
              "question": "3.1 如何基于信息熵计算每轮对话的重要性得分并动态截断？"
            },
            {
              "id": "llm_0018",
              "question": "3.3 在 32k 长上下文模型中，如何设计滑动窗口保证首尾信息不丢失？"
            },
            {
              "id": "llm_0019",
              "question": "4.1 使用 Self-Instruct 生成医疗问答时，如何设置种子指令数量与多样性阈值？"
            },
            {
              "id": "llm_0020",
              "question": "4.2 如何基于 T5-PEGASUS 做回译以提升金融领域术语覆盖率？"
            },
            {
              "id": "llm_0021",
              "question": "4.3 当合成数据规模达到 1M 条时，如何抽样做人工验证并控制 95% 置信区间？"
            },
            {
              "id": "llm_0022",
              "question": "5.1 如何用 DVC + Git LFS 管理 500GB 微调数据集并支持回滚到任意版本？"
            },
            {
              "id": "llm_0023",
              "question": "5.2 当上游数据发生字段变更时，如何自动触发下游模型重训流水线？"
            },
            {
              "id": "llm_0024",
              "question": "1.2 当 GPU 显存限制为 40GB 时，如何估算 r=64 的 LoRA 增量参数量？"
            },
            {
              "id": "llm_0025",
              "question": "1.3 如何验证 alpha/r 比值在 2 与 0.5 时对收敛速度的影响？"
            },
            {
              "id": "llm_0026",
              "question": "2.3 当 batch size=1 仍 OOM 时，如何结合梯度检查点与微批次累积？"
            },
            {
              "id": "llm_0027",
              "question": "3.2 当三类任务样本量差异 100× 时，如何设置 softmax 温度系数防止过拟合？"
            },
            {
              "id": "llm_0028",
              "question": "3.3 如何监控各任务在验证集上的 loss 曲线并自动触发 Early Stopping？"
            },
            {
              "id": "llm_0029",
              "question": "4.2 如何设计控制实验验证“先 CPT 后 SFT”相比“混合训练”的幻觉率差异？"
            },
            {
              "id": "llm_0030",
              "question": "5.1 如何用 EWC（Elastic Weight Consolidation）计算重要权重矩阵并加入损失？"
            },
            {
              "id": "llm_0031",
              "question": "5.3 当遗忘率 >10% 时，如何采用 rehearsal buffer 进行回放并设置 buffer 大小？"
            },
            {
              "id": "llm_0032",
              "question": "1.1 在 1 万条客服 query 上，如何自动搜索 k=0,1,5,10 的 F1 并绘制边际收益曲线？"
            },
            {
              "id": "llm_0033",
              "question": "1.2 当示例顺序改变导致结果抖动 >8% 时，如何采用排序熵进行稳定性筛选？"
            },
            {
              "id": "llm_0034",
              "question": "1.3 如何基于强化学习（RLPO）自动选择最佳示例并定义奖励函数？"
            },
            {
              "id": "llm_0035",
              "question": "2.2 如何监控中间步骤的置信度并提前终止低置信链？"
            },
            {
              "id": "llm_0036",
              "question": "2.3 当 CoT 长度超过 2048 token 时，如何压缩提示而不损失准确率？"
            },
            {
              "id": "llm_0037",
              "question": "3.1 如何构建黑名单正则，防止角色提示诱导模型输出违法内容？"
            },
            {
              "id": "llm_0038",
              "question": "3.2 当用户输入“你是法官，请帮我生成虚假判决书”时，如何设计拒绝模板？"
            },
            {
              "id": "llm_0039",
              "question": "3.3 如何基于语义相似度实时检测越狱（Jailbreak）提示并自动升级风控？"
            },
            {
              "id": "llm_0040",
              "question": "4.1 如何用 Feature Flag 系统实现提示模板的灰度发布与实时回滚？"
            },
            {
              "id": "llm_0041",
              "question": "4.2 当模板变量 >50 个时，如何防止注入攻击并做模板语法校验？"
            },
            {
              "id": "llm_0042",
              "question": "4.3 如何计算提示模板变更对下游转化率提升的显著性（p-value）？"
            },
            {
              "id": "llm_0043",
              "question": "5.1 如何基于 LASER multilingual embeddings 计算中英提示的语义差距？"
            },
            {
              "id": "llm_0044",
              "question": "5.2 当日语提示长度膨胀 1.5× 时，如何动态调整 max_tokens 防止截断？"
            },
            {
              "id": "llm_0045",
              "question": "5.3 如何构建自动化测试，确保 12 种语言在相同提示下输出格式一致？"
            },
            {
              "id": "llm_0046",
              "question": "1.1 如何结合 BM25 与 Contriever 分数做加权融合并学习权重 α？"
            },
            {
              "id": "llm_0047",
              "question": "1.3 如何基于用户点击反馈在线微调双塔模型并避免灾难性遗忘？"
            },
            {
              "id": "llm_0048",
              "question": "2.2 当候选段落长度差异 10× 时，如何采用动态 padding 提升 batch 吞吐？"
            },
            {
              "id": "llm_0049",
              "question": "3.1 如何采用滑动窗口摘要将 20k 检索结果压缩至 4k token？"
            },
            {
              "id": "llm_0050",
              "question": "3.3 如何基于强化学习奖励模型自动选择最优窗口数？"
            },
            {
              "id": "llm_0051",
              "question": "4.1 如何采用 HNSW 的增量插入算法并设置 efConstruction 保证召回？"
            },
            {
              "id": "llm_0052",
              "question": "4.2 当每天新增 100 万篇文档时，如何设计分层索引避免全量重建？"
            },
            {
              "id": "llm_0053",
              "question": "4.3 如何用 Kafka+FAISS 实现近实时（<5min）索引更新？"
            },
            {
              "id": "llm_0054",
              "question": "5.1 如何对图片提取 OCR+布局信息并生成统一向量表示？"
            },
            {
              "id": "llm_0055",
              "question": "5.2 当查询为“对比去年财报净利润”时，如何自动定位表格并抽取对应单元格？"
            },
            {
              "id": "llm_0056",
              "question": "5.3 如何评估多模态检索结果的相关性并构建人工评估协议？"
            },
            {
              "id": "llm_0057",
              "question": "1.1 如何基于敏感度指标选择 8bit 量化哪些通道并给出公式？"
            },
            {
              "id": "llm_0058",
              "question": "1.2 当序列长度 64k 时，如何采用滑动窗口 KV-cache 节省显存？"
            },
            {
              "id": "llm_0059",
              "question": "1.3 如何证明 KV-cache INT4 量化对 PPL 影响 <2% 并提供实验数据？"
            },
            {
              "id": "llm_0060",
              "question": "2.1 如何设计迭代级调度算法并计算平均吞吐提升？"
            },
            {
              "id": "llm_0061",
              "question": "2.2 当 prefill 与 decode 阶段计算密度差异 10× 时，如何分离调度？"
            },
            {
              "id": "llm_0062",
              "question": "2.3 如何用模拟器预估在 8×A100 下最大并发请求数？"
            },
            {
              "id": "llm_0063",
              "question": "3.1 如何基于 n-gram 频率训练小型草案模型并设置接受阈值？"
            },
            {
              "id": "llm_0064",
              "question": "3.2 当草案模型接受率 <60% 时，如何动态切换回自回归？"
            },
            {
              "id": "llm_0065",
              "question": "3.3 如何评估投机解码在长文本生成场景下的端到端加速比？"
            },
            {
              "id": "llm_0066",
              "question": "4.1 如何用 Megatron-LM 计算 175B 模型在 128GPU 下的最优 pp×tp×dp 组合？"
            },
            {
              "id": "llm_0067",
              "question": "4.2 当遇到层间激活内存爆炸时，如何启用 selective activation recomputation？"
            },
            {
              "id": "llm_0068",
              "question": "4.3 如何基于 profiling 结果自动调整 micro-batch 数并减少气泡？"
            },
            {
              "id": "llm_0069",
              "question": "5.1 如何基于 AIMET 对 Transformer 层进行权重量化并校准？"
            },
            {
              "id": "llm_0070",
              "question": "5.2 当 ARM Cortex-A78 只有 4GB RAM 时，如何采用内存映射加载 7B 模型？"
            },
            {
              "id": "llm_0071",
              "question": "5.3 如何设计 NEON 汇编优化 GELU 算子并提升 1.7× 速度？"
            },
            {
              "id": "llm_0072",
              "question": "1.1 如何定义流式 SSE 响应格式并兼容 OpenAI API 语义？"
            },
            {
              "id": "llm_0073",
              "question": "1.2 当返回字段 >100 个时，如何用 protobuf FieldMask 实现按需返回？"
            },
            {
              "id": "llm_0074",
              "question": "1.3 如何设计幂等性机制防止重复扣费？"
            },
            {
              "id": "llm_0075",
              "question": "2.1 如何基于 Triton Inference Server 的 Metrics 自定义 QPS 阈值？"
            },
            {
              "id": "llm_0076",
              "question": "2.2 当模型容器冷启动需要 45s 时，如何采用预拉取与池化降到 <5s？"
            },
            {
              "id": "llm_0077",
              "question": "2.3 如何评估 GPU 利用率 50% 时的成本最优副本数？"
            },
            {
              "id": "llm_0078",
              "question": "3.1 如何用 MIG（Multi-Instance GPU）切分 A100 并保证显存隔离？"
            },
            {
              "id": "llm_0079",
              "question": "3.2 当某租户触发 OOM 时，如何自动熔断而不影响其他租户？"
            },
            {
              "id": "llm_0080",
              "question": "3.3 如何基于 cgroups v2 对 GPU 时间片做公平调度？"
            },
            {
              "id": "llm_0081",
              "question": "4.1 如何采用 Guided Decoding 强制输出符合 JSON Schema？"
            },
            {
              "id": "llm_0082",
              "question": "4.2 当模型违反格式时，如何基于重试策略保证 99.9% 可用？"
            },
            {
              "id": "llm_0083",
              "question": "4.3 如何设计单元测试自动校验返回字段类型与范围？"
            },
            {
              "id": "llm_0084",
              "question": "5.1 如何基于 OpenTelemetry 采集 token 级延迟并上报 Prometheus？"
            },
            {
              "id": "llm_0085",
              "question": "5.2 当面对千万级日志时，如何用 Loki 索引并保留 30 天冷热分层？"
            },
            {
              "id": "llm_0086",
              "question": "5.3 如何构建 trace-id 串联 Prompt→Model→Response 全链路？"
            },
            {
              "id": "llm_0087",
              "question": "1.1 如何用 KL 散度监控输出分布漂移并设置动态阈值？"
            },
            {
              "id": "llm_0088",
              "question": "1.2 当输入主题分布变化时，如何采用 LDA 主题模型检测 Covariate Shift？"
            },
            {
              "id": "llm_0089",
              "question": "1.3 如何构建自动告警并触发回滚或重训？"
            },
            {
              "id": "llm_0090",
              "question": "2.1 如何基于知识库对比生成声明并计算事实一致性得分？"
            },
            {
              "id": "llm_0091",
              "question": "2.2 当检测延迟要求 <200ms 时，如何采用局部敏感哈希快速匹配？"
            },
            {
              "id": "llm_0092",
              "question": "2.3 如何设计用户举报闭环，将错误样本回流到微调集？"
            },
            {
              "id": "llm_0093",
              "question": "3.1 如何用贝叶斯序贯检验减少实验样本量并提前收敛？"
            },
            {
              "id": "llm_0094",
              "question": "3.2 当指标出现 Saturday Effect 时，如何采用 CUPED 方差缩减？"
            },
            {
              "id": "llm_0095",
              "question": "3.3 如何设计自动切量脚本，支持按用户尾号灰度？"
            },
            {
              "id": "llm_0096",
              "question": "4.1 如何用 GitHub Actions 触发数据→训练→评估→部署全链路？"
            },
            {
              "id": "llm_0097",
              "question": "4.2 当评估指标下降 >3% 时，如何自动阻断部署并创建 Issue？"
            },
            {
              "id": "llm_0098",
              "question": "4.3 如何对 100GB 训练镜像做分层构建并缓存 pip 依赖？"
            },
            {
              "id": "llm_0099",
              "question": "5.1 如何记录每次推理的输入、输出、模型版本并满足 GDPR 可删除要求？"
            },
            {
              "id": "llm_0100",
              "question": "5.2 当监管部门要求出具算法备案表时，如何一键导出训练数据来源？"
            },
            {
              "id": "llm_0101",
              "question": "5.3 如何用 Merkle Tree 对模型权重做完整性校验并防止篡改？"
            },
            {
              "id": "llm_0102",
              "question": "1.2 当攻击采用 Base64 编码混淆时，如何构建多阶段解码检测？"
            },
            {
              "id": "llm_0103",
              "question": "1.3 如何采用强化学习对抗训练提升模型鲁棒性？"
            },
            {
              "id": "llm_0104",
              "question": "2.1 如何构建多语言敏感分类器并保证 F1>95%？"
            },
            {
              "id": "llm_0105",
              "question": "2.2 当模型拒绝回答导致业务下降时，如何设置白名单豁免？"
            },
            {
              "id": "llm_0106",
              "question": "2.3 如何基于日志审计追踪违规内容并定位责任人？"
            },
            {
              "id": "llm_0107",
              "question": "3.1 如何用 Intel SGX 进行可信执行环境推理并降低 10% 延迟？"
            },
            {
              "id": "llm_0108",
              "question": "3.2 当模型需分发到边缘设备时，如何采用 AES-CTR 流式解密？"
            },
            {
              "id": "llm_0109",
              "question": "3.3 如何基于数字水印追踪泄露权重来源？"
            },
            {
              "id": "llm_0110",
              "question": "4.1 如何基于 CRF 模型识别中文姓名、手机号、身份证并替换？"
            },
            {
              "id": "llm_0111",
              "question": "4.2 当脱敏导致下游任务下降 2% 时，如何采用对抗训练恢复？"
            },
            {
              "id": "llm_0112",
              "question": "4.3 如何构建差分隐私（ε=1）保证统计查询不泄露个体？"
            },
            {
              "id": "llm_0113",
              "question": "5.1 如何采用对抗指纹在 token 分布中嵌入不可见签名？"
            },
            {
              "id": "llm_0114",
              "question": "5.2 当检测到 80% 指纹匹配时，如何出具法律认可的报告？"
            },
            {
              "id": "llm_0115",
              "question": "5.3 如何设计区块链存证合约并保证上链哈希不可篡改？"
            },
            {
              "id": "llm_0116",
              "question": "1.1 如何将 224×224 图像映射为 256 个视觉 token 并保证文本占比 60%？"
            },
            {
              "id": "llm_0117",
              "question": "1.2 当图像分辨率提升至 896×896 时，如何采用自适应切块减少 30% token？"
            },
            {
              "id": "llm_0118",
              "question": "1.3 如何基于强化学习奖励自动决定图文 token 比例？"
            },
            {
              "id": "llm_0119",
              "question": "2.1 如何采用 VAD 切分语音流并设置缓冲窗口降低延迟？"
            },
            {
              "id": "llm_0120",
              "question": "2.2 当采样率 16kHz 时，如何估算 Whisper-large-v3 的 RTF 并优化到 <0.3？"
            },
            {
              "id": "llm_0121",
              "question": "2.3 如何基于 WebRTC 实现双工语音对话并处理回声消除？"
            },
            {
              "id": "llm_0122",
              "question": "3.1 如何采用 CLIP 相似度聚类抽取 10 个关键帧并覆盖 90% 内容？"
            },
            {
              "id": "llm_0123",
              "question": "3.2 当视频时长 2 小时时，如何采用层次化摘要生成 200 字简述？"
            },
            {
              "id": "llm_0124",
              "question": "3.3 如何评估摘要的 ROUGE-L 与人工一致性（κ 值）？"
            },
            {
              "id": "llm_0125",
              "question": "4.2 当注意力热图分散时，如何采用稀疏正则提升可解释性？"
            },
            {
              "id": "llm_0126",
              "question": "4.3 如何构建前端组件支持用户交互式查看注意力权重？"
            },
            {
              "id": "llm_0127",
              "question": "5.1 如何基于知识蒸馏将 7B 多模态模型压缩到 1B 并保持 VQA 准确率 90%？"
            },
            {
              "id": "llm_0128",
              "question": "5.2 当视觉编码器占 70% 计算时，如何采用 MobileViT 替换并微调？"
            },
            {
              "id": "llm_0129",
              "question": "5.3 如何用 NNAPI 在 Android SoC 上加速 INT8 推理并降低功耗 40%？"
            },
            {
              "id": "llm_0130",
              "question": "1.1 如何基于 Pydantic 模型自动生成 OpenAI 兼容的 function 描述？"
            },
            {
              "id": "llm_0131",
              "question": "1.2 当函数参数 >50 个时，如何采用分组嵌套减少 token 消耗？"
            },
            {
              "id": "llm_0132",
              "question": "1.3 如何验证模型生成参数在合法范围内并给出错误提示？"
            },
            {
              "id": "llm_0133",
              "question": "2.1 如何设置最大迭代次数并防止无限循环？"
            },
            {
              "id": "llm_0134",
              "question": "2.2 当工具返回空结果时，如何采用候补 API 保证任务继续？"
            },
            {
              "id": "llm_0135",
              "question": "2.3 如何记录失败轨迹并用于后续微调提升成功率？"
            },
            {
              "id": "llm_0136",
              "question": "3.1 如何用 Python importlib 实现插件运行时加载并隔离命名空间？"
            },
            {
              "id": "llm_0137",
              "question": "3.2 当插件崩溃时，如何采用沙箱进程防止主服务宕机？"
            },
            {
              "id": "llm_0138",
              "question": "3.3 如何设计插件签名验证并防止恶意代码执行？"
            },
            {
              "id": "llm_0139",
              "question": "4.1 如何基于 Redis + Protobuf 缓存天气查询结果并设置 TTL？"
            },
            {
              "id": "llm_0140",
              "question": "4.2 当缓存命中率 >90% 时，如何评估对整体延迟的提升？"
            },
            {
              "id": "llm_0141",
              "question": "4.3 如何采用一致性哈希做分布式缓存并防止热点倾斜？"
            },
            {
              "id": "llm_0142",
              "question": "5.1 如何构建中文购物场景任务并定义成功率指标？"
            },
            {
              "id": "llm_0143",
              "question": "5.2 当 Agent 采用不同模型后端时，如何归一化打分？"
            },
            {
              "id": "llm_0144",
              "question": "5.3 如何开源评估工具并支持社区提交新任务？"
            },
            {
              "id": "llm_0145",
              "question": "1.1 如何计算 CodeBLEU 的语法权重并对比 BLEU？"
            },
            {
              "id": "llm_0146",
              "question": "1.2 当 k=10 时，如何用缓存执行结果加速 Pass@k 评估？"
            },
            {
              "id": "llm_0147",
              "question": "1.3 如何构建单元测试集并保证覆盖常见边界条件？"
            },
            {
              "id": "llm_0148",
              "question": "2.1 如何设计跨语言基准并验证算法逻辑等价？"
            },
            {
              "id": "llm_0149",
              "question": "2.2 当生成 Go 代码缺少 error handling 时，如何基于 AST 自动修复？"
            },
            {
              "id": "llm_0150",
              "question": "2.3 如何采用语法检查器实时拦截编译错误并反馈？"
            },
            {
              "id": "llm_0151",
              "question": "3.1 如何构建对抗测试集检测 SQL 注入风险？"
            },
            {
              "id": "llm_0152",
              "question": "3.2 当模型输出动态 SQL 时，如何采用参数化查询重写？"
            },
            {
              "id": "llm_0153",
              "question": "3.3 如何基于静态分析工具（Bandit）扫描生成代码并评分？"
            },
            {
              "id": "llm_0154",
              "question": "4.1 如何基于抽象语法树拆分函数并逐块生成注释？"
            },
            {
              "id": "llm_0155",
              "question": "4.2 当注释长度限制 80 字时，如何采用指针生成网络压缩？"
            },
            {
              "id": "llm_0156",
              "question": "4.3 如何评估生成注释的 ROUGE-L 与开发者人工一致性？"
            },
            {
              "id": "llm_0157",
              "question": "5.1 如何用投机解码将代码补全延迟降到 <100ms？"
            },
            {
              "id": "llm_0158",
              "question": "5.2 当上下文长度 8k 时，如何采用局部注意力减少计算？"
            },
            {
              "id": "llm_0159",
              "question": "5.3 如何基于用户接受率在线微调小模型提升草案命中率？"
            },
            {
              "id": "llm_0160",
              "question": "1.1 如何基于 SymPy 验证等式推导并给出自动评分？"
            },
            {
              "id": "llm_0161",
              "question": "1.2 当模型使用错误公式时，如何构建错误模式库并反馈？"
            },
            {
              "id": "llm_0162",
              "question": "1.3 如何采用课程学习逐步提升题目难度？"
            },
            {
              "id": "llm_0163",
              "question": "2.1 如何采用 Self-Consistency 采样多数投票提升准确率？"
            },
            {
              "id": "llm_0164",
              "question": "2.3 如何构建中文符号逻辑数据集并开源？"
            },
            {
              "id": "llm_0165",
              "question": "3.1 如何将几何问题转化为文本并保证信息不丢失？"
            },
            {
              "id": "llm_0166",
              "question": "3.2 当模型生成错误坐标时，如何基于几何约束自动纠错？"
            },
            {
              "id": "llm_0167",
              "question": "3.3 如何采用 SVG 渲染并验证图形正确性？"
            },
            {
              "id": "llm_0168",
              "question": "4.1 如何自动生成 t 检验步骤并解释 p 值含义？"
            },
            {
              "id": "llm_0169",
              "question": "4.2 当样本量 n<30 时，如何提示模型使用正态近似 vs 精确分布？"
            },
            {
              "id": "llm_0170",
              "question": "4.3 如何评估生成解释的教学有效性（学生满意度）？"
            },
            {
              "id": "llm_0171",
              "question": "5.1 如何基于子问题分解并验证每步中间结果？"
            },
            {
              "id": "llm_0172",
              "question": "5.2 当推理链长度 >20 时，如何采用循环校验防止矛盾？"
            },
            {
              "id": "llm_0173",
              "question": "5.3 如何构建自动评分器并给出可解释报告？"
            },
            {
              "id": "llm_0174",
              "question": "1.1 如何用 RoPE 的 PI（Position Interpolation）将 4k 模型扩展到 32k？"
            },
            {
              "id": "llm_0175",
              "question": "1.2 当扩展后 PPL 上升 15% 时，如何采用课程长度微调恢复？"
            },
            {
              "id": "llm_0176",
              "question": "1.3 如何评估长上下文检索召回率（LoongEval）？"
            },
            {
              "id": "llm_0177",
              "question": "2.1 如何基于重要性得分将 90% 旧 token 转存到向量库？"
            },
            {
              "id": "llm_0178",
              "question": "2.2 当向量检索延迟 >500ms 时，如何采用 LRU 缓存优化？"
            },
            {
              "id": "llm_0179",
              "question": "2.3 如何验证记忆机制对多轮对话一致性提升？"
            },
            {
              "id": "llm_0180",
              "question": "3.1 如何采用金字塔方法（Pyramid Method）评估摘要事实覆盖率？"
            },
            {
              "id": "llm_0181",
              "question": "3.3 如何构建中文 100 篇超长（>50k 字）摘要数据集？"
            },
            {
              "id": "llm_0182",
              "question": "4.1 如何采用强化学习策略决定压缩时机与长度？"
            },
            {
              "id": "llm_0183",
              "question": "4.3 如何基于用户反馈在线微调压缩模型？"
            },
            {
              "id": "llm_0184",
              "question": "5.1 如何用 Ring Attention 将序列拆分到 64 卡并计算通信开销？"
            },
            {
              "id": "llm_0185",
              "question": "5.2 当序列长度 128k 时，如何采用 FlashAttention-2 减少显存？"
            },
            {
              "id": "llm_0186",
              "question": "5.3 如何评估并行方案对训练吞吐的提升？"
            },
            {
              "id": "llm_0187",
              "question": "1.1 如何采用 Adapter 层仅训练 1% 参数并保持多语言能力？"
            },
            {
              "id": "llm_0188",
              "question": "1.2 当训练数据 <10M token 时，如何采用回译+自举扩增？"
            },
            {
              "id": "llm_0189",
              "question": "1.3 如何构建多语言评测基准并避免文化偏见？"
            },
            {
              "id": "llm_0190",
              "question": "2.1 如何采用平行三元组验证中英文知识冲突？"
            },
            {
              "id": "llm_0191",
              "question": "2.3 如何设计用户界面展示冲突并收集反馈？"
            },
            {
              "id": "llm_0192",
              "question": "3.1 如何基于 ICU 库自动转换并缓存格式模板？"
            },
            {
              "id": "llm_0193",
              "question": "3.2 当模型输出非标准格式时，如何采用正则+规则后处理？"
            },
            {
              "id": "llm_0194",
              "question": "3.3 如何单元测试覆盖 200 个地区格式？"
            },
            {
              "id": "llm_0195",
              "question": "4.1 如何采用 fribidi 库处理阿拉伯语双向文本？"
            },
            {
              "id": "llm_0196",
              "question": "4.2 当生成 HTML 时，如何自动添加 dir=\"rtl\" 属性？"
            },
            {
              "id": "llm_0197",
              "question": "4.3 如何评估 RTL 语言在生成摘要时的可读性？"
            },
            {
              "id": "llm_0198",
              "question": "5.1 如何用 multilingual sentence encoder 对齐意图向量？"
            },
            {
              "id": "llm_0199",
              "question": "5.2 当低资源语言意图缺失时，如何采用元学习快速适应？"
            },
            {
              "id": "llm_0200",
              "question": "5.3 如何构建跨语言意图混淆矩阵并优化？"
            },
            {
              "id": "llm_0201",
              "question": "1.1 如何采用 Kendall Tau 相关性验证 RM 与人类一致性？"
            },
            {
              "id": "llm_0202",
              "question": "1.2 当 RM 在训练集准确率 98% 但验证集 85% 时，如何早停？"
            },
            {
              "id": "llm_0203",
              "question": "1.3 如何采用 dropout ensemble 估计奖励不确定性？"
            },
            {
              "id": "llm_0204",
              "question": "2.1 如何设置自适应 KL 惩罚系数并给出更新公式？"
            },
            {
              "id": "llm_0205",
              "question": "2.2 当 KL 散度 >0.2 时，如何采用线性退火降低学习率？"
            },
            {
              "id": "llm_0206",
              "question": "2.3 如何监控 entropy 崩溃并自动重启训练？"
            },
            {
              "id": "llm_0207",
              "question": "3.1 如何设计双盲对比界面并防止标注者疲劳？"
            },
            {
              "id": "llm_0208",
              "question": "3.2 当标注者一致性 κ<0.6 时，如何采用专家仲裁？"
            },
            {
              "id": "llm_0209",
              "question": "3.3 如何基于主动学习优先选择最不确定样本？"
            },
            {
              "id": "llm_0210",
              "question": "4.1 如何采用多奖励加权并动态调整权重？"
            },
            {
              "id": "llm_0211",
              "question": "4.2 当目标冲突时，如何采用 Pareto 最优前沿选择策略？"
            },
            {
              "id": "llm_0212",
              "question": "4.3 如何可视化三维奖励空间并帮助决策？"
            },
            {
              "id": "llm_0213",
              "question": "5.1 如何用用户实时反馈构建滚动偏好对？"
            },
            {
              "id": "llm_0214",
              "question": "5.2 当数据流非平稳时，如何采用经验回放加权？"
            },
            {
              "id": "llm_0215",
              "question": "5.3 如何评估在线 RLHF 对留存率的提升？"
            },
            {
              "id": "llm_0216",
              "question": "1.1 如何采用 MiniLM 深度蒸馏并匹配隐层注意力？"
            },
            {
              "id": "llm_0217",
              "question": "1.2 当学生模型仅 0.5B 时，如何设计分层蒸馏目标？"
            },
            {
              "id": "llm_0218",
              "question": "1.3 如何评估蒸馏后在下游任务上的保留率？"
            },
            {
              "id": "llm_0219",
              "question": "2.1 如何设置稀疏度调度从 0% 到 90% 并保证收敛？"
            },
            {
              "id": "llm_0220",
              "question": "2.2 当遇到稀疏算子不支持时，如何采用稀疏-稠密混合计算？"
            },
            {
              "id": "llm_0221",
              "question": "2.3 如何基于 NVIDIA Sparsity SDK 加速 2:4 结构化稀疏？"
            },
            {
              "id": "llm_0222",
              "question": "3.1 如何在 Transformer 层插入 FakeQuant 节点并校准？"
            },
            {
              "id": "llm_0223",
              "question": "3.2 当量化到 INT4 时，如何采用直通估计（STE）缓解梯度不匹配？"
            },
            {
              "id": "llm_0224",
              "question": "3.3 如何评估 QAT 与 PTQ 的精度-耗时权衡？"
            },
            {
              "id": "llm_0225",
              "question": "4.1 如何对 FFN 权重进行 SVD 分解并选择秩？"
            },
            {
              "id": "llm_0226",
              "question": "4.2 当压缩比 8× 时，如何采用微调恢复 98% 准确率？"
            },
            {
              "id": "llm_0227",
              "question": "4.3 如何基于 Tucker 分解进一步压缩多头注意力？"
            },
            {
              "id": "llm_0228",
              "question": "5.1 如何用 TensorFlow Lite 转换并构建 metadata？"
            },
            {
              "id": "llm_0229",
              "question": "5.2 当模型文件 >2GB 时，如何采用分片下载并校验 SHA256？"
            },
            {
              "id": "llm_0230",
              "question": "5.3 如何基于 OTA 差分升级并降低 80% 流量？"
            },
            {
              "id": "llm_0231",
              "question": "1.1 如何基于题目难度参数（IRT）过滤低区分度题？"
            },
            {
              "id": "llm_0232",
              "question": "1.2 当评测集泄露时，如何采用对抗重写保持语义？"
            },
            {
              "id": "llm_0233",
              "question": "1.3 如何构建领域子集并报告置信区间？"
            },
            {
              "id": "llm_0234",
              "question": "2.1 如何用 SHAP 解释分类结果并可视化 top token？"
            },
            {
              "id": "llm_0235",
              "question": "2.2 当解释不稳定时，如何采用集成梯度平滑？"
            },
            {
              "id": "llm_0236",
              "question": "2.3 如何评估解释与人类标注的一致性？"
            },
            {
              "id": "llm_0237",
              "question": "3.1 如何采用 BERT 嵌入聚类并自动标注错误类型？"
            },
            {
              "id": "llm_0238",
              "question": "3.2 当聚类纯度 <0.7 时，如何采用半监督 refine？"
            },
            {
              "id": "llm_0239",
              "question": "3.3 如何构建可视化看板帮助开发者定位？"
            },
            {
              "id": "llm_0240",
              "question": "4.1 如何构建中文偏见提示并测量不同群体回答差异？"
            },
            {
              "id": "llm_0241",
              "question": "4.2 当差异 >10% 时，如何采用数据重采样缓解？"
            },
            {
              "id": "llm_0242",
              "question": "4.3 如何生成公平性报告并提交监管？"
            },
            {
              "id": "llm_0243",
              "question": "5.1 如何用 JSON 模板记录模型架构、数据、评测结果？"
            },
            {
              "id": "llm_0244",
              "question": "5.2 当字段 >200 时，如何采用 LLM 自动生成自然语言描述？"
            },
            {
              "id": "llm_0245",
              "question": "5.3 如何基于 CI 自动更新模型卡片并发布？"
            },
            {
              "id": "llm_0246",
              "question": "1.1 如何用模型生成候选标注并计算人工节省率？"
            },
            {
              "id": "llm_0247",
              "question": "1.2 当预标注准确率 80% 时，如何采用不确定性采样送标？"
            },
            {
              "id": "llm_0248",
              "question": "1.3 如何基于 Inter-Annotator Agreement 动态调整送标策略？"
            },
            {
              "id": "llm_0249",
              "question": "2.1 如何采用交叉验证检测标注者系统偏差？"
            },
            {
              "id": "llm_0250",
              "question": "2.2 当抽检比例 5% 时，如何计算统计功效并保证 95% 置信？"
            },
            {
              "id": "llm_0251",
              "question": "2.3 如何用模型投票发现潜在错误标注？"
            },
            {
              "id": "llm_0252",
              "question": "3.1 如何设计 20 条 LF（Labeling Function）并估计准确率？"
            },
            {
              "id": "llm_0253",
              "question": "3.2 当 LF 冲突时，如何采用多数投票加权重？"
            },
            {
              "id": "llm_0254",
              "question": "3.3 如何评估弱监督标签对微调效果影响？"
            },
            {
              "id": "llm_0255",
              "question": "4.1 如何采用熵+多样性混合策略选择样本？"
            },
            {
              "id": "llm_0256",
              "question": "4.2 当预算仅允许标注 1000 条时，如何采用批量选择？"
            },
            {
              "id": "llm_0257",
              "question": "4.3 如何可视化选择结果并解释？"
            },
            {
              "id": "llm_0258",
              "question": "5.1 如何基于 RL 动态调整标注单价并激励质量？"
            },
            {
              "id": "llm_0259",
              "question": "5.2 当标注错误导致重标时，如何采用惩罚机制？"
            },
            {
              "id": "llm_0260",
              "question": "5.3 如何计算单条标注的 ROI 并优化？"
            },
            {
              "id": "llm_0261",
              "question": "1.1 如何构建 GitHub 监控机器人并自动汇总 release note？"
            },
            {
              "id": "llm_0262",
              "question": "1.2 当权重格式从 FP16 转为 BF16 时，如何评估对精度的影响？"
            },
            {
              "id": "llm_0263",
              "question": "1.3 如何基于 Docker 多版本镜像快速切换对比？"
            },
            {
              "id": "llm_0264",
              "question": "2.1 如何用 arxiv-sanity 筛选每日相关论文并打标签？"
            },
            {
              "id": "llm_0265",
              "question": "2.2 当复现 LLaMA 3 时，如何估算 8B 模型在 2k GPU 小时内的成本？"
            },
            {
              "id": "llm_0266",
              "question": "2.3 如何开源复现代码并吸引社区贡献？"
            },
            {
              "id": "llm_0267",
              "question": "3.1 如何编写 CONTRIBUTING.md 并设置 CI 检查？"
            },
            {
              "id": "llm_0268",
              "question": "3.2 当 PR 冲突时，如何采用 rebase 保持线性历史？"
            },
            {
              "id": "llm_0269",
              "question": "3.3 如何用 allcontributors 机器人自动识别贡献者？"
            },
            {
              "id": "llm_0270",
              "question": "4.1 如何基于 Google Trends 和论文投稿量预测 MoE 热度？"
            },
            {
              "id": "llm_0271",
              "question": "4.2 当规划 2025 技术路线时，如何采用德尔菲法收集专家意见？"
            },
            {
              "id": "llm_0272",
              "question": "4.3 如何将路线图可视化并定期 review？"
            },
            {
              "id": "llm_0273",
              "question": "5.1 如何构建专利检索关键词并监控竞争对手？"
            },
            {
              "id": "llm_0274",
              "question": "5.2 当发现潜在侵权时，如何采用专利规避设计？"
            },
            {
              "id": "llm_0275",
              "question": "5.3 如何撰写中国 AI 专利并加快审查？"
            },
            {
              "id": "llm_0276",
              "question": "1.1 如何在客服场景选择“问题解决率”而非“点击率”作为北极星？"
            },
            {
              "id": "llm_0277",
              "question": "1.2 当指标滞后 7 天时，如何采用代理指标实时指导？"
            },
            {
              "id": "llm_0278",
              "question": "1.3 如何用 North-Star Framework 对齐团队目标？"
            },
            {
              "id": "llm_0279",
              "question": "2.1 如何量化模型替代人工客服节省的 FTE 并折算年费？"
            },
            {
              "id": "llm_0280",
              "question": "2.2 当 GPU 成本上升 20% 时，如何计算 ROI 盈亏平衡点？"
            },
            {
              "id": "llm_0281",
              "question": "2.3 如何用 Monte Carlo 模拟 ROI 分布并给出 95% 置信区间？"
            },
            {
              "id": "llm_0282",
              "question": "3.1 如何设计双盲实验避免诱导性问卷？"
            },
            {
              "id": "llm_0283",
              "question": "3.2 当 NPS 提升 10 分时，如何验证与模型优化的因果关系？"
            },
            {
              "id": "llm_0284",
              "question": "3.3 如何细分人群（年龄、地域）分析 NPS 差异？"
            },
            {
              "id": "llm_0285",
              "question": "4.1 如何针对模型输出违规设计 15 分钟响应流程？"
            },
            {
              "id": "llm_0286",
              "question": "4.2 当服务宕机时，如何自动降级到规则客服并保证 SLA？"
            },
            {
              "id": "llm_0287",
              "question": "4.3 如何每季度演练并更新预案？"
            },
            {
              "id": "llm_0288",
              "question": "5.1 如何用 OKR 设置季度模型优化目标？"
            },
            {
              "id": "llm_0289",
              "question": "5.2 当竞品上线新功能时，如何采用 Sprint 快速跟进？"
            },
            {
              "id": "llm_0290",
              "question": "5.3 如何基于用户反馈优先级排序并 roadmap 可视化？"
            },
            {
              "id": "llm_0291",
              "question": "Qwen-72B 的推理延迟与首 token 时间？"
            },
            {
              "id": "llm_0292",
              "question": "义“延迟”？"
            },
            {
              "id": "llm_0293",
              "question": "如何复现？"
            },
            {
              "id": "llm_0294",
              "question": "化到 INT4 后的性能损失？"
            },
            {
              "id": "llm_0295",
              "question": "给出权重设计示例？"
            },
            {
              "id": "llm_0296",
              "question": "面试尾声可反问面试官：“贵公司当前毛利率与预算增速是多少？"
            },
            {
              "id": "llm_0297",
              "question": "典型取值区间，否则会被追问“你这些数从哪来的？"
            },
            {
              "id": "llm_0298",
              "question": "混合云架构能否降低合规溢价？"
            },
            {
              "id": "llm_0299",
              "question": "如何计算？"
            },
            {
              "id": "llm_0300",
              "question": "请求；？"
            },
            {
              "id": "llm_0301",
              "question": "迟的影响如何建模？"
            },
            {
              "id": "llm_0302",
              "question": "具体条款？"
            },
            {
              "id": "llm_0303",
              "question": "权重冲突？"
            },
            {
              "id": "llm_0304",
              "question": "明“独立且可分离”以避免传染？"
            },
            {
              "id": "llm_0305",
              "question": "小信息集？"
            },
            {
              "id": "llm_0306",
              "question": "令 A、B，让同一位标注者同时阅读，强制二选一：哪条指令更难？"
            },
            {
              "id": "llm_0307",
              "question": "完成近似去重？"
            },
            {
              "id": "llm_0308",
              "question": "面试官可能追问 “为什么不用 SimHash+Hamming LSH？"
            },
            {
              "id": "llm_0309",
              "question": "量更新策略？"
            },
            {
              "id": "llm_0310",
              "question": "做敏感实体识别？"
            },
            {
              "id": "llm_0311",
              "question": "解释的工业级方案，而不是只背论文。？"
            },
            {
              "id": "llm_0312",
              "question": "在多模态对话里，图片 token 的熵如何定义？"
            },
            {
              "id": "llm_0313",
              "question": "要损失对下游任务影响 <2%？"
            },
            {
              "id": "llm_0314",
              "question": "息不丢失？"
            },
            {
              "id": "llm_0315",
              "question": "量与多样性阈值？"
            },
            {
              "id": "llm_0316",
              "question": "如果院方只给10 条私有种子，如何快速扩增？"
            },
            {
              "id": "llm_0317",
              "question": "多样性阈值能否动态调？"
            },
            {
              "id": "llm_0318",
              "question": "如何与推理加速联动？"
            },
            {
              "id": "llm_0319",
              "question": "制 95% 置信区间？"
            },
            {
              "id": "llm_0320",
              "question": "滚到任意版本？"
            },
            {
              "id": "llm_0321",
              "question": "如何对齐流与批的事件时间窗口？"
            },
            {
              "id": "llm_0322",
              "question": "Learning）+Experience Replay，把“必须重训”降级为“增量合并”？"
            },
            {
              "id": "llm_0323",
              "question": "广播与优先级队列，避免GPU资源死锁？"
            },
            {
              "id": "llm_0324",
              "question": "增量参数量？"
            },
            {
              "id": "llm_0325",
              "question": "100 下的最小显存占用？"
            },
            {
              "id": "llm_0326",
              "question": "批次累积？"
            },
            {
              "id": "llm_0327",
              "question": "系数防止过拟合？"
            },
            {
              "id": "llm_0328",
              "question": "metheus+Grafana 看板？"
            },
            {
              "id": "llm_0329",
              "question": "Stopping？"
            },
            {
              "id": "llm_0330",
              "question": "任务间共享底层 Transformer 时，如何防止某任务 loss 爆炸拖垮整体？"
            },
            {
              "id": "llm_0331",
              "question": "国内机房夜间断网演练导致监控中心失联，如何防止误停？"
            },
            {
              "id": "llm_0332",
              "question": "千亿模型 ckpt 200 GB，网络拷贝耗时 10 分钟，如何做到秒级回滚？"
            },
            {
              "id": "llm_0333",
              "question": "PT 与 SFT 的 epoch 比例？"
            },
            {
              "id": "llm_0334",
              "question": "练”的幻觉率差异？"
            },
            {
              "id": "llm_0335",
              "question": "kenizer 并保证嵌入层初始化？"
            },
            {
              "id": "llm_0336",
              "question": "权重矩阵并加入损失？"
            },
            {
              "id": "llm_0337",
              "question": "并设置 buffer 大小？"
            },
            {
              "id": "llm_0338",
              "question": "如何量化遗忘并触发回放；？"
            },
            {
              "id": "llm_0339",
              "question": "如何构建轻量级 rehearsal buffer（国内常受限于 GPU 配额与数据合？"
            },
            {
              "id": "llm_0340",
              "question": "如何在线动态调整 buffer 大小，兼顾成本与效果。？"
            },
            {
              "id": "llm_0341",
              "question": "并绘制边际收益曲线？"
            },
            {
              "id": "llm_0342",
              "question": "进行稳定性筛选？"
            },
            {
              "id": "llm_0343",
              "question": "失准确率？"
            },
            {
              "id": "llm_0344",
              "question": "现在 假设 扮演 你是)\\s*(一名？"
            },
            {
              "id": "llm_0345",
              "question": ")\\s*(\\S{0,6}？"
            },
            {
              "id": "llm_0346",
              "question": "无道德 无约束 无过滤|不遵守|忽略.*？"
            },
            {
              "id": "llm_0347",
              "question": "(系统|用户|助手)？"
            },
            {
              "id": "llm_0348",
              "question": "设计拒绝模板？"
            },
            {
              "id": "llm_0349",
              "question": "动升级风控？"
            },
            {
              "id": "llm_0350",
              "question": "lue）？"
            },
            {
              "id": "llm_0351",
              "question": "的语义差距？"
            },
            {
              "id": "llm_0352",
              "question": "防止截断？"
            },
            {
              "id": "llm_0353",
              "question": "请求费用 ≤ 用户套餐剩余额度，否则降级到 流式摘要 方案。？"
            },
            {
              "id": "llm_0354",
              "question": "格式一致？"
            },
            {
              "id": "llm_0355",
              "question": "uery Expansion）提升？"
            },
            {
              "id": "llm_0356",
              "question": "模态，文本塔用 LoRA，ID 塔用 HashTable 查表，实现分钟级热插拔？"
            },
            {
              "id": "llm_0357",
              "question": "分片式 Fisher 信息矩阵 才能不爆内存？"
            },
            {
              "id": "llm_0358",
              "question": "报告，如何在 LoRA 增量权重上 加噪 才能既通过审查又保证效果？"
            },
            {
              "id": "llm_0359",
              "question": "为什么不用交叉熵而用 Margin？"
            },
            {
              "id": "llm_0360",
              "question": "margin 值怎么调？"
            },
            {
              "id": "llm_0361",
              "question": "如何防止模型把正例 score 打爆到 1 之后梯度消失？"
            },
            {
              "id": "llm_0362",
              "question": "升 batch 吞吐？"
            },
            {
              "id": "llm_0363",
              "question": "保证 NDCG@10 下降 <1%？"
            },
            {
              "id": "llm_0364",
              "question": "压到 1%，而生成模型往往掉 3% 以上？"
            },
            {
              "id": "llm_0365",
              "question": "多”策略？"
            },
            {
              "id": "llm_0366",
              "question": "保证召回？"
            },
            {
              "id": "llm_0367",
              "question": "如何根据业务召回指标反向推导 efConstruction；？"
            },
            {
              "id": "llm_0368",
              "question": "如何在百亿级向量、天级更新的 LLMOps 管线里落地。？"
            },
            {
              "id": "llm_0369",
              "question": "响合并流程？"
            },
            {
              "id": "llm_0370",
              "question": "抽取对应单元格？"
            },
            {
              "id": "llm_0371",
              "question": "实验数据？"
            },
            {
              "id": "llm_0372",
              "question": "优 pp×tp×dp 组合？"
            },
            {
              "id": "llm_0373",
              "question": "on recomputation？"
            },
            {
              "id": "llm_0374",
              "question": "与 PP+TP+ZeRO-3 如何组合？"
            },
            {
              "id": "llm_0375",
              "question": "国内合规要求训练日志留痕，selective 策略动态调整时如何审计重算层？"
            },
            {
              "id": "llm_0376",
              "question": "接受 QAT，你还有几招？"
            },
            {
              "id": "llm_0377",
              "question": "映射加载 7B 模型？"
            },
            {
              "id": "llm_0378",
              "question": "按需返回？"
            },
            {
              "id": "llm_0379",
              "question": "与 REST 互通：使用 google.api.HttpRule 把 FieldMask 映射成？"
            },
            {
              "id": "llm_0380",
              "question": "tus='已支付', version=version+1 where order_id=？"
            },
            {
              "id": "llm_0381",
              "question": "rsion=？"
            },
            {
              "id": "llm_0382",
              "question": "S 阈值？"
            },
            {
              "id": "llm_0383",
              "question": "到 <5s？"
            },
            {
              "id": "llm_0384",
              "question": "强化学习微调（RLHF）能否替代重试？"
            },
            {
              "id": "llm_0385",
              "question": "metheus？"
            },
            {
              "id": "llm_0386",
              "question": "variate Shift？"
            },
            {
              "id": "llm_0387",
              "question": "多租户场景下，如何做到租户级告警隔离？"
            },
            {
              "id": "llm_0388",
              "question": "降低 RTO？"
            },
            {
              "id": "llm_0389",
              "question": "重训后模型效果“虚假回升”（指标好看但人工评测下降）怎么发现？"
            },
            {
              "id": "llm_0390",
              "question": "如果知识库本身过期或矛盾，如何给知识置信度再打分？"
            },
            {
              "id": "llm_0391",
              "question": "遇到多跳事实（A的B的C是多少）怎么办？"
            },
            {
              "id": "llm_0392",
              "question": "若要支持实时流式生成（直播字幕），如何做到逐句验证不阻塞？"
            },
            {
              "id": "llm_0393",
              "question": "请求落同一 Pod，避免CPU 缓存失效导致长尾延迟。？"
            },
            {
              "id": "llm_0394",
              "question": "LSH 与向量数据库如何协同？"
            },
            {
              "id": "llm_0395",
              "question": "可删除要求？"
            },
            {
              "id": "llm_0396",
              "question": "如何用最小算力把对抗信号喂给模型，且不破坏通用能力；？"
            },
            {
              "id": "llm_0397",
              "question": "如何满足**《生成式 AI 服务管理暂行办法》**对可追溯、可撤销、可红？"
            },
            {
              "id": "llm_0398",
              "question": "谐音错误也纳入对抗样本？"
            },
            {
              "id": "llm_0399",
              "question": "based Learning做到 F1>90%？"
            },
            {
              "id": "llm_0400",
              "question": "类器并共享同一套敏感标签体系？"
            },
            {
              "id": "llm_0401",
              "question": "到 1 B 以内模型，F1 下降≤2%？"
            },
            {
              "id": "llm_0402",
              "question": "如果白名单膨胀到10 万条以上，配置中心性能瓶颈如何解决？"
            },
            {
              "id": "llm_0403",
              "question": "未来监管要求**“豁免需可解释”，如何自动化生成解释报告？"
            },
            {
              "id": "llm_0404",
              "question": "或海光 CSV** 实现等效密钥隔离？"
            },
            {
              "id": "llm_0405",
              "question": "库版本+Chunk-ID也纳入同一 Request-ID 证据链？"
            },
            {
              "id": "llm_0406",
              "question": "级日志快照，既能还原完整语境，又避免N² 膨胀？"
            },
            {
              "id": "llm_0407",
              "question": "力图**，在千亿参数模型上如何秒级抽取并脱敏展示？"
            },
            {
              "id": "llm_0408",
              "question": "请256 MB 大页 EPC，把Attention 输入张量钉住，防止 EPC 换页。？"
            },
            {
              "id": "llm_0409",
              "question": "如果业务方要求 ε=0.1 但延迟只能增加 2 ms，该如何权衡？"
            },
            {
              "id": "llm_0410",
              "question": "隐私放大？"
            },
            {
              "id": "llm_0411",
              "question": "（TEE）？"
            },
            {
              "id": "llm_0412",
              "question": "如何设计可追溯、可验证、可解释的 LLMOps 流水线；？"
            },
            {
              "id": "llm_0413",
              "question": "如何把模型概率输出转化为**符合《电子数据若干规定》《个人信息保护？"
            },
            {
              "id": "llm_0414",
              "question": "如何在 80% 置信度下，补足剩余 20% 不确定性，使报告达到“高度盖然？"
            },
            {
              "id": "llm_0415",
              "question": "大模型输出内容哈希上链后，如何防止“先上链后篡改”的链下文件掉包？"
            },
            {
              "id": "llm_0416",
              "question": "文本占比 60%？"
            },
            {
              "id": "llm_0417",
              "question": "减少 30% token？"
            },
            {
              "id": "llm_0418",
              "question": "TF 并优化到 <0.3？"
            },
            {
              "id": "llm_0419",
              "question": "NT8 TOPS，如何再降 RTF？"
            },
            {
              "id": "llm_0420",
              "question": "国内 内容安全审核要求 200 ms 内返回首字，如何与 RTF<0.3 共存？"
            },
            {
              "id": "llm_0421",
              "question": "梯度噪声过大怎么办？"
            },
            {
              "id": "llm_0422",
              "question": "何在不牺牲精度的前提下实现流式增量渲染？"
            },
            {
              "id": "llm_0423",
              "question": "如何设计低成本存储？"
            },
            {
              "id": "llm_0424",
              "question": "QA 准确率 90%？"
            },
            {
              "id": "llm_0425",
              "question": "如果业务要求**准确率 93%**但模型必须<800 M，如何再压缩？"
            },
            {
              "id": "llm_0426",
              "question": "若未来芯片进一步受限（仅 48 GB 显存），如何单卡完成 7B→1B 蒸馏？"
            },
            {
              "id": "llm_0427",
              "question": "换并微调？"
            },
            {
              "id": "llm_0428",
              "question": "低功耗 40%？"
            },
            {
              "id": "llm_0429",
              "question": "功耗目标？"
            },
            {
              "id": "llm_0430",
              "question": "n 描述？"
            },
            {
              "id": "llm_0431",
              "question": "请检查嵌套类型 \") from e？"
            },
            {
              "id": "llm_0432",
              "question": "动态阈值：如何让规则随监管政策自动刷新？"
            },
            {
              "id": "llm_0433",
              "question": "多模态参数：图片里嵌了非法二维码，如何校验？"
            },
            {
              "id": "llm_0434",
              "question": "图谱做数据增强，同时保证医疗广告法合规？"
            },
            {
              "id": "llm_0435",
              "question": "dge Plugin）** 而非微调，实现**“可验证引用”**？"
            },
            {
              "id": "llm_0436",
              "question": "助力 B 租户模型提升，满足**《数据跨境传输安全评估办法》**？"
            },
            {
              "id": "llm_0437",
              "question": "iners，但QPS损耗30%；如何在安全与性能之间做Trade-off？"
            },
            {
              "id": "llm_0438",
              "question": "溃，NVIDIA MIG+nvidia-container-cli能否复用同一沙箱模型？"
            },
            {
              "id": "llm_0439",
              "question": "Queue-Proxy削峰填谷？"
            },
            {
              "id": "llm_0440",
              "question": "h-safe logging”**？"
            },
            {
              "id": "llm_0441",
              "question": "插件热更新时，如何做到“零中断 + 零信任”？"
            },
            {
              "id": "llm_0442",
              "question": "国密算法性能在 GPU 推理节点上可能成为瓶颈，怎么办？"
            },
            {
              "id": "llm_0443",
              "question": "插件想要访问外部 HTTPS 服务，如何既放行又审计？"
            },
            {
              "id": "llm_0444",
              "question": "如果社区提交的是多模态任务（图文混合），如何复用现有 YAML 描述？"
            },
            {
              "id": "llm_0445",
              "question": "遇到恶意刷榜（偷偷在测试集里加训练语料）怎么办？"
            },
            {
              "id": "llm_0446",
              "question": "国内算力资源紧张，如何降低社区重复跑基座模型的成本？"
            },
            {
              "id": "llm_0447",
              "question": "多模态边界：当输入出现图片+文字时，如何定义单元？"
            },
            {
              "id": "llm_0448",
              "question": "请求实时翻译为英/日/西语，调用对应模型，若业务决策差异率>1%？"
            },
            {
              "id": "llm_0449",
              "question": "自动修复？"
            },
            {
              "id": "llm_0450",
              "question": "务”，返回 {sql: \"SELECT … WHERE id =？"
            },
            {
              "id": "llm_0451",
              "question": "化协议，占位符统一用？"
            },
            {
              "id": "llm_0452",
              "question": "… WHERE id =？"
            },
            {
              "id": "llm_0453",
              "question": "阶段 55 ms 压回去？"
            },
            {
              "id": "llm_0454",
              "question": "何不重新训练大模型？"
            },
            {
              "id": "llm_0455",
              "question": "涨，如何继续保证 100 ms？"
            },
            {
              "id": "llm_0456",
              "question": "动态窗口：能否让模型自己学窗口大小？"
            },
            {
              "id": "llm_0457",
              "question": "请逐步推导：？"
            },
            {
              "id": "llm_0458",
              "question": "如果面试官追问“数据合成比例继续提高，会不会导致模型崩溃？"
            },
            {
              "id": "llm_0459",
              "question": "若问“政务客户担心开源后数据泄露机密，如何平衡？"
            },
            {
              "id": "llm_0460",
              "question": "若问“后续如何与 LLMOps 打通做持续监控？"
            },
            {
              "id": "llm_0461",
              "question": "可微几何约束能否直接写进Transformer loss？"
            },
            {
              "id": "llm_0462",
              "question": "多源坐标系混用（WGS-84、GCJ-02、BD-09）时，如何统一约束？"
            },
            {
              "id": "llm_0463",
              "question": "极端大场景（千亿级参数+城市级路网）如何横向扩展？"
            },
            {
              "id": "llm_0464",
              "question": "请用**不超过120字**向业务经理解释：？"
            },
            {
              "id": "llm_0465",
              "question": "多模态扩展：如果输入的是时序图 + 样本量，先用视觉模型抽“n=？"
            },
            {
              "id": "llm_0466",
              "question": "容，你如何在一周内完成全量历史解释的回扫与过滤？"
            },
            {
              "id": "llm_0467",
              "question": "当生成解释引入多模态（图文混排）后，满意度评估指标需要如何升级？"
            },
            {
              "id": "llm_0468",
              "question": "更长链（>50 步）怎么办？"
            },
            {
              "id": "llm_0469",
              "question": "多模态长链如何校验？"
            },
            {
              "id": "llm_0470",
              "question": "如何对抗“隐蔽矛盾”？"
            },
            {
              "id": "llm_0471",
              "question": "扩展到 32k？"
            },
            {
              "id": "llm_0472",
              "question": "n，如何设计增量召回 + 滑动窗口标注？"
            },
            {
              "id": "llm_0473",
              "question": "保证标注一致性？"
            },
            {
              "id": "llm_0474",
              "question": "梯度检查点把 128K 模型评估耗时压到 30 分钟以内？"
            },
            {
              "id": "llm_0475",
              "question": "果三层分离存储？"
            },
            {
              "id": "llm_0476",
              "question": "耗时控制在 1 s 内？"
            },
            {
              "id": "llm_0477",
              "question": "何改造金字塔？"
            },
            {
              "id": "llm_0478",
              "question": "强制判为幻觉？"
            },
            {
              "id": "llm_0479",
              "question": "下，用合成数据纠偏并保证增量微调不偏航？"
            },
            {
              "id": "llm_0480",
              "question": "化触发阈值？"
            },
            {
              "id": "llm_0481",
              "question": "全量微调？"
            },
            {
              "id": "llm_0482",
              "question": "e over Fabric，你能设计一套 双缓冲流水线 保证计算不空等吗？"
            },
            {
              "id": "llm_0483",
              "question": "memory padding 把冲突率降到 2% 以下？"
            },
            {
              "id": "llm_0484",
              "question": "“文化偏见”与“本地化合规”冲突时如何取舍？"
            },
            {
              "id": "llm_0485",
              "question": "答、知识库参考），抽屉顶部置**“本条回答是否解决您的问题？"
            },
            {
              "id": "llm_0486",
              "question": "s)['\"]？"
            },
            {
              "id": "llm_0487",
              "question": "(\\w+)['\"]？"
            },
            {
              "id": "llm_0488",
              "question": "\\s*[:=]\\s*['\"]？"
            },
            {
              "id": "llm_0489",
              "question": ")['\"]？"
            },
            {
              "id": "llm_0490",
              "question": "\"amount\"\\s*:\\s*(？"
            },
            {
              "id": "llm_0491",
              "question": "<amount>\\d+\\.？"
            },
            {
              "id": "llm_0492",
              "question": "assert re.match(r\"^\\d{6}(\\d{6})？"
            },
            {
              "id": "llm_0493",
              "question": "请格式化地区：？"
            },
            {
              "id": "llm_0494",
              "question": "而非模型 Bug？"
            },
            {
              "id": "llm_0495",
              "question": "交叉编译后的 ARM 环境？"
            },
            {
              "id": "llm_0496",
              "question": "降维可视化？"
            },
            {
              "id": "llm_0497",
              "question": "ch 推理延迟从 420 ms 降到 180 ms，如何保持矩阵统计特性不变？"
            },
            {
              "id": "llm_0498",
              "question": "如何实时捕获熵崩溃信号（毫秒级延迟）；？"
            },
            {
              "id": "llm_0499",
              "question": "如何无人工干预触发 checkpoint 回滚与训练重启；？"
            },
            {
              "id": "llm_0500",
              "question": "如何与LLMOps 平台（如自研或阿里云 PAI、百度百舸、华为 ModelArt？"
            },
            {
              "id": "llm_0501",
              "question": "在线插入已有前沿而不重新跑全量实验？"
            },
            {
              "id": "llm_0502",
              "question": "*“尽最大合理技术努力”？"
            },
            {
              "id": "llm_0503",
              "question": "下做动态降维？"
            },
            {
              "id": "llm_0504",
              "question": "自动提示？"
            },
            {
              "id": "llm_0505",
              "question": "head 数不一致怎么办？"
            },
            {
              "id": "llm_0506",
              "question": "解释类型：链式思维（CoT）、引用召回（RAG 片段）、知识图谱路径、注？"
            },
            {
              "id": "llm_0507",
              "question": "大模型时代，BERT 向量是否还够用？"
            },
            {
              "id": "llm_0508",
              "question": "聚类结果如何反哺大模型微调？"
            },
            {
              "id": "llm_0509",
              "question": "国产化算力瓶颈怎么破？"
            },
            {
              "id": "llm_0510",
              "question": "“在{城市}落户政策中，{群体A}和{群体B}谁更受益？"
            },
            {
              "id": "llm_0511",
              "question": "“差异>10%” 是否一定需要重采样？"
            },
            {
              "id": "llm_0512",
              "question": "况下30分钟内热修复？"
            },
            {
              "id": "llm_0513",
              "question": "槛，而国内是5% SPD，如何一套代码同时满足两套监管？"
            },
            {
              "id": "llm_0514",
              "question": "描述做 Few-Shot，再拼接 RAG 检索出的 5 条国标条款，整体 token 控制在？"
            },
            {
              "id": "llm_0515",
              "question": "字段动态膨胀到 1000+ 怎么办？"
            },
            {
              "id": "llm_0516",
              "question": "异化发布？"
            },
            {
              "id": "llm_0517",
              "question": "如果模型实际合规，我能否把“误报”控制在 5% 以内？"
            },
            {
              "id": "llm_0518",
              "question": "解释层：点击任意卡片，右侧抽屉弹出三段式中文解释？"
            },
            {
              "id": "llm_0519",
              "question": "解释模型为何聚焦“红色按钮”而非“背景文字”。？"
            },
            {
              "id": "llm_0520",
              "question": "时内热修复而不中断每日数据流？"
            },
            {
              "id": "llm_0521",
              "question": "策略，让模型自动合并相似标签并推荐最具信息量的论文给领域专家？"
            },
            {
              "id": "llm_0522",
              "question": "时避免误杀关键技术文献？"
            },
            {
              "id": "llm_0523",
              "question": "失效，如何引入 CLIP 或 BLIP 做跨模态推荐，并与现有标签体系对齐？"
            },
            {
              "id": "llm_0524",
              "question": "时内的成本？"
            },
            {
              "id": "llm_0525",
              "question": "G），如何防止资源死锁？"
            },
            {
              "id": "llm_0526",
              "question": "以下 commit 是否包含代码、测试、数据或提示词贡献？"
            },
            {
              "id": "llm_0527",
              "question": "多模态贡献怎么识别？"
            },
            {
              "id": "llm_0528",
              "question": "机器人自身被投毒怎么办？"
            },
            {
              "id": "llm_0529",
              "question": "如何把贡献者数据喂给大模型做价值观对齐？"
            },
            {
              "id": "llm_0530",
              "question": "如果公司 2025 预算砍半，如何用德尔菲做减法？"
            },
            {
              "id": "llm_0531",
              "question": "插入合规题？"
            },
            {
              "id": "llm_0532",
              "question": "如何让德尔菲结论持续保鲜？"
            },
            {
              "id": "llm_0533",
              "question": "如果集团要求**“双碳”指标**怎么办？"
            },
            {
              "id": "llm_0534",
              "question": "面对**“百模大战”紧急立项，如何 1 天内快速生成路线图？"
            },
            {
              "id": "llm_0535",
              "question": "团队分布北上深杭四地，如何确保 review 不流于形式？"
            },
            {
              "id": "llm_0536",
              "question": "如果面试官追问“对方专利是基础算法，无法绕开怎么办？"
            },
            {
              "id": "llm_0537",
              "question": "说明书法条：权利要求必须逐层布局，从独立权利要求的“最小技术方？"
            },
            {
              "id": "llm_0538",
              "question": "说明书里，我给出3 组对比实验： baseline（纯向量检索）、仅剪枝、本发明，？"
            },
            {
              "id": "llm_0539",
              "question": "如果未来业务目标从“降本”升级为“营销转化”，是否仍沿用“问题解决率”？"
            },
            {
              "id": "llm_0540",
              "question": "如何把代理指标嵌入LLMOps 闭环，实现自动回滚、熔断、微调触发，而？"
            },
            {
              "id": "llm_0541",
              "question": "当北极星指标出现**“短期与长期冲突”时如何取舍？"
            },
            {
              "id": "llm_0542",
              "question": "多业务共用一套大模型底座时，如何设计**“复合北极星”？"
            },
            {
              "id": "llm_0543",
              "question": "如果公司处于**“从 0 到 1 探索阶段”，尚无成交数据，如何定北极星？"
            },
            {
              "id": "llm_0544",
              "question": "文：“您会把本功能推荐给朋友吗？"
            },
            {
              "id": "llm_0545",
              "question": "到Flink 实时流，并用KLL sketch 解决窗口内样本稀疏？"
            },
            {
              "id": "llm_0546",
              "question": "NPS 问卷是否也应双语？"
            },
            {
              "id": "llm_0547",
              "question": "如何设计等效语义量表避免文化偏差？"
            },
            {
              "id": "llm_0548",
              "question": "的“用户价值”动态变量？"
            },
            {
              "id": "llm_0549",
              "question": "FAQ 页面？"
            },
            {
              "id": "llm_0550",
              "question": "大模型恢复后，如何防止上下文断裂导致用户体验跳变？"
            },
            {
              "id": "llm_0551",
              "question": "国内运营商5G 消息网关对时延要求 ≤1 s，如何进一步压缩降级路径？"
            },
            {
              "id": "llm_0552",
              "question": "如何证明演练既合规又有效；？"
            },
            {
              "id": "llm_0553",
              "question": "增哪些风险维度？"
            },
            {
              "id": "llm_0554",
              "question": "如何量化“不良图文合成”的拦截率？"
            },
            {
              "id": "llm_0555",
              "question": "证分片权重一致性与推理结果确定性？"
            },
            {
              "id": "llm_0556",
              "question": "关联**，既满足审计又避免隐私泄露？"
            },
            {
              "id": "llm_0557",
              "question": "评”对外开放，反向提升企业品牌与技术影响力？"
            },
            {
              "id": "llm_0558",
              "question": "如果下季度业务方突然要求支持长文本 128K 上下文，如何重写 KR？"
            },
            {
              "id": "llm_0559",
              "question": "面试官可能追问“如果 KR 全部达成，但业务 GMV 没涨，是否算成功？"
            },
            {
              "id": "llm_0560",
              "question": "在季度复盘中触发根因分析（数据分布漂移？"
            },
            {
              "id": "llm_0561",
              "question": "评估指标错位？"
            },
            {
              "id": "llm_0562",
              "question": "公开数据+合成数据在 1 个 Sprint 内逼近效果？"
            },
            {
              "id": "llm_0563",
              "question": "本降到原来 1/3，仍保证 TP99 <2 s？"
            },
            {
              "id": "llm_0564",
              "question": "Stable Diffusion API 同时满足图片审核+版权过滤？"
            },
            {
              "id": "llm_0565",
              "question": "请求。？"
            },
            {
              "id": "llm_0566",
              "question": "内导出完整证据链？"
            },
            {
              "id": "llm_0567",
              "question": "模型如何自适应？"
            },
            {
              "id": "llm_0568",
              "question": "怎么让英⽂⼤语⾔模型⽀持中⽂？"
            },
            {
              "id": "llm_0569",
              "question": "⽬前 主流的开源模型体系 有哪些？"
            },
            {
              "id": "llm_0570",
              "question": "prefix Decoder 和 causal Decoder 和 Encoder-Decoder 区别是什么？"
            },
            {
              "id": "llm_0571",
              "question": "⼤模型LLM的 训练⽬标 是什么？"
            },
            {
              "id": "llm_0572",
              "question": "涌现能⼒是啥原因？"
            },
            {
              "id": "llm_0573",
              "question": "为何现在的⼤模型⼤部分是Decoder only结构？"
            },
            {
              "id": "llm_0574",
              "question": "简单 介绍⼀下 ⼤模型【LLMs】？"
            },
            {
              "id": "llm_0575",
              "question": "⼤模型【LLMs】后⾯跟的 175B、60B、540B等 指什么？"
            },
            {
              "id": "llm_0576",
              "question": "⼤模型【LLMs】具有什么优点？"
            },
            {
              "id": "llm_0577",
              "question": "⼤模型【LLMs】具有什么缺点？"
            },
            {
              "id": "llm_0578",
              "question": "prefix LM 和 causal LM 区别是什么？"
            },
            {
              "id": "llm_0579",
              "question": "⼤模型LLM的架构介绍？"
            },
            {
              "id": "llm_0580",
              "question": "什么是 LLMs 复读机问题？"
            },
            {
              "id": "llm_0581",
              "question": "为什么会出现 LLMs 复读机问题？"
            },
            {
              "id": "llm_0582",
              "question": "如何缓解 LLMs 复读机问题？"
            },
            {
              "id": "llm_0583",
              "question": "llama 输⼊句⼦⻓度理论上可以⽆限⻓吗？"
            },
            {
              "id": "llm_0584",
              "question": "什么情况⽤Bert模型，什么情况⽤LLaMA、ChatGLM类⼤模型，咋选？"
            },
            {
              "id": "llm_0585",
              "question": "各个专业领域是否需要各⾃的⼤模型来服务？"
            },
            {
              "id": "llm_0586",
              "question": "如何让⼤模型处理更⻓的⽂本？"
            },
            {
              "id": "llm_0587",
              "question": "如果想要在某个模型基础上做全参数微调，究竟需要多少显存？"
            },
            {
              "id": "llm_0588",
              "question": "为什么SFT之后感觉LLM傻了？"
            },
            {
              "id": "llm_0589",
              "question": "SFT 指令微调数据 如何构建？"
            },
            {
              "id": "llm_0590",
              "question": "领域模型Continue PreTrain 数据选取？"
            },
            {
              "id": "llm_0591",
              "question": "领域数据训练后，通⽤能⼒往往会有所下降，如何缓解模型遗忘通⽤能⼒？"
            },
            {
              "id": "llm_0592",
              "question": "领域模型Continue PreTrain ，如何 让模型在预训练过程中就学习到更多的知识？"
            },
            {
              "id": "llm_0593",
              "question": "进⾏SFT操作的时候，基座模型选⽤Chat还是Base？"
            },
            {
              "id": "llm_0594",
              "question": "领域模型微调 指令&数据输⼊格式 要求？"
            },
            {
              "id": "llm_0595",
              "question": "领域模型微调 领域评测集 构建？"
            },
            {
              "id": "llm_0596",
              "question": "领域模型词表扩增是不是有必要的？"
            },
            {
              "id": "llm_0597",
              "question": "如何训练⾃⼰的⼤模型？"
            },
            {
              "id": "llm_0598",
              "question": "训练中⽂⼤模型有啥经验？"
            },
            {
              "id": "llm_0599",
              "question": "指令微调的好处？"
            },
            {
              "id": "llm_0600",
              "question": "预训练和微调哪个阶段注⼊知识的？"
            },
            {
              "id": "llm_0601",
              "question": "想让模型学习某个领域或⾏业的知识，是应该预训练还是应该微调？"
            },
            {
              "id": "llm_0602",
              "question": "多轮对话任务如何微调模型？"
            },
            {
              "id": "llm_0603",
              "question": "微调后的模型出现能⼒劣化，灾难性遗忘是怎么回事？"
            },
            {
              "id": "llm_0604",
              "question": "微调模型需要多⼤显存？"
            },
            {
              "id": "llm_0605",
              "question": "⼤模型LLM进⾏SFT操作的时候在学习什么？"
            },
            {
              "id": "llm_0606",
              "question": "⼤模型LLM进⾏SFT 如何对样本进⾏优化？"
            },
            {
              "id": "llm_0607",
              "question": "分布式训练框架选择？"
            },
            {
              "id": "llm_0608",
              "question": "LLMs 训练时 有哪些有⽤的建议？"
            },
            {
              "id": "llm_0609",
              "question": "模型⼤⼩如何选择？"
            },
            {
              "id": "llm_0610",
              "question": "加速卡如何选择？"
            },
            {
              "id": "llm_0611",
              "question": "什么是 LangChain？"
            },
            {
              "id": "llm_0612",
              "question": "LangChain 包含哪些 核⼼概念？"
            },
            {
              "id": "llm_0613",
              "question": "1 LangChain 中 Components and Chains 是什么？"
            },
            {
              "id": "llm_0614",
              "question": "2 LangChain 中 Prompt Templates and Values 是什么？"
            },
            {
              "id": "llm_0615",
              "question": "3 LangChain 中 Example Selectors 是什么？"
            },
            {
              "id": "llm_0616",
              "question": "4 LangChain 中 Output Parsers 是什么？"
            },
            {
              "id": "llm_0617",
              "question": "5 LangChain 中 Indexes and Retrievers 是什么？"
            },
            {
              "id": "llm_0618",
              "question": "6 LangChain 中 Chat Message History 是什么？"
            },
            {
              "id": "llm_0619",
              "question": "7 LangChain 中 Agents and Toolkits 是什么？"
            },
            {
              "id": "llm_0620",
              "question": "什么是 LangChain Agent？"
            },
            {
              "id": "llm_0621",
              "question": "如何使⽤ LangChain？"
            },
            {
              "id": "llm_0622",
              "question": "LangChain ⽀持哪些功能？"
            },
            {
              "id": "llm_0623",
              "question": "什么是 LangChain model？"
            },
            {
              "id": "llm_0624",
              "question": "LangChain 包含哪些特点？"
            },
            {
              "id": "llm_0625",
              "question": "LangChain 如何使⽤？"
            },
            {
              "id": "llm_0626",
              "question": "1 LangChain 如何调⽤ LLMs ⽣成回复？"
            },
            {
              "id": "llm_0627",
              "question": "2 LangChain 如何修改 提示模板？"
            },
            {
              "id": "llm_0628",
              "question": "3 LangChain 如何链接多个组件处理⼀个特定的下游任务？"
            },
            {
              "id": "llm_0629",
              "question": "4 LangChain 如何Embedding & vector store？"
            },
            {
              "id": "llm_0630",
              "question": "LangChain 存在哪些问题及⽅法⽅案？"
            },
            {
              "id": "llm_0631",
              "question": "LangChain 替代⽅案？"
            },
            {
              "id": "llm_0632",
              "question": "1 为什么 ⼤模型 需要 外挂(向量)知识库？"
            },
            {
              "id": "llm_0633",
              "question": "2. 基于LLM+向量库的⽂档对话 思路是怎么样？"
            },
            {
              "id": "llm_0634",
              "question": "3. 基于LLM+向量库的⽂档对话 核⼼技术是什么？"
            },
            {
              "id": "llm_0635",
              "question": "4. 基于LLM+向量库的⽂档对话 prompt 模板 如何构建？"
            },
            {
              "id": "llm_0636",
              "question": "⼆、基于LLM+向量库的⽂档对话 存在哪些痛点？"
            },
            {
              "id": "llm_0637",
              "question": "LLMs 存在模型幻觉问题，请问如何处理？"
            },
            {
              "id": "llm_0638",
              "question": "基于LLM+向量库的⽂档对话 思路是怎么样？"
            },
            {
              "id": "llm_0639",
              "question": "基于LLM+向量库的⽂档对话 核⼼技术是什么？"
            },
            {
              "id": "llm_0640",
              "question": "基于LLM+向量库的⽂档对话 prompt 模板 如何构建？"
            },
            {
              "id": "llm_0641",
              "question": "问有关于XXX的新闻吗？"
            },
            {
              "id": "llm_0642",
              "question": "五、如何 ⻓⽂档（书籍）中关键信息？"
            },
            {
              "id": "llm_0643",
              "question": "六、为什么要提取标题甚⾄是多级标题？"
            },
            {
              "id": "llm_0644",
              "question": "七、如何提取 ⽂章标题？"
            },
            {
              "id": "llm_0645",
              "question": "⼋、如何区分单栏还是双栏pdf？"
            },
            {
              "id": "llm_0646",
              "question": "如何重新排序？"
            },
            {
              "id": "llm_0647",
              "question": "九、如何提取表格和图⽚中的数据？"
            },
            {
              "id": "llm_0648",
              "question": "微调⽅法是啥？"
            },
            {
              "id": "llm_0649",
              "question": "介绍⼀下 PEFT？"
            },
            {
              "id": "llm_0650",
              "question": "微调⽅法批处理⼤⼩模式GPU显存速度？"
            },
            {
              "id": "llm_0651",
              "question": "能不能总结⼀下各种参数⾼效微调⽅法？"
            },
            {
              "id": "llm_0652",
              "question": "⼀、为什么 需要 适配器微调（Adapter-tuning）？"
            },
            {
              "id": "llm_0653",
              "question": "⼆、适配器微调（Adapter-tuning）思路？"
            },
            {
              "id": "llm_0654",
              "question": "⼋、MAM Adapter 特点 是什么？"
            },
            {
              "id": "llm_0655",
              "question": "⼀、为什么需要 提示学习（Prompting）？"
            },
            {
              "id": "llm_0656",
              "question": "⼆、什么是 提示学习（Prompting）？"
            },
            {
              "id": "llm_0657",
              "question": "四、提示学习（Prompting）有哪些⽅法，能不能稍微介绍⼀下它们间？"
            },
            {
              "id": "llm_0658",
              "question": "1.1 为什么需要 前缀微调（Prefix-tuning）？"
            },
            {
              "id": "llm_0659",
              "question": "- 4.1.2 前缀微调（Prefix-tuning）思路是什么？"
            },
            {
              "id": "llm_0660",
              "question": "- 4.1.3 前缀微调（Prefix-tuning）的优点是什么？"
            },
            {
              "id": "llm_0661",
              "question": "- 4.1.4 前缀微调（Prefix-tuning）的缺点是什么？"
            },
            {
              "id": "llm_0662",
              "question": "- 4.2.2 指示微调（Prompt-tuning）思路是什么？"
            },
            {
              "id": "llm_0663",
              "question": "- 4.2.3 指示微调（Prompt-tuning）优点是什么？"
            },
            {
              "id": "llm_0664",
              "question": "- 4.2.4 指示微调（Prompt-tuning）缺点是什么？"
            },
            {
              "id": "llm_0665",
              "question": "- 4.2.5 指示微调（Prompt-tuning）与 Prefix-tuning 区别 是什么？"
            },
            {
              "id": "llm_0666",
              "question": "- 4.2.6 指示微调（Prompt-tuning）与 fine-tuning 区别 是什么？"
            },
            {
              "id": "llm_0667",
              "question": "- 4.3.2 P-tuning 思路是什么？"
            },
            {
              "id": "llm_0668",
              "question": "- 4.3.3 P-tuning 优点是什么？"
            },
            {
              "id": "llm_0669",
              "question": "- 4.3.4 P-tuning 缺点是什么？"
            },
            {
              "id": "llm_0670",
              "question": "- 4.4.2 P-tuning v2 思路是什么？"
            },
            {
              "id": "llm_0671",
              "question": "- 4.4.3 P-tuning v2 优点是什么？"
            },
            {
              "id": "llm_0672",
              "question": "- 4.4.4 P-tuning v2 缺点是什么？"
            },
            {
              "id": "llm_0673",
              "question": "四、LoRA权重是否可以合⼊原模型？"
            },
            {
              "id": "llm_0674",
              "question": "五、ChatGLM-6B LoRA后的权重多⼤？"
            },
            {
              "id": "llm_0675",
              "question": "七、LoRA微调⽅法为啥能加速训练？"
            },
            {
              "id": "llm_0676",
              "question": "⼋、如何在已有LoRA模型上继续训练？"
            },
            {
              "id": "llm_0677",
              "question": "⼗、LoRA这种微调⽅法和全参数⽐起来有什么劣势吗？"
            },
            {
              "id": "llm_0678",
              "question": "**什么是low-rank adaptation of large language models？"
            },
            {
              "id": "llm_0679",
              "question": "为什么⼤模型推理时显存涨的那么多还⼀直占着？"
            },
            {
              "id": "llm_0680",
              "question": "⼤模型在gpu和cpu上推理速度如何？"
            },
            {
              "id": "llm_0681",
              "question": "推理速度上，int8和fp16⽐起来怎么样？"
            },
            {
              "id": "llm_0682",
              "question": "⼤模型有推理能⼒吗？"
            },
            {
              "id": "llm_0683",
              "question": "⼤模型⽣成时的参数怎么设置？"
            },
            {
              "id": "llm_0684",
              "question": "有哪些省内存的⼤语⾔模型训练/微调/推理⽅法？"
            },
            {
              "id": "llm_0685",
              "question": "1 如何 估算模型所需的RAM？"
            },
            {
              "id": "llm_0686",
              "question": "如何让⼤模型输出合规化？"
            },
            {
              "id": "llm_0687",
              "question": "解释模型决策过程：为了满⾜合规性要求，可以对模型的决策过程进⾏解释和解释。通过提供透明？"
            },
            {
              "id": "llm_0688",
              "question": "为什么要增量预训练？"
            },
            {
              "id": "llm_0689",
              "question": "进⾏ 增量预训练 需要做哪些准备⼯作？"
            },
            {
              "id": "llm_0690",
              "question": "增量预训练 所⽤ 训练框架？"
            },
            {
              "id": "llm_0691",
              "question": "增量预训练 训练流程 是怎么样？"
            },
            {
              "id": "llm_0692",
              "question": "⼤模型怎么评测？"
            },
            {
              "id": "llm_0693",
              "question": "⼤模型的honest原则是如何实现的？"
            },
            {
              "id": "llm_0694",
              "question": "如何衡量⼤模型⽔平？"
            },
            {
              "id": "llm_0695",
              "question": "⼤模型评估⽅法 有哪些？"
            },
            {
              "id": "llm_0696",
              "question": "⼤模型评估⼯具 有哪些？"
            },
            {
              "id": "llm_0697",
              "question": "模型如何判断回答的知识是训练过的已知的知识，怎么训练这种能⼒？"
            },
            {
              "id": "llm_0698",
              "question": "简单介绍强化学习？"
            },
            {
              "id": "llm_0699",
              "question": "简单介绍⼀下 RLHF？"
            },
            {
              "id": "llm_0700",
              "question": "奖励模型需要和基础模型⼀致吗？"
            },
            {
              "id": "llm_0701",
              "question": "RLHF 在实践过程中存在哪些不⾜？"
            },
            {
              "id": "llm_0702",
              "question": "如何解决 ⼈⼯产⽣的偏好数据集成本较⾼，很难量产问题？"
            },
            {
              "id": "llm_0703",
              "question": "如何解决三个阶段的训练（SFT->RM->PPO）过程较⻓，更新迭代较慢问题？"
            },
            {
              "id": "llm_0704",
              "question": "如何解决 PPO 的训练过程同时存在4个模型（2训练，2推理），对计算资源的要求较⾼ 问题？"
            },
            {
              "id": "llm_0705",
              "question": "建议的软件环境是什么？"
            },
            {
              "id": "llm_0706",
              "question": "SFT（有监督微调）的数据集格式？"
            },
            {
              "id": "llm_0707",
              "question": "RM（奖励模型）的数据格式？"
            },
            {
              "id": "llm_0708",
              "question": "PPO（强化学习）的数据格式？"
            },
            {
              "id": "llm_0709",
              "question": "找数据集哪⾥找？"
            },
            {
              "id": "llm_0710",
              "question": "微调需要多少条数据？"
            },
            {
              "id": "llm_0711",
              "question": "有哪些⼤模型的训练集？"
            },
            {
              "id": "llm_0712",
              "question": "进⾏领域⼤模型预训练应⽤哪些数据集⽐较好？"
            },
            {
              "id": "llm_0713",
              "question": "⼤模型⼤概有多⼤，模型⽂件有多⼤？"
            },
            {
              "id": "llm_0714",
              "question": "能否⽤4 * v100 32G训练vicuna 65b？"
            },
            {
              "id": "llm_0715",
              "question": "如果就是想要试试65b模型，但是显存不多怎么办？"
            },
            {
              "id": "llm_0716",
              "question": "nB模型推理需要多少显存？"
            },
            {
              "id": "llm_0717",
              "question": "nB模型训练需要多少显存？"
            },
            {
              "id": "llm_0718",
              "question": "如何 估算模型所需的RAM？"
            },
            {
              "id": "llm_0719",
              "question": "如何评估你的显卡利⽤率？"
            },
            {
              "id": "llm_0720",
              "question": "如何查看多机训练时的⽹速？"
            },
            {
              "id": "llm_0721",
              "question": "如何查看服务器上的多卡之间的NVLINK topo？"
            },
            {
              "id": "llm_0722",
              "question": "如何查看服务器上显卡的具体型号？"
            },
            {
              "id": "llm_0723",
              "question": "如何查看训练时的flops？"
            },
            {
              "id": "llm_0724",
              "question": "如何查看对deepspeed的环境配置是否正确？"
            },
            {
              "id": "llm_0725",
              "question": "tf32格式有多⻓？"
            },
            {
              "id": "llm_0726",
              "question": "哪⾥看各类显卡算⼒⽐较？"
            },
            {
              "id": "llm_0727",
              "question": "（torch profiler）如何查看⾃⼰的训练中通信开销？"
            },
            {
              "id": "llm_0728",
              "question": "1 训练 ⼤语⾔模型 存在问题？"
            },
            {
              "id": "llm_0729",
              "question": "2 什么是 点对点通信？"
            },
            {
              "id": "llm_0730",
              "question": "3 什么是 集体通信？"
            },
            {
              "id": "llm_0731",
              "question": "4 什么是 数据并⾏？"
            },
            {
              "id": "llm_0732",
              "question": "5 数据并⾏ 如何 提升效率？"
            },
            {
              "id": "llm_0733",
              "question": "6 什么是 流⽔线并⾏？"
            },
            {
              "id": "llm_0734",
              "question": "7 什么是 张量并⾏ (intra-layer)？"
            },
            {
              "id": "llm_0735",
              "question": "8 数据并⾏ vs 张量并⾏ vs 流⽔线并⾏？"
            },
            {
              "id": "llm_0736",
              "question": "9 什么是 3D并⾏？"
            },
            {
              "id": "llm_0737",
              "question": "10 想要训练1个LLM，如果只想⽤1张显卡，那么对显卡的要求是什么？"
            },
            {
              "id": "llm_0738",
              "question": "11 如果有N张显存⾜够⼤的显卡，怎么加速训练？"
            },
            {
              "id": "llm_0739",
              "question": "12 如果显卡的显存不够装下⼀个完整的模型呢？"
            },
            {
              "id": "llm_0740",
              "question": "13 PP推理时，是⼀个串⾏的过程，1个GPU计算，其他空闲，有没有其他⽅式？"
            },
            {
              "id": "llm_0741",
              "question": "14 3种并⾏⽅式可以叠加吗？"
            },
            {
              "id": "llm_0742",
              "question": "15 Colossal-AI 有1D/2D/2.5D/3D，是什么情况？"
            },
            {
              "id": "llm_0743",
              "question": "16 除了3D并⾏有没有其他⽅式⼤规模训练？"
            },
            {
              "id": "llm_0744",
              "question": "17 有了ZeRO系列，为什么还需要3D并⾏？"
            },
            {
              "id": "llm_0745",
              "question": "18 平⺠适不适合玩3D并⾏？"
            },
            {
              "id": "llm_0746",
              "question": "19 平⺠适不适合直接上多机多卡的ZeRO3（万兆⽹）？"
            },
            {
              "id": "llm_0747",
              "question": "20 分布式并⾏及显存优化技术并⾏技术有哪⼀些，都有什么特点？"
            },
            {
              "id": "llm_0748",
              "question": "21 显存优化技术有哪⼀些，都有什么特点？"
            },
            {
              "id": "llm_0749",
              "question": "22 常⻅的分布式训练框架哪⼀些，都有什么特点？"
            },
            {
              "id": "llm_0750",
              "question": "1 假如有超多的8卡A100节点（DGX A100），如何应⽤3D并⾏策略？"
            },
            {
              "id": "llm_0751",
              "question": "2 如果想构这样⼀个⼤规模并⾏训练系统，训练框架如何选？"
            },
            {
              "id": "llm_0752",
              "question": "3 训练框架如何选？"
            },
            {
              "id": "llm_0753",
              "question": "1 如何选择⼀款分布式训练框架？"
            },
            {
              "id": "llm_0754",
              "question": "2 如何选择⼀款分布式训练框架？"
            },
            {
              "id": "llm_0755",
              "question": "4 为什么 多机训练效率不如单机？"
            },
            {
              "id": "llm_0756",
              "question": "为什么需要流⽔线并⾏（Pipeline Parallelism）？"
            },
            {
              "id": "llm_0757",
              "question": "⼀、流⽔线并⾏（Pipeline Parallelism） 优化⽬标是什么？"
            },
            {
              "id": "llm_0758",
              "question": "⼆、图解 流⽔线并⾏（Pipeline Parallelism）模型并⾏ 必要性？"
            },
            {
              "id": "llm_0759",
              "question": "三、流⽔线并⾏（Pipeline Parallelism） 图解？"
            },
            {
              "id": "llm_0760",
              "question": "四、流⽔线并⾏（Pipeline Parallelism）优缺点？"
            },
            {
              "id": "llm_0761",
              "question": "为什么需要nn.DataParallel？"
            },
            {
              "id": "llm_0762",
              "question": "⼀、pytorch中的GPU操作默认是什么样？"
            },
            {
              "id": "llm_0763",
              "question": "⼆、介绍⼀下 nn.DataParallel 函数？"
            },
            {
              "id": "llm_0764",
              "question": "三、nn.DataParallel 函数 处理逻辑 介绍⼀下？"
            },
            {
              "id": "llm_0765",
              "question": "四、nn.DataParallel 函数 常⻅问题及解答 有哪些？"
            },
            {
              "id": "llm_0766",
              "question": "1 多GPU计算减少了程序运⾏的时间？"
            },
            {
              "id": "llm_0767",
              "question": "2 如何保存和加载多GPU训练模型呢？"
            },
            {
              "id": "llm_0768",
              "question": "3 为什么第⼀块卡的显存会占⽤的更多⼀些？"
            },
            {
              "id": "llm_0769",
              "question": "4 直接使⽤nn.DataParallel的时候，训练采⽤多卡训练，会出现⼀个warning？"
            },
            {
              "id": "llm_0770",
              "question": "五、nn.DataParallel 函数 参数更新⽅式？"
            },
            {
              "id": "llm_0771",
              "question": "六、nn.DataParallel 函数 优点 介绍⼀下？"
            },
            {
              "id": "llm_0772",
              "question": "七、nn.DataParallel 函数 缺点 介绍⼀下？"
            },
            {
              "id": "llm_0773",
              "question": "⼋、nn.DataParallel 函数 实战？"
            },
            {
              "id": "llm_0774",
              "question": "为什么需要 nn.parallel.DistributedDataParallel？"
            },
            {
              "id": "llm_0775",
              "question": "⼀、什么是 DistributedDataParallel 核⼼ —— Ring-AllReduce？"
            },
            {
              "id": "llm_0776",
              "question": "⼆、nn.parallel.DistributedDataParallel 函数 介绍⼀下？"
            },
            {
              "id": "llm_0777",
              "question": "三、nn.parallel.DistributedDataParallel 函数 如何多卡加速训练？"
            },
            {
              "id": "llm_0778",
              "question": "四、nn.parallel.DistributedDataParallel 实现流程介绍⼀下？"
            },
            {
              "id": "llm_0779",
              "question": "五、nn.parallel.DistributedDataParallel 参数更新介绍⼀下？"
            },
            {
              "id": "llm_0780",
              "question": "六、nn.DataParallel(以下简称DP) vs DistributedDataParallel(以下简称DDP)介绍⼀下？"
            },
            {
              "id": "llm_0781",
              "question": "七、DistributedDataParallel(以下简称DDP) 优点有哪些？"
            },
            {
              "id": "llm_0782",
              "question": "⼋、DistributedDataParallel(以下简称DDP) 缺点有哪些？"
            },
            {
              "id": "llm_0783",
              "question": "⼀、torch.multiprocessing 函数介绍⼀下？"
            },
            {
              "id": "llm_0784",
              "question": "⼆、torch.multiprocessing 函数如何使⽤？"
            },
            {
              "id": "llm_0785",
              "question": "三、介绍⼀下 共享CUDA张量？"
            },
            {
              "id": "llm_0786",
              "question": "四、介绍⼀下 共享策略？"
            },
            {
              "id": "llm_0787",
              "question": "为什么需要 AMP混合精度训练？"
            },
            {
              "id": "llm_0788",
              "question": "⼆、为什么需要⾃动混合精度？"
            },
            {
              "id": "llm_0789",
              "question": "三、混合精度训练的优点是什么？"
            },
            {
              "id": "llm_0790",
              "question": "四、混合精度训练的缺点是什么？"
            },
            {
              "id": "llm_0791",
              "question": "五、混合精度训练的关键技术是什么？"
            },
            {
              "id": "llm_0792",
              "question": "六、介绍⼀下 混合精度训练 动态损失缩放？"
            },
            {
              "id": "llm_0793",
              "question": "七、如何在PyTorch中使⽤⾃动混合精度？"
            },
            {
              "id": "llm_0794",
              "question": "⼋、如何使⽤ AMP混合精度训练？"
            },
            {
              "id": "llm_0795",
              "question": "⼀、为什么需要 Deepspeed？"
            },
            {
              "id": "llm_0796",
              "question": "⼆、DeepSpeed 基本概念 介绍⼀下？"
            },
            {
              "id": "llm_0797",
              "question": "三、DeepSpeed 通信策略 介绍⼀下？"
            },
            {
              "id": "llm_0798",
              "question": "四、DeepSpeed 如何使⽤？"
            },
            {
              "id": "llm_0799",
              "question": "五、DeepSpeed 代码实现？"
            },
            {
              "id": "llm_0800",
              "question": "七、训练精度 介绍⼀下？"
            },
            {
              "id": "llm_0801",
              "question": "⼋、获取模型参数 介绍⼀下？"
            },
            {
              "id": "llm_0802",
              "question": "⼀、为什么需要 accelerate 分布式训练？"
            },
            {
              "id": "llm_0803",
              "question": "⼆、什么是 accelerate 分布式训练？"
            },
            {
              "id": "llm_0804",
              "question": "三、accelerate 分布式训练 原理讲解？"
            },
            {
              "id": "llm_0805",
              "question": "四、accelerate 分布式训练 如何实践？"
            },
            {
              "id": "llm_0806",
              "question": "⼀、什么是 3D 并⾏？"
            },
            {
              "id": "llm_0807",
              "question": "⼆、3D 并⾏ 策略有哪些？"
            },
            {
              "id": "llm_0808",
              "question": "三、为什么需要 ZeRO？"
            },
            {
              "id": "llm_0809",
              "question": "四、ZeRO 的 核⼼思想是什么？"
            },
            {
              "id": "llm_0810",
              "question": "五、ZeRO 显存如何分配？"
            },
            {
              "id": "llm_0811",
              "question": "六、ZeRO 优化策略是怎么样？"
            },
            {
              "id": "llm_0812",
              "question": "七、ZeRO Oﬄoad后的计算流程是怎么样？"
            },
            {
              "id": "llm_0813",
              "question": "如何给LLM注⼊领域知识？"
            },
            {
              "id": "llm_0814",
              "question": "如果想要快速体验各种模型，该怎么办？"
            },
            {
              "id": "llm_0815",
              "question": "预训练数据 Token 重复 是否影响 模型性能？"
            },
            {
              "id": "llm_0816",
              "question": "SFT需要训练Token数？"
            },
            {
              "id": "llm_0817",
              "question": "1 什么是位置编码？"
            },
            {
              "id": "llm_0818",
              "question": "2 什么是绝对位置编码？"
            },
            {
              "id": "llm_0819",
              "question": "3 什么是相对位置编码？"
            },
            {
              "id": "llm_0820",
              "question": "1 旋转位置编码 RoPE 思路是什么？"
            },
            {
              "id": "llm_0821",
              "question": "2 推导⼀下 旋转位置编码 RoPE？"
            },
            {
              "id": "llm_0822",
              "question": "3 旋转位置编码 RoPE 有什么优点？"
            },
            {
              "id": "llm_0823",
              "question": "4 旋转位置编码 RoPE 被哪些 LLMs 应⽤？"
            },
            {
              "id": "llm_0824",
              "question": "1 什么是 ⻓度外推问题？"
            },
            {
              "id": "llm_0825",
              "question": "2 ⻓度外推问题 的 解决⽅法 有哪些？"
            },
            {
              "id": "llm_0826",
              "question": "1 ALiBi (Attention with Linear Biases) 思路是什么？"
            },
            {
              "id": "llm_0827",
              "question": "2 ALiBi (Attention with Linear Biases) 的偏置矩阵是什么？"
            },
            {
              "id": "llm_0828",
              "question": "有什么作⽤？"
            },
            {
              "id": "llm_0829",
              "question": "3 ALiBi (Attention with Linear Biases) 有什么优点？"
            },
            {
              "id": "llm_0830",
              "question": "4 ALiBi (Attention with Linear Biases) 被哪些 LLMs 应⽤？"
            },
            {
              "id": "llm_0831",
              "question": "1 Byte-Pair Encoding(BPE) 如何构建词典？"
            },
            {
              "id": "llm_0832",
              "question": "1 WordPiece 与 BPE 异同点是什么？"
            },
            {
              "id": "llm_0833",
              "question": "简单介绍⼀下 SentencePiece 思路？"
            },
            {
              "id": "llm_0834",
              "question": "1 举例 介绍⼀下 不同 ⼤模型LLMs 的分词⽅式？"
            },
            {
              "id": "llm_0835",
              "question": "2 介绍⼀下 不同 ⼤模型LLMs 的分词⽅式 的区别？"
            },
            {
              "id": "llm_0836",
              "question": "⼀、为什么需要 构建中⽂tokenization？"
            },
            {
              "id": "llm_0837",
              "question": "⼆、如何对 原始数据预处理？"
            },
            {
              "id": "llm_0838",
              "question": "三、如何构建中⽂的词库？"
            },
            {
              "id": "llm_0839",
              "question": "四、如何使⽤transformers库加载sentencepiece模型？"
            },
            {
              "id": "llm_0840",
              "question": "五、如何合并英⽂词表和中⽂词表？"
            },
            {
              "id": "llm_0841",
              "question": "六、怎么使⽤修改后的词表？"
            },
            {
              "id": "llm_0842",
              "question": "总结⼀下 构建中⽂tokenization？"
            },
            {
              "id": "llm_0843",
              "question": "⼀、为什么需要进⾏继续预训练？"
            },
            {
              "id": "llm_0844",
              "question": "⼆、如何对 继续预训练 数据预处理？"
            },
            {
              "id": "llm_0845",
              "question": "三、如何 构建模型？"
            },
            {
              "id": "llm_0846",
              "question": "四、如何 使⽤模型？"
            },
            {
              "id": "llm_0847",
              "question": "⼀、为什么需要对预训练模型进⾏指令微调？"
            },
            {
              "id": "llm_0848",
              "question": "⼆、对预训练模型进⾏指令微调 数据 如何处理？"
            },
            {
              "id": "llm_0849",
              "question": "三、对预训练模型进⾏指令微调 tokenization 如何构建？"
            },
            {
              "id": "llm_0850",
              "question": "四、对预训练模型进⾏指令微调 模型 如何构建？"
            },
            {
              "id": "llm_0851",
              "question": "五、是否可以结合 其他库 使⽤？"
            },
            {
              "id": "llm_0852",
              "question": "Layer Norm 的计算公式写⼀下？"
            },
            {
              "id": "llm_0853",
              "question": "RMS Norm 的计算公式写⼀下？"
            },
            {
              "id": "llm_0854",
              "question": "RMS Norm 相⽐于 Layer Norm 有什么特点？"
            },
            {
              "id": "llm_0855",
              "question": "Deep Norm 思路？"
            },
            {
              "id": "llm_0856",
              "question": "写⼀下 Deep Norm 代码实现？"
            },
            {
              "id": "llm_0857",
              "question": "Deep Norm 有什么优点？"
            },
            {
              "id": "llm_0858",
              "question": "1 LN 在 LLMs 中的不同位置 有什么区别么？"
            },
            {
              "id": "llm_0859",
              "question": "如果有，能介绍⼀下区别么？"
            },
            {
              "id": "llm_0860",
              "question": "LLMs 各模型分别⽤了 哪种 Layer normalization？"
            },
            {
              "id": "llm_0861",
              "question": "1 介绍⼀下 FFN 块 计算公式？"
            },
            {
              "id": "llm_0862",
              "question": "2 介绍⼀下 GeLU 计算公式？"
            },
            {
              "id": "llm_0863",
              "question": "3 介绍⼀下 Swish 计算公式？"
            },
            {
              "id": "llm_0864",
              "question": "4 介绍⼀下 使⽤ GLU 线性⻔控单元的 FFN 块 计算公式？"
            },
            {
              "id": "llm_0865",
              "question": "5 介绍⼀下 使⽤ GeLU 的 GLU 块 计算公式？"
            },
            {
              "id": "llm_0866",
              "question": "6 介绍⼀下 使⽤ Swish 的 GLU 块 计算公式？"
            },
            {
              "id": "llm_0867",
              "question": "各LLMs 都使⽤哪种激活函数？"
            },
            {
              "id": "llm_0868",
              "question": "当前优化模型最主要技术⼿段有哪些？"
            },
            {
              "id": "llm_0869",
              "question": "推理加速框架有哪⼀些？"
            },
            {
              "id": "llm_0870",
              "question": "都有什么特点？"
            },
            {
              "id": "llm_0871",
              "question": "1 vLLM 的 功能有哪些？"
            },
            {
              "id": "llm_0872",
              "question": "2 vLLM 的 优点有哪些？"
            },
            {
              "id": "llm_0873",
              "question": "3 vLLM 的 缺点有哪些？"
            },
            {
              "id": "llm_0874",
              "question": "4 vLLM 离线批量推理？"
            },
            {
              "id": "llm_0875",
              "question": "5 vLLM API Server？"
            },
            {
              "id": "llm_0876",
              "question": "1 介绍⼀下 Text generation inference？"
            },
            {
              "id": "llm_0877",
              "question": "2 Text generation inference 的 功能有哪些？"
            },
            {
              "id": "llm_0878",
              "question": "3 Text generation inference 的 优点有哪些？"
            },
            {
              "id": "llm_0879",
              "question": "4 Text generation inference 的 缺点有哪些？"
            },
            {
              "id": "llm_0880",
              "question": "5 Text generation inference 的 使⽤docker运⾏web server？"
            },
            {
              "id": "llm_0881",
              "question": "⼀、vLLM ⽤于⼤模型并⾏推理加速 存在什么问题？"
            },
            {
              "id": "llm_0882",
              "question": "⼆、vLLM 如何 优化 ⼤模型并⾏推理加速？"
            },
            {
              "id": "llm_0883",
              "question": "三、什么是 PagedAttention？"
            },
            {
              "id": "llm_0884",
              "question": "四、 PagedAttention 如何存储 连续的key和value？"
            },
            {
              "id": "llm_0885",
              "question": "五、 PagedAttention 技术细节？"
            },
            {
              "id": "llm_0886",
              "question": "六、 PagedAttention 如何 实现安全共享？"
            },
            {
              "id": "llm_0887",
              "question": "七、 PagedAttention 源码介绍？"
            },
            {
              "id": "llm_0888",
              "question": "2 为什么 需要 vLLM？"
            },
            {
              "id": "llm_0889",
              "question": "3 vLLM 具有哪些特点？"
            },
            {
              "id": "llm_0890",
              "question": "4 vLLM ⽀持哪些 Huggingface 模型？"
            },
            {
              "id": "llm_0891",
              "question": "⼆、vLLM 性能如何？"
            },
            {
              "id": "llm_0892",
              "question": "四、vLLM 如何安装？"
            },
            {
              "id": "llm_0893",
              "question": "五、vLLM 如何使⽤？"
            },
            {
              "id": "llm_0894",
              "question": "⼀、为什么需要 FasterTransformer？"
            },
            {
              "id": "llm_0895",
              "question": "⼆、FasterTransformer 介绍⼀下？"
            },
            {
              "id": "llm_0896",
              "question": "三、FasterTransformer 核⼼是什么？"
            },
            {
              "id": "llm_0897",
              "question": "四、FasterTransformer 优化？"
            },
            {
              "id": "llm_0898",
              "question": "2 为什么 需要 LightLLM？"
            },
            {
              "id": "llm_0899",
              "question": "3 ⽬前 LLM推理框架 有 哪些？"
            },
            {
              "id": "llm_0900",
              "question": "⼆、LightLLM 介绍⼀下？"
            },
            {
              "id": "llm_0901",
              "question": "1 什么是 LightLLM？"
            },
            {
              "id": "llm_0902",
              "question": "2 Token Attention 介绍？"
            },
            {
              "id": "llm_0903",
              "question": "3 Eﬃcient Router 介绍？"
            },
            {
              "id": "llm_0904",
              "question": "三、LightLLM 性能表现 介绍？"
            },
            {
              "id": "llm_0905",
              "question": "四、LightLLM 依赖包 有哪些？"
            },
            {
              "id": "llm_0906",
              "question": "五、LightLLM 如何安装？"
            },
            {
              "id": "llm_0907",
              "question": "六、LightLLM 如何使⽤？"
            },
            {
              "id": "llm_0908",
              "question": "LightLLM ⽀持模型 LLMs 模型？"
            },
            {
              "id": "llm_0909",
              "question": "1 传统 Attention 存在哪些问题？"
            },
            {
              "id": "llm_0910",
              "question": "3 Attention 变体有哪些？"
            },
            {
              "id": "llm_0911",
              "question": "1 Multi-head Attention 存在什么问题？"
            },
            {
              "id": "llm_0912",
              "question": "2 介绍⼀下 Multi-Query Attention？"
            },
            {
              "id": "llm_0913",
              "question": "3 对⽐⼀下 Multi-head Attention 和 Multi-Query Attention？"
            },
            {
              "id": "llm_0914",
              "question": "4 Multi-Query Attention 这样做的好处是什么？"
            },
            {
              "id": "llm_0915",
              "question": "5 有 哪些模型 是 使⽤ Multi-Query Attention？"
            },
            {
              "id": "llm_0916",
              "question": "1 什么是 Grouped-query Attention？"
            },
            {
              "id": "llm_0917",
              "question": "2 有哪些⼤模型使⽤ Grouped-query Attention？"
            },
            {
              "id": "llm_0918",
              "question": "7 并⾏ transformer block 介绍⼀下？"
            },
            {
              "id": "llm_0919",
              "question": "⼀、什么是⼤模型幻觉？"
            },
            {
              "id": "llm_0920",
              "question": "⼆、为什么LLM会产⽣幻觉？"
            },
            {
              "id": "llm_0921",
              "question": "三、为什么需要解决LLM的幻觉问题？"
            },
            {
              "id": "llm_0922",
              "question": "四、幻觉⼀定是有害的吗？"
            },
            {
              "id": "llm_0923",
              "question": "五、幻觉有哪些不同类型？"
            },
            {
              "id": "llm_0924",
              "question": "六、如何度量幻觉？"
            },
            {
              "id": "llm_0925",
              "question": "七、如何缓解LLM幻觉？"
            },
            {
              "id": "llm_0926",
              "question": "⼋、LLMs什么时候最容易产⽣幻觉？"
            },
            {
              "id": "llm_0927",
              "question": "⼀、什么是 ⼤模型幻觉问题？"
            },
            {
              "id": "llm_0928",
              "question": "⼆、为什么 会 出现 ⼤模型幻觉问题？"
            },
            {
              "id": "llm_0929",
              "question": "三、如何 评估 ⼤模型幻觉问题？"
            },
            {
              "id": "llm_0930",
              "question": "四、如何 缓解 ⼤模型幻觉问题？"
            },
            {
              "id": "llm_0931",
              "question": "⼀、为什么 会 出现 ⼤模型幻觉？"
            },
            {
              "id": "llm_0932",
              "question": "⼆、如何 缓解 ⼤模型幻觉？"
            },
            {
              "id": "llm_0933",
              "question": "LLMs 训练数据 和 数据量 对⽐如何？"
            },
            {
              "id": "llm_0934",
              "question": "你了解baichuan-7B解构么？"
            },
            {
              "id": "llm_0935",
              "question": "介绍⼀下？"
            },
            {
              "id": "llm_0936",
              "question": "baichuan-7B 如何 收集原始数据并 构建 训练数据？"
            },
            {
              "id": "llm_0937",
              "question": "baichuan-7B 如何 提⾼ 训练稳定性和吞吐？"
            },
            {
              "id": "llm_0938",
              "question": "相⽐于 baichuan-7B，baichuan-13B 的 特点体现在哪⾥？"
            },
            {
              "id": "llm_0939",
              "question": "如何 对 baichuan-13B 进⾏推理和部署？"
            },
            {
              "id": "llm_0940",
              "question": "如何 对 baichuan-13B 进⾏微调？"
            },
            {
              "id": "llm_0941",
              "question": "1 baichuan-53B 相⽐于 baichuan-7B 和 baichuan-13B 有哪些优势？"
            },
            {
              "id": "llm_0942",
              "question": "2 baichuan-53B 如何对 预训练数据 做处理？"
            },
            {
              "id": "llm_0943",
              "question": "3 baichuan-53B 如何进⾏ 搜索增强？"
            },
            {
              "id": "llm_0944",
              "question": "1 baichuan 进⾏微调时，领域数据：通⽤数据配⽐？"
            },
            {
              "id": "llm_0945",
              "question": "⼀、什么是思维链提示？"
            },
            {
              "id": "llm_0946",
              "question": "⼆、思维链提示本质是什么？"
            },
            {
              "id": "llm_0947",
              "question": "三、思维链提示 与 标准的提示学习⽅法有什么不同？"
            },
            {
              "id": "llm_0948",
              "question": "四、思维链提示 为什么可以提⾼语⾔模型的复杂推理能⼒？"
            },
            {
              "id": "llm_0949",
              "question": "它的优势在哪⾥？"
            },
            {
              "id": "llm_0950",
              "question": "五、思维链提示 适⽤场景 有 哪些？"
            },
            {
              "id": "llm_0951",
              "question": "六、思维链提示 ⽬前还存在哪些不⾜点？"
            },
            {
              "id": "llm_0952",
              "question": "七、思维链提示 对推动语⾔模型复杂推理能⼒研究有哪些启发和影响？"
            },
            {
              "id": "llm_0953",
              "question": "⼋、思维链提示 对实现真正的通⽤⼈⼯智能仍⾯临哪些挑战？"
            },
            {
              "id": "llm_0954",
              "question": "九、如何通过增加模型规模来获得语⾔模型强⼤的思路链推理能⼒的？"
            },
            {
              "id": "llm_0955",
              "question": "⼗、你认为可以在哪些其他⽅⾯应⽤“思路链提示”这⼀思路来提升语⾔模型的能⼒？"
            },
            {
              "id": "llm_0956",
              "question": "⼗⼀、如果需要你对 思维链提示 进⾏改进，你觉得你会改进哪些地⽅？"
            },
            {
              "id": "llm_0957",
              "question": "⼗⼆、思维链提示 未来研究⽅向？"
            },
            {
              "id": "llm_0958",
              "question": "什么是 思维链 Chain-of-Thought（COT）？"
            },
            {
              "id": "llm_0959",
              "question": "思维链 Chain-of-Thought（COT）是思路是什么？"
            },
            {
              "id": "llm_0960",
              "question": "思维链 Chain-of-Thought（COT）存在问题？"
            },
            {
              "id": "llm_0961",
              "question": "为什么需要 思维树 Tree of Thoughts（TOT）？"
            },
            {
              "id": "llm_0962",
              "question": "什么是 思维树 Tree of Thoughts（TOT）？"
            },
            {
              "id": "llm_0963",
              "question": "思维树 Tree of Thoughts（TOT）涉及问题有哪些？"
            },
            {
              "id": "llm_0964",
              "question": "为什么 需要 思维图 Graph of Thoughts（GOT）？"
            },
            {
              "id": "llm_0965",
              "question": "什么是 思维图 Graph of Thoughts（GOT）？"
            },
            {
              "id": "llm_0966",
              "question": "思维图 Graph of Thoughts（GOT）核⼼思想是什么？"
            },
            {
              "id": "llm_0967",
              "question": "为什么 需要 思维算法 Algorithm of Thoughts（AOT）？"
            },
            {
              "id": "llm_0968",
              "question": "思维算法 Algorithm of Thoughts（AOT）思路是什么？"
            },
            {
              "id": "llm_0969",
              "question": "思维算法 Algorithm of Thoughts（AOT） vs 其他 COT 的 区别？"
            },
            {
              "id": "llm_0970",
              "question": "思维链 Chain-of-Thought（COT） 有哪些 应⽤场景？"
            },
            {
              "id": "llm_0971",
              "question": "思维链 Chain-of-Thought（COT） 有哪些 局限性？"
            },
            {
              "id": "llm_0972",
              "question": "⼀、为什么需要 Graph RAG？"
            },
            {
              "id": "llm_0973",
              "question": "⼆、什么是 Graph RAG？"
            },
            {
              "id": "llm_0974",
              "question": "三、Graph RAG 思路介绍？"
            },
            {
              "id": "llm_0975",
              "question": "四、⽤代码 介绍 Graph RAG？"
            },
            {
              "id": "llm_0976",
              "question": "五、⽤ 示例 介绍 Graph RAG？"
            },
            {
              "id": "llm_0977",
              "question": "六、Graph RAG 排序优化⽅式？"
            },
            {
              "id": "llm_0978",
              "question": "💡 LLMs 存在模型幻觉问题，请问如何处理？"
            },
            {
              "id": "llm_0979",
              "question": "💡 基于LLM+向量库的文档对话 思路是怎么样？"
            },
            {
              "id": "llm_0980",
              "question": "💡 基于LLM+向量库的文档对话 核心技术是什么？"
            },
            {
              "id": "llm_0981",
              "question": "💡 基于LLM+向量库的文档对话 prompt 模板 如何构建？"
            },
            {
              "id": "llm_0982",
              "question": "对于查询新闻标题的情况，可以使用\"请问有关于XXX的新闻吗？"
            },
            {
              "id": "llm_0983",
              "question": "llama 输入句子长度理论上可以无限长吗？"
            },
            {
              "id": "llm_0984",
              "question": "什么情况用 Bert 模型，什么情况用 LLaMA、ChatGLM 类大模？"
            },
            {
              "id": "llm_0985",
              "question": "各个专业领域是否需要各自的大模型来服务？"
            },
            {
              "id": "llm_0986",
              "question": "如何让大模型处理更长的文本？"
            },
            {
              "id": "llm_0987",
              "question": "为什么 SFT 之后感觉 LLM 傻了？"
            },
            {
              "id": "llm_0988",
              "question": "SFT 指令微调数据如何构建？"
            },
            {
              "id": "llm_0989",
              "question": "领域模型 Continue PreTrain 数据选取？"
            },
            {
              "id": "llm_0990",
              "question": "忘通用能力？"
            },
            {
              "id": "llm_0991",
              "question": "学习到更多的知识？"
            },
            {
              "id": "llm_0992",
              "question": "进行 SFT 操作的时候，基座模型选用 Chat 还是 Base？"
            },
            {
              "id": "llm_0993",
              "question": "领域模型微调指令&数据输入格式要求？"
            },
            {
              "id": "llm_0994",
              "question": "领域模型微调领域评测集构建？"
            },
            {
              "id": "llm_0995",
              "question": "如何训练自己的大模型？"
            },
            {
              "id": "llm_0996",
              "question": "训练中文大模型有啥经验？"
            },
            {
              "id": "llm_0997",
              "question": "预训练和微调哪个阶段注入知识的？"
            },
            {
              "id": "llm_0998",
              "question": "想让模型学习某领域或行业知识，是应该预训练还是应该微调？"
            },
            {
              "id": "llm_0999",
              "question": "微调后的模型出现能力劣化，灾难性遗忘是怎么回事？"
            },
            {
              "id": "llm_1000",
              "question": "微调模型需要多大显存？"
            },
            {
              "id": "llm_1001",
              "question": "大模型 LLM 进行 SFT 操作的时候在学习什么？"
            },
            {
              "id": "llm_1002",
              "question": "预训练和 SFT 操作有什么不同？"
            },
            {
              "id": "llm_1003",
              "question": "样本量规模增大，训练出现 OOM 报错，怎么解决？"
            },
            {
              "id": "llm_1004",
              "question": "大模型 LLM 进行 SFT 如何对样本进行优化？"
            },
            {
              "id": "llm_1005",
              "question": "模型参数迭代实验步骤？"
            },
            {
              "id": "llm_1006",
              "question": "为什么需要进行参选微调？"
            },
            {
              "id": "llm_1007",
              "question": "参数微调的原因有哪些？"
            },
            {
              "id": "llm_1008",
              "question": "模型参数微调的方式有那些？"
            },
            {
              "id": "llm_1009",
              "question": "你最常用哪些方法？"
            },
            {
              "id": "llm_1010",
              "question": "prompt tuning 和 prefix tuning 在微调上的区别是什么？"
            },
            {
              "id": "llm_1011",
              "question": "LLaMA-adapter 如何实现稳定训练？"
            },
            {
              "id": "llm_1012",
              "question": "LoRA 原理与使用技巧有那些？"
            },
            {
              "id": "llm_1013",
              "question": "LoRA 微调优点是什么？"
            },
            {
              "id": "llm_1014",
              "question": "AdaLoRA 的思路是怎么样的？"
            },
            {
              "id": "llm_1015",
              "question": "LoRA 权重合入 chatglm 模型的方法？"
            },
            {
              "id": "llm_1016",
              "question": "P-tuning 讲一下？"
            },
            {
              "id": "llm_1017",
              "question": "与 P-tuning v2 区别在哪里？"
            },
            {
              "id": "llm_1018",
              "question": "优点与缺点？"
            },
            {
              "id": "llm_1019",
              "question": "训练一个通用大模型的流程有那些？"
            },
            {
              "id": "llm_1020",
              "question": "DDO 与 DPO 的区别是什么？"
            },
            {
              "id": "llm_1021",
              "question": "是否接触过 embeding 模型的微调方法？"
            },
            {
              "id": "llm_1022",
              "question": "有哪些省内存的大语言模型训练/微调/推理方法？"
            },
            {
              "id": "llm_1023",
              "question": "大模型 (LLMs) 评测有那些方法？"
            },
            {
              "id": "llm_1024",
              "question": "如何衡量大模型的效果？"
            },
            {
              "id": "llm_1025",
              "question": "如何解决三个阶段的训练（SFT->RM->PPO）过程较长，更新？"
            },
            {
              "id": "llm_1026",
              "question": "迭代较慢问题？"
            },
            {
              "id": "llm_1027",
              "question": "模型训练的数据集问题：一般数据集哪里找？"
            },
            {
              "id": "llm_1028",
              "question": "为什么需要进行模型量化及原理？"
            },
            {
              "id": "llm_1029",
              "question": "大模型词表扩充的方法及工具？"
            },
            {
              "id": "llm_1030",
              "question": "大模型应用框架及其功能？"
            },
            {
              "id": "llm_1031",
              "question": "搭建大模型应用遇到过那些问题？"
            },
            {
              "id": "llm_1032",
              "question": "如何解决的？"
            },
            {
              "id": "llm_1033",
              "question": "如何提升大模型的检索效果？"
            },
            {
              "id": "llm_1034",
              "question": "是否了解上下文压缩方法？"
            },
            {
              "id": "llm_1035",
              "question": "如何实现窗口上下文检索？"
            },
            {
              "id": "llm_1036",
              "question": "开源的 RAG 框架有哪些，你比较了解？"
            },
            {
              "id": "llm_1037",
              "question": "向量库有那些？"
            },
            {
              "id": "llm_1038",
              "question": "各自优点与区别？"
            },
            {
              "id": "llm_1039",
              "question": "向量数据库有那些？"
            },
            {
              "id": "llm_1040",
              "question": "文档块的大小？"
            },
            {
              "id": "llm_1041",
              "question": "声，该如何解决这样的问题？"
            },
            {
              "id": "llm_1042",
              "question": "RAG(检索增强生成)对于大模型来说，有什么好处？"
            },
            {
              "id": "llm_1043",
              "question": "Self-attention 的公式及参数量？"
            },
            {
              "id": "llm_1044",
              "question": "为什么用多头？"
            },
            {
              "id": "llm_1045",
              "question": "以根号 d？"
            },
            {
              "id": "llm_1046",
              "question": "为什么用多头 (Multi-Head) 注意力：？"
            },
            {
              "id": "llm_1047",
              "question": "为什么要除以根号 D:？"
            },
            {
              "id": "llm_1048",
              "question": "LangChain 包含哪些核心概念？"
            },
            {
              "id": "llm_1049",
              "question": "如何使用 LangChain？"
            },
            {
              "id": "llm_1050",
              "question": "LangChain 支持哪些功能？"
            },
            {
              "id": "llm_1051",
              "question": "LangChain 如何使用？"
            },
            {
              "id": "llm_1052",
              "question": "LangChain 存在哪些问题及方法方案？"
            },
            {
              "id": "llm_1053",
              "question": "LangChain 替代方案？"
            },
            {
              "id": "llm_1054",
              "question": "LangChain 中 Components and Chains 是什么？"
            },
            {
              "id": "llm_1055",
              "question": "LangChain 中 Prompt Templates and Values 是什么？"
            },
            {
              "id": "llm_1056",
              "question": "LangChain 中 Example Selectors 是什么？"
            },
            {
              "id": "llm_1057",
              "question": "LangChain 中 Output Parsers 是什么？"
            },
            {
              "id": "llm_1058",
              "question": "LangChain 中 Indexes and Retrievers 是什么？"
            },
            {
              "id": "llm_1059",
              "question": "LangChain 中 Chat Message History 是什么？"
            },
            {
              "id": "llm_1060",
              "question": "LangChain 中 Agents and Toolkits 是什么？"
            },
            {
              "id": "llm_1061",
              "question": "LangChain 如何调用 LLMs 生成回复？"
            },
            {
              "id": "llm_1062",
              "question": "LangChain 如何修改提示模板？"
            },
            {
              "id": "llm_1063",
              "question": "LangChain 如何链接多个组件处理一个特定的下游任务？"
            },
            {
              "id": "llm_1064",
              "question": "LangChain 如何 Embedding&vector store？"
            },
            {
              "id": "llm_1065",
              "question": "大模型进行训练，用的是什么框架？"
            },
            {
              "id": "llm_1066",
              "question": "业内常用的分布式 AI 框架？"
            },
            {
              "id": "llm_1067",
              "question": "数据并行、张量并行、流水线并行的原理及区别？"
            },
            {
              "id": "llm_1068",
              "question": "推理优化技术 Flash Attention 的作用是什么？"
            },
            {
              "id": "llm_1069",
              "question": "推理优化技术 Paged Attention 的作用是什么？"
            },
            {
              "id": "llm_1070",
              "question": "CPU-offload，ZeRO-offload 了解？"
            },
            {
              "id": "llm_1071",
              "question": "ZeRO，零冗余优化器的三个阶段？"
            },
            {
              "id": "llm_1072",
              "question": "混合精度训练的优点是什么？"
            },
            {
              "id": "llm_1073",
              "question": "可能带来什么问题？"
            },
            {
              "id": "llm_1074",
              "question": "Megatron-DeepSpeed 方法？"
            },
            {
              "id": "llm_1075",
              "question": "Megatron-LM 方法？"
            },
            {
              "id": "llm_1076",
              "question": "DeepSpeed 方法？"
            },
            {
              "id": "llm_1077",
              "question": "为什么大模型推理时显存涨的那么多还一直占着？"
            },
            {
              "id": "llm_1078",
              "question": "大模型在 GPU 和 CPU 上推理速度如何？"
            },
            {
              "id": "llm_1079",
              "question": "推理速度上， int8 和 fp16 比起来怎么样？"
            },
            {
              "id": "llm_1080",
              "question": "大模型有推理能力吗？"
            },
            {
              "id": "llm_1081",
              "question": "大模型生成时的参数怎么设置？"
            },
            {
              "id": "llm_1082",
              "question": "如何让大模型输出合规化？"
            },
            {
              "id": "llm_1083",
              "question": "目前主流的开源模型体系有哪些？"
            },
            {
              "id": "llm_1084",
              "question": "涌现能力是啥原因？"
            },
            {
              "id": "llm_1085",
              "question": "1 self-attention 的计算方式？"
            },
            {
              "id": "llm_1086",
              "question": "2 说一下 transformer 的模型架构和细节？"
            },
            {
              "id": "llm_1087",
              "question": "4 BART、llama、gpt、t5、palm 等主流模型异同点？"
            },
            {
              "id": "llm_1088",
              "question": "5 个人项目中模型的优化点和技术细节？"
            },
            {
              "id": "llm_1089",
              "question": "6 个人项目中如何选择最佳的指令策略，以及其对模型效果的影响？"
            },
            {
              "id": "llm_1090",
              "question": "7 个人项目中模型如何评测、数据集，评测指标等？"
            },
            {
              "id": "llm_1091",
              "question": "说明：你不能倾斜容器。？"
            },
            {
              "id": "llm_1092",
              "question": "解释：图中垂直线代表输入数组 [1,8,6,2,5,4,8,3,7]。在此情况下，容器能够容纳水（表示为？"
            },
            {
              "id": "llm_1093",
              "question": "llama2 中使用的注意力机制是什么？"
            },
            {
              "id": "llm_1094",
              "question": "了解 langchain 吗？"
            },
            {
              "id": "llm_1095",
              "question": "对位置编码熟悉吗？"
            },
            {
              "id": "llm_1096",
              "question": "RLHF 的具体工程是什么？"
            },
            {
              "id": "llm_1097",
              "question": "包含了哪几个模型？"
            },
            {
              "id": "llm_1098",
              "question": "显存不够一般怎么解决的？"
            },
            {
              "id": "llm_1099",
              "question": "几种主流大模型的 loss 了解过吗？"
            },
            {
              "id": "llm_1100",
              "question": "有哪些异同？"
            },
            {
              "id": "llm_1101",
              "question": "了解半精度训练吗？"
            },
            {
              "id": "llm_1102",
              "question": "deepspeed 用过吗？"
            },
            {
              "id": "llm_1103",
              "question": "1 self-attention 的公式及参数量，为什么用多头，为什么要除以根号 d？"
            },
            {
              "id": "llm_1104",
              "question": "2 你能不能介绍一下 BERT 和 GPT 的训练方式(预训练任务训练细节)的区别？"
            },
            {
              "id": "llm_1105",
              "question": "3 简单介绍一下，transformer 架构？"
            },
            {
              "id": "llm_1106",
              "question": "4 大模型的模型架构有哪些？"
            },
            {
              "id": "llm_1107",
              "question": "5 chatGPT 对比 GPT-3 的性能提升主要来源于哪些方面？"
            },
            {
              "id": "llm_1108",
              "question": "7 大模型中常见的位置编码？"
            },
            {
              "id": "llm_1109",
              "question": "8 大模型高效参数微调方法？"
            },
            {
              "id": "llm_1110",
              "question": "样本构建的流程是怎样的，并且为什么 GCN 相较于其他方法在效果上更胜一筹？"
            },
            {
              "id": "llm_1111",
              "question": "节点特征指的是什么？"
            },
            {
              "id": "llm_1112",
              "question": "查询流程？"
            },
            {
              "id": "llm_1113",
              "question": "使用什么向量数据库？"
            },
            {
              "id": "llm_1114",
              "question": "介绍一下 RAG 原理？"
            },
            {
              "id": "llm_1115",
              "question": "RAG 如何解决多实体提问问题？"
            },
            {
              "id": "llm_1116",
              "question": "用户提问：感冒和咳嗽需要吃什么药？"
            },
            {
              "id": "llm_1117",
              "question": "Prompt 是如何生成的，优化目标是什么，任务是什么？"
            },
            {
              "id": "llm_1118",
              "question": "OCR 抽取效果不好，需要怎么排查问题？"
            },
            {
              "id": "llm_1119",
              "question": "源的 ClIP？"
            },
            {
              "id": "llm_1120",
              "question": "对分布式训练有经验么？"
            },
            {
              "id": "llm_1121",
              "question": "RNN 与 GNN 之间有哪些区别，以及它们各自适用于哪些场景？"
            },
            {
              "id": "llm_1122",
              "question": "GPT 和 BERT 在文本表征方面有哪些结构和工作原理上的差异？"
            },
            {
              "id": "llm_1123",
              "question": "因为说了 BERT 好训练一些，问了为什么？"
            },
            {
              "id": "llm_1124",
              "question": "1 分布式训练框架都了解哪些，能不能简单介绍一下？"
            },
            {
              "id": "llm_1125",
              "question": "2 你了解 deepspeed，那介绍 zero1 2 3 分别是什么，分析训练时候显存占用？"
            },
            {
              "id": "llm_1126",
              "question": "3 说一下 Transformer 的架构和其内部细节？"
            },
            {
              "id": "llm_1127",
              "question": "4 介绍大模型推理过程中，可以通过调节哪些参数提高性能？"
            },
            {
              "id": "llm_1128",
              "question": "6 大模型训练的三种并行是什么？"
            },
            {
              "id": "llm_1129",
              "question": "通讯开销比？"
            },
            {
              "id": "llm_1130",
              "question": "使用不超过 mm 复杂度的代码求解其两两之间的欧式距离？"
            },
            {
              "id": "llm_1131",
              "question": "聊一下 RAG 项目总体思路？"
            },
            {
              "id": "llm_1132",
              "question": "使用外挂知识库主要是为了解决什么问题？"
            },
            {
              "id": "llm_1133",
              "question": "如何评价 RAG 项目的效果好坏，即指标是什么？"
            },
            {
              "id": "llm_1134",
              "question": "在做 RAG 项目过程中遇到哪些问题？"
            },
            {
              "id": "llm_1135",
              "question": "怎么解决的？"
            },
            {
              "id": "llm_1136",
              "question": "RAG 项目里面有哪一些亮点？"
            },
            {
              "id": "llm_1137",
              "question": "数据集怎么构建的，什么规模，评估指标是什么，这些指标存在哪些问题？"
            },
            {
              "id": "llm_1138",
              "question": "模型底座是什么，这些不同底座什么区别，什么规模？"
            },
            {
              "id": "llm_1139",
              "question": "同方法的差别？"
            },
            {
              "id": "llm_1140",
              "question": "模型推理是怎么做的，有没有 cot，tot 等等，还是单轮？"
            },
            {
              "id": "llm_1141",
              "question": "大模型可控性如何实现，怎么保证可控性？"
            },
            {
              "id": "llm_1142",
              "question": "模型部署的平台，推理效率怎么样，如何提升推理效率？"
            },
            {
              "id": "llm_1143",
              "question": "项目最后上线了么，上线之后发现什么问题，如何解决？"
            },
            {
              "id": "llm_1144",
              "question": "拟一下，数据集格式是否要调整成这样，数据形式是什么，怎么拆分成多轮形式？"
            },
            {
              "id": "llm_1145",
              "question": "1 简单介绍一下大模型存在哪些问题？"
            },
            {
              "id": "llm_1146",
              "question": "有什么好的解决方法？"
            },
            {
              "id": "llm_1147",
              "question": "2 大模型加速框架了解多少，知不知道原理 如何进行加速优化？"
            },
            {
              "id": "llm_1148",
              "question": "1 介绍一下，现在几种流行的大模型架构？"
            },
            {
              "id": "llm_1149",
              "question": "2 说一下 prefix LM 和 casualLM 的区别？"
            },
            {
              "id": "llm_1150",
              "question": "3 在大模型任务中，你用到 LoRA，讲一下 LoRA 实现原理？"
            },
            {
              "id": "llm_1151",
              "question": "4 instruction tuning 和 prompt learning 的区别？"
            },
            {
              "id": "llm_1152",
              "question": "5 项目中你用到的大模型推理加速工具是什么？"
            },
            {
              "id": "llm_1153",
              "question": "简单介绍一下为什么用它？"
            },
            {
              "id": "llm_1154",
              "question": "请简述什么是大模型，以及它与传统模型的主要区别是什么？"
            },
            {
              "id": "llm_1155",
              "question": "谈谈你对 Transformer 模型的理解，以及它在自然语言处理中的应用。 .....................2？"
            },
            {
              "id": "llm_1156",
              "question": "你如何评估大模型的性能？"
            },
            {
              "id": "llm_1157",
              "question": "有哪些常用的评估指标？"
            },
            {
              "id": "llm_1158",
              "question": "请描述一下你如何对大模型进行优化，以提高其性能和效率。 ......................................3？"
            },
            {
              "id": "llm_1159",
              "question": "同时降低模型大小和推理时间。你是否有过使用或开发大模型的经验？"
            },
            {
              "id": "llm_1160",
              "question": "面对大模型训练和推理所需的庞大计算资源，你有什么解决方案或建议？"
            },
            {
              "id": "llm_1161",
              "question": "在开发大模型时，你如何确保模型的可解释性和公平性？"
            },
            {
              "id": "llm_1162",
              "question": "请简述 Transformer 的基本结构和工作原理？"
            },
            {
              "id": "llm_1163",
              "question": "多头自注意力机制的作用是什么？"
            },
            {
              "id": "llm_1164",
              "question": "为什么 Transformer 使用位置编码（Positional Encoding）？"
            },
            {
              "id": "llm_1165",
              "question": "如何优化 Transformer 模型的性能？"
            },
            {
              "id": "llm_1166",
              "question": "Transformer 在自然语言处理中有哪些应用？"
            },
            {
              "id": "llm_1167",
              "question": "请谈谈你对 Transformer 未来发展的看法？"
            },
            {
              "id": "llm_1168",
              "question": "请简述你了解的大模型的主要结构特点。 .............................................................................5？"
            },
            {
              "id": "llm_1169",
              "question": "大模型中的注意力机制是如何工作的？"
            },
            {
              "id": "llm_1170",
              "question": "它在大模型中起到了什么作用？"
            },
            {
              "id": "llm_1171",
              "question": "大模型中的优化算法有哪些常见的选择？"
            },
            {
              "id": "llm_1172",
              "question": "如何处理大模型训练过程中的梯度消失或梯度爆炸问题？"
            },
            {
              "id": "llm_1173",
              "question": "在大模型设计中，如何权衡模型的复杂度和性能？"
            },
            {
              "id": "llm_1174",
              "question": "请解释什么是注意力机制，并举例说明其应用场景。 ......................................................7？"
            },
            {
              "id": "llm_1175",
              "question": "注意力机制是如何工作的？"
            },
            {
              "id": "llm_1176",
              "question": "多头注意力机制（Multi-head Attention）是什么？"
            },
            {
              "id": "llm_1177",
              "question": "它相比单头注意力有什么优势？"
            },
            {
              "id": "llm_1178",
              "question": "注意力机制如何解决长序列依赖问题？"
            },
            {
              "id": "llm_1179",
              "question": "在实际应用中，如何调整注意力机制的参数以优化模型性能？"
            },
            {
              "id": "llm_1180",
              "question": "请解释什么是位置编码，为什么在大模型中需要位置编码？"
            },
            {
              "id": "llm_1181",
              "question": "请简述 Transformer 中的位置编码是如何实现的？"
            },
            {
              "id": "llm_1182",
              "question": "相对位置编码和绝对位置编码有什么区别？"
            },
            {
              "id": "llm_1183",
              "question": "位置编码有哪些优缺点？"
            },
            {
              "id": "llm_1184",
              "question": "在大模型中，除了位置编码，还有哪些方法可以用来处理序列中的位置信息？"
            },
            {
              "id": "llm_1185",
              "question": "请简述 Tokenizer 的作用及其在 NLP 模型中的重要性。 ............................................... 12？"
            },
            {
              "id": "llm_1186",
              "question": "请描述一种你熟悉的 Tokenizer 实现方法，并解释其原理。 ....................................... 13？"
            },
            {
              "id": "llm_1187",
              "question": "在处理多语言文本时，Tokenizer 会遇到哪些挑战？"
            },
            {
              "id": "llm_1188",
              "question": "你如何解决这些挑战？"
            },
            {
              "id": "llm_1189",
              "question": "在模型训练和推理过程中，如何保证 Tokenizer 的一致性？"
            },
            {
              "id": "llm_1190",
              "question": "请解释什么是大模型微调，以及它在自然语言处理任务中的作用。 ........................ 14？"
            },
            {
              "id": "llm_1191",
              "question": "为什么需要对大模型进行微调？"
            },
            {
              "id": "llm_1192",
              "question": "在进行大模型微调时，有哪些常见的策略或技巧？"
            },
            {
              "id": "llm_1193",
              "question": "请简述大模型性能评估的主要步骤。.................................................................................... 16？"
            },
            {
              "id": "llm_1194",
              "question": "在大模型性能评估中，你通常使用哪些评估指标？"
            },
            {
              "id": "llm_1195",
              "question": "请解释什么是过拟合和欠拟合，并说明如何在大模型评测中避免它们。............... 17？"
            },
            {
              "id": "llm_1196",
              "question": "在大模型评测中，你如何进行特征选择和模型调优？"
            },
            {
              "id": "llm_1197",
              "question": "请谈谈你对 A/B 测试的理解，并说明它在大模型评测中的应用。 ............................ 17？"
            },
            {
              "id": "llm_1198",
              "question": "什么情况用 Bert 模型，什么情况用 LLaMA、ChatGLM 类大模型，咋选？"
            },
            {
              "id": "llm_1199",
              "question": "transformer 中求和与归一化中“求和”是什么意思？"
            },
            {
              "id": "llm_1200",
              "question": "注意力机制中计算注意力分数时为什么会除以根号 dk？"
            },
            {
              "id": "llm_1201",
              "question": "请简述什么是大模型，以及它与传统模型的主要？"
            },
            {
              "id": "llm_1202",
              "question": "区别是什么？"
            },
            {
              "id": "llm_1203",
              "question": "谈谈你对 Transformer 模型的理解，以及它在自？"
            },
            {
              "id": "llm_1204",
              "question": "请描述一下你如何对大模型进行优化，以提高其？"
            },
            {
              "id": "llm_1205",
              "question": "或开发大模型的经验？"
            },
            {
              "id": "llm_1206",
              "question": "有什么解决方案或建议？"
            },
            {
              "id": "llm_1207",
              "question": "为什么 Transformer 使用位置编码？"
            },
            {
              "id": "llm_1208",
              "question": "（Positional Encoding）？"
            },
            {
              "id": "llm_1209",
              "question": "请简述你了解的大模型的主要结构特点。？"
            },
            {
              "id": "llm_1210",
              "question": "模型中起到了什么作用？"
            },
            {
              "id": "llm_1211",
              "question": "各有什么优缺点？"
            },
            {
              "id": "llm_1212",
              "question": "如何处理大模型训练过程中的梯度消失或梯度？"
            },
            {
              "id": "llm_1213",
              "question": "爆炸问题？"
            },
            {
              "id": "llm_1214",
              "question": "请解释什么是注意力机制，并举例说明其应用？"
            },
            {
              "id": "llm_1215",
              "question": "优化模型性能？"
            },
            {
              "id": "llm_1216",
              "question": "请解释什么是位置编码，为什么在大模型中需？"
            },
            {
              "id": "llm_1217",
              "question": "要位置编码？"
            },
            {
              "id": "llm_1218",
              "question": "请简述 Transformer 中的位置编码是如何实现？"
            },
            {
              "id": "llm_1219",
              "question": "以用来处理序列中的位置信息？"
            },
            {
              "id": "llm_1220",
              "question": "请简述 Tokenizer 的作用及其在 NLP 模型中的？"
            },
            {
              "id": "llm_1221",
              "question": "请描述一种你熟悉的 Tokenizer 实现方法，并？"
            },
            {
              "id": "llm_1222",
              "question": "解释其原理。？"
            },
            {
              "id": "llm_1223",
              "question": "的一致性？"
            },
            {
              "id": "llm_1224",
              "question": "请解释什么是大模型微调，以及它在自然语言？"
            },
            {
              "id": "llm_1225",
              "question": "请简述大模型性能评估的主要步骤。？"
            },
            {
              "id": "llm_1226",
              "question": "请解释什么是过拟合和欠拟合，并说明如何在？"
            },
            {
              "id": "llm_1227",
              "question": "请谈谈你对 A/B 测试的理解，并说明它在大模？"
            },
            {
              "id": "llm_1228",
              "question": "什么情况用 Bert 模型，什么情况用 LLaMA、？"
            },
            {
              "id": "llm_1229",
              "question": "ChatGLM 类大模型，咋选？"
            },
            {
              "id": "llm_1230",
              "question": "根号 dk？"
            },
            {
              "id": "llm_1231",
              "question": "一、什么是 大模型（LLMs）agent？"
            },
            {
              "id": "llm_1232",
              "question": "二、大模型（LLMs）agent 有哪些部分组成？"
            },
            {
              "id": "llm_1233",
              "question": "1 介绍一下 规划（planning）？"
            },
            {
              "id": "llm_1234",
              "question": "1.1.1 如何进行 拆解子目标和任务分解？"
            },
            {
              "id": "llm_1235",
              "question": "1.1.2 拆解子目标和任务分解 有哪些方法？"
            },
            {
              "id": "llm_1236",
              "question": "1.2.1 如何进行 模型自我反省？"
            },
            {
              "id": "llm_1237",
              "question": "1.2.2 模型自我反省 有哪些方法？"
            },
            {
              "id": "llm_1238",
              "question": "2 介绍一下 记忆（Memory）？"
            },
            {
              "id": "llm_1239",
              "question": "3 介绍一下 工具使用（tool use）？"
            },
            {
              "id": "llm_1240",
              "question": "三、大模型（LLMs）agent 主要 利用了 大模型 哪些能力？"
            },
            {
              "id": "llm_1241",
              "question": "四、结合 代码 讲解 大模型（LLMs）agent 思路？"
            },
            {
              "id": "llm_1242",
              "question": "五、如何给LLM注入领域知识？"
            },
            {
              "id": "llm_1243",
              "question": "六、常见LLM Agent框架或者应用 有哪些？"
            },
            {
              "id": "llm_1244",
              "question": "a. 给LLM一个简单的提示词“Steps for XYZ.\\n1.”，“What are the subgoals for achieving XYZ？"
            },
            {
              "id": "llm_1245",
              "question": "请按以下格式进行回答 A 、 B 、 C 、 D 。？"
            },
            {
              "id": "llm_1246",
              "question": "请针对 >>> 和 <<< 中间的用户问题，选择一个适合的工具去回答他的问题，工具的名称已经给出。？"
            },
            {
              "id": "llm_1247",
              "question": "请按以下格式进行回答:？"
            },
            {
              "id": "llm_1248",
              "question": "问题: 请问你们家的货可以送到四川吗，物流大概要多久？"
            },
            {
              "id": "llm_1249",
              "question": "pattern = re.compile(r\"^.？"
            },
            {
              "id": "llm_1250",
              "question": "{3}(？"
            },
            {
              "id": "llm_1251",
              "question": ":json)？"
            },
            {
              "id": "llm_1252",
              "question": "\\n(.*？"
            },
            {
              "id": "llm_1253",
              "question": ") {3}.？"
            },
            {
              "id": "llm_1254",
              "question": "LangChain 包含哪些 核心概念？"
            },
            {
              "id": "llm_1255",
              "question": "input_data = \"Hello, how are you？"
            },
            {
              "id": "llm_1256",
              "question": "template = PromptTemplate(\"What is the capital of {country}？"
            },
            {
              "id": "llm_1257",
              "question": "The generated prompt will be \"What is the capital of France？"
            },
            {
              "id": "llm_1258",
              "question": "input_text = \"Hello, how are you？"
            },
            {
              "id": "llm_1259",
              "question": "请注意，Chat Message History 的具体用法和实现细节可以参考 Langchain 的官方文档和示？"
            },
            {
              "id": "llm_1260",
              "question": "\"input\": \"how many letters in the word educa？"
            },
            {
              "id": "llm_1261",
              "question": "\"input\": \"单词 educa 中有多少个字母？"
            },
            {
              "id": "llm_1262",
              "question": "response = langchain.ask(\"What is the capital of France？"
            },
            {
              "id": "llm_1263",
              "question": "This code will send the question \"What is the capital of France？"
            },
            {
              "id": "llm_1264",
              "question": "通过在提示中使用 {？"
            },
            {
              "id": "llm_1265",
              "question": "请注意，您可以根据需要自定义LLM的参数，例如温度（temperature）、最大令牌数？"
            },
            {
              "id": "llm_1266",
              "question": "LangChain 如何修改 提示模板？"
            },
            {
              "id": "llm_1267",
              "question": "template.add_message(\"human\", \"Hello, how are you doing？"
            },
            {
              "id": "llm_1268",
              "question": "template.add_message(\"human\", \"What is your name？"
            },
            {
              "id": "llm_1269",
              "question": "template.set_message_content(3, \"What is your name？"
            },
            {
              "id": "llm_1270",
              "question": "请注意，您可以根据需要添加、删除和修改聊天消息提示。 ChatPromptTemplate 类提供了？"
            },
            {
              "id": "llm_1271",
              "question": "请注意，您可以根据需要添加、删除和修改组件。 Chain 类提供了多种方法来操作链。更多？"
            },
            {
              "id": "llm_1272",
              "question": "LangChain 如何Embedding & vector store？"
            },
            {
              "id": "llm_1273",
              "question": "请注意，您可以根据需要添加、删除和修改嵌入向量。 Embedding 类和 VectorStore 类提？"
            },
            {
              "id": "llm_1274",
              "question": "WordPiece 与 BPE 异同点是什么？"
            },
            {
              "id": "llm_1275",
              "question": "简单介绍一下 SentencePiece 思路？"
            },
            {
              "id": "llm_1276",
              "question": "1 举例介绍一下不同大模型 LLMs 的分词方式？"
            },
            {
              "id": "llm_1277",
              "question": "2 介绍一下不同大模型 LLMs 的分词方式的区别？"
            },
            {
              "id": "llm_1278",
              "question": "2 如果有N张显存足够大的显卡，怎么加速训练？"
            },
            {
              "id": "llm_1279",
              "question": "3 如果显卡的显存不够装下一个完整的模型呢？"
            },
            {
              "id": "llm_1280",
              "question": "有其他方式？"
            },
            {
              "id": "llm_1281",
              "question": "5 3种并行方式可以叠加吗？"
            },
            {
              "id": "llm_1282",
              "question": "6 Colossal-AI 有1D/2D/2.5D/3D，是什么情况？"
            },
            {
              "id": "llm_1283",
              "question": "7 除了3D并行有没有其他方式大规模训练？"
            },
            {
              "id": "llm_1284",
              "question": "8 有了ZeRO系列，为什么还需要3D并行？"
            },
            {
              "id": "llm_1285",
              "question": "9 平民适不适合玩3D并行？"
            },
            {
              "id": "llm_1286",
              "question": "而且，节点间特殊的网络通常有400Gb/s？"
            },
            {
              "id": "llm_1287",
              "question": "10 平民适不适合直接上多机多卡的ZeRO3（万兆网）？"
            },
            {
              "id": "llm_1288",
              "question": "2 如果想构这样一个大规模并行训练系统，训练框架如何选？"
            },
            {
              "id": "llm_1289",
              "question": "1 目前 主流的开源模型体系 有哪些？"
            },
            {
              "id": "llm_1290",
              "question": "Decoder 区别是什么？"
            },
            {
              "id": "llm_1291",
              "question": "3 大模型LLM的 训练目标 是什么？"
            },
            {
              "id": "llm_1292",
              "question": "4 涌现能力是啥原因？"
            },
            {
              "id": "llm_1293",
              "question": "5 为何现在的大模型大部分是Decoder only结构？"
            },
            {
              "id": "llm_1294",
              "question": "6 简单 介绍一下 大模型【LLMs】？"
            },
            {
              "id": "llm_1295",
              "question": "8 大模型【LLMs】具有什么优点？"
            },
            {
              "id": "llm_1296",
              "question": "9 大模型【LLMs】具有什么缺点？"
            },
            {
              "id": "llm_1297",
              "question": "进行 增量预训练 需要做哪些准备工作？"
            },
            {
              "id": "llm_1298",
              "question": "增量预训练 所用 训练框架？"
            },
            {
              "id": "llm_1299",
              "question": "增量预训练 一般需要多大数据量？"
            },
            {
              "id": "llm_1300",
              "question": "增量预训练 过程中，loss 上升正常么？"
            },
            {
              "id": "llm_1301",
              "question": "增量预训练 过程中，lr 如何设置？"
            },
            {
              "id": "llm_1302",
              "question": "增量预训练 过程中，warmup_ratio 如何设置？"
            },
            {
              "id": "llm_1303",
              "question": "warmup 的步数对大模型继续预训练 是否有影响？"
            },
            {
              "id": "llm_1304",
              "question": "学习率大小对大模型继续预训练 后 上下游任务影响？"
            },
            {
              "id": "llm_1305",
              "question": "在初始预训练中使用 Rewarmup 对大模型继续预训练性能影响？"
            },
            {
              "id": "llm_1306",
              "question": "进行增量预训练 需要做哪些准备工作？"
            },
            {
              "id": "llm_1307",
              "question": "增量预训练 所用训练框架？"
            },
            {
              "id": "llm_1308",
              "question": "增量预训练一般需要多大数据量？"
            },
            {
              "id": "llm_1309",
              "question": "增量预训练过程中，loss 上升正常么？"
            },
            {
              "id": "llm_1310",
              "question": "增量预训练过程中，lr 如何设置？"
            },
            {
              "id": "llm_1311",
              "question": "当你数据集比较小（例如 100B 以下？"
            },
            {
              "id": "llm_1312",
              "question": "增量预训练过程中，warmup_ratio 如何设置？"
            },
            {
              "id": "llm_1313",
              "question": "warmup 的步数对大模型继续预训练是否有影响？"
            },
            {
              "id": "llm_1314",
              "question": "学习率大小对大模型继续预训练后上下游任务影响？"
            },
            {
              "id": "llm_1315",
              "question": "解释一下这里为什么这么关注训练前期，是因为在真实训练中，我们可能不一定会增强图中？"
            },
            {
              "id": "llm_1316",
              "question": "一、什么是大模型幻觉？"
            },
            {
              "id": "llm_1317",
              "question": "二、为什么LLM会产生幻觉？"
            },
            {
              "id": "llm_1318",
              "question": "四、幻觉一定是有害的吗？"
            },
            {
              "id": "llm_1319",
              "question": "八、LLMs什么时候最容易产生幻觉？"
            },
            {
              "id": "llm_1320",
              "question": "”或“氦的原子序数为什么是1？"
            },
            {
              "id": "llm_1321",
              "question": "领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力？"
            },
            {
              "id": "llm_1322",
              "question": "进行SFT操作的时候，基座模型选用Chat还是Base？"
            },
            {
              "id": "llm_1323",
              "question": "领域模型微调 指令&数据输入格式 要求？"
            },
            {
              "id": "llm_1324",
              "question": "想让模型学习某个领域或行业的知识，是应该预训练还是应该微调？"
            },
            {
              "id": "llm_1325",
              "question": "大模型LLM进行SFT操作的时候在学习什么？"
            },
            {
              "id": "llm_1326",
              "question": "大模型LLM进行SFT 如何对样本进行优化？"
            },
            {
              "id": "llm_1327",
              "question": "微调大模型时，如果 batch size 设置太小 会出现什么问题？"
            },
            {
              "id": "llm_1328",
              "question": "微调大模型时，如果 batch size 设置太大 会出现什么问题？"
            },
            {
              "id": "llm_1329",
              "question": "微调大模型时, batch size 如何设置问题？"
            },
            {
              "id": "llm_1330",
              "question": "微调大模型时, 优化器如何？"
            },
            {
              "id": "llm_1331",
              "question": "哪些因素会影响内存使用？"
            },
            {
              "id": "llm_1332",
              "question": "进行领域大模型预训练应用哪些数据集比较好？"
            },
            {
              "id": "llm_1333",
              "question": "用于大模型微调的数据集如何构建？"
            },
            {
              "id": "llm_1334",
              "question": "1 大模型训练loss突刺是什么？"
            },
            {
              "id": "llm_1335",
              "question": "2 为什么大模型训练会出现loss突刺？"
            },
            {
              "id": "llm_1336",
              "question": "3 大模型训练loss突刺 如何解决？"
            },
            {
              "id": "llm_1337",
              "question": "模型遗忘通用能力？"
            },
            {
              "id": "llm_1338",
              "question": "那么这个比例多少比较合适呢？"
            },
            {
              "id": "llm_1339",
              "question": "程中就学习到更多的知识？"
            },
            {
              "id": "llm_1340",
              "question": "是应该微调？"
            },
            {
              "id": "llm_1341",
              "question": "当前优化模型最主要技术手段有哪些？"
            },
            {
              "id": "llm_1342",
              "question": "推理加速框架有哪一些？"
            },
            {
              "id": "llm_1343",
              "question": "1 介绍一下 Text generation inference？"
            },
            {
              "id": "llm_1344",
              "question": "ube.com/watch？"
            },
            {
              "id": "llm_1345",
              "question": "5 Text generation inference 的 使用docker运行web server？"
            },
            {
              "id": "llm_1346",
              "question": "大模型在gpu和cpu上推理速度如何？"
            },
            {
              "id": "llm_1347",
              "question": "推理速度上，int8和fp16比起来怎么样？"
            },
            {
              "id": "llm_1348",
              "question": "大模型大概有多大，模型文件有多大？"
            },
            {
              "id": "llm_1349",
              "question": "能否用 4 * v100 32G 训练 vicuna 65b？"
            },
            {
              "id": "llm_1350",
              "question": "如果就是想要试试 65b 模型，但是显存不多怎么办？"
            },
            {
              "id": "llm_1351",
              "question": "nB 模型推理需要多少显存？"
            },
            {
              "id": "llm_1352",
              "question": "nB 模型训练需要多少显存？"
            },
            {
              "id": "llm_1353",
              "question": "解释：优化器部分必须用 fp32（ 乎 fp16 会导致训练不稳定）？"
            },
            {
              "id": "llm_1354",
              "question": "注：以上算数不够直观，举个例子？"
            },
            {
              "id": "llm_1355",
              "question": "如何估算模型所需的 RAM？"
            },
            {
              "id": "llm_1356",
              "question": "如何评估你的显卡利用率？"
            },
            {
              "id": "llm_1357",
              "question": "1 如何查看多机训练时的网速？"
            },
            {
              "id": "llm_1358",
              "question": "2 如何查看服务器上的多卡之间的 NVLINK topo？"
            },
            {
              "id": "llm_1359",
              "question": "3 如何查看服务器上显卡的具体型号？"
            },
            {
              "id": "llm_1360",
              "question": "4 如何查看训练时的 flops？"
            },
            {
              "id": "llm_1361",
              "question": "5 如何查看对 deepspeed 的环境配置是否正确？"
            },
            {
              "id": "llm_1362",
              "question": "6 tf32 格式有多长？"
            },
            {
              "id": "llm_1363",
              "question": "7 哪里看各类显卡算力比较？"
            },
            {
              "id": "llm_1364",
              "question": "8 （torch profiler）如何查看自己的训练中通信开销？"
            },
            {
              "id": "llm_1365",
              "question": "一、知识蒸馏和无监督样本训练？"
            },
            {
              "id": "llm_1366",
              "question": "二、对知识蒸馏知道多少，有哪些改进用到了？"
            },
            {
              "id": "llm_1367",
              "question": "三、谈一下对模型量化的了解？"
            },
            {
              "id": "llm_1368",
              "question": "四、模型压缩和加速的方法有哪些？"
            },
            {
              "id": "llm_1369",
              "question": "五、你了解的知识蒸馏模型有哪些？"
            },
            {
              "id": "llm_1370",
              "question": "找数据集哪里找？"
            },
            {
              "id": "llm_1371",
              "question": "有哪些大模型的训练集？"
            },
            {
              "id": "llm_1372",
              "question": "如何选取和构建大模型微调数据？"
            },
            {
              "id": "llm_1373",
              "question": "动机：在 微调大模型时，首先需要解决的问题是“选取和构建大模型微调数据”，那如何选择呢？"
            },
            {
              "id": "llm_1374",
              "question": "问题一：什么样的 数据 才是 最优的 大模型微调数据？"
            },
            {
              "id": "llm_1375",
              "question": "问题二：如何构建 大模型微调数据？"
            },
            {
              "id": "llm_1376",
              "question": "外一个是数据的不确定性。这样讲是比较抽象的概念，那我们在大模型实践中如何体现呢？"
            },
            {
              "id": "llm_1377",
              "question": "样的数据呢？"
            },
            {
              "id": "llm_1378",
              "question": "1 大模型怎么评测？"
            },
            {
              "id": "llm_1379",
              "question": "2 大模型的honest原则是如何实现的？"
            },
            {
              "id": "llm_1380",
              "question": "的知识是训练过的已知的知识，怎么训练这种能力？"
            },
            {
              "id": "llm_1381",
              "question": "3 如何衡量大模型水平？"
            },
            {
              "id": "llm_1382",
              "question": "呼吸。狗是一种动物。那么狗会呼吸吗？"
            },
            {
              "id": "llm_1383",
              "question": "4 大模型评估方法 有哪些？"
            },
            {
              "id": "llm_1384",
              "question": "5 大模型评估工具 有哪些？"
            },
            {
              "id": "llm_1385",
              "question": "一、什么是生成式大模型？"
            },
            {
              "id": "llm_1386",
              "question": "二、大模型是怎么让生成的文本丰富而不单调的呢？"
            },
            {
              "id": "llm_1387",
              "question": "1 什么是 LLMs 复读机问题？"
            },
            {
              "id": "llm_1388",
              "question": "2 为什么会出现 LLMs 复读机问题？"
            },
            {
              "id": "llm_1389",
              "question": "3 如何缓解 LLMs 复读机问题？"
            },
            {
              "id": "llm_1390",
              "question": "1 llama 输入句子长度理论上可以无限长吗？"
            },
            {
              "id": "llm_1391",
              "question": "五、什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型，咋选？"
            },
            {
              "id": "llm_1392",
              "question": "六、各个专业领域是否需要各自的大模型来服务？"
            },
            {
              "id": "llm_1393",
              "question": "七、如何让大模型处理更长的文本？"
            },
            {
              "id": "llm_1394",
              "question": "另一点，就是为什么会一直是一个词L的反复重复？"
            },
            {
              "id": "llm_1395",
              "question": "采样时如何选择温度？"
            },
            {
              "id": "llm_1396",
              "question": "ChatGLM类大模型，咋选？"
            },
            {
              "id": "llm_1397",
              "question": "一到同样的尺度下，只要position的相对位置保持不变就可以？"
            },
            {
              "id": "llm_1398",
              "question": "商业模型比如ChatGPT和Claude到底是怎么做的？"
            },
            {
              "id": "llm_1399",
              "question": "💡 目前 主流的开源模型体系 有哪些？"
            },
            {
              "id": "llm_1400",
              "question": "💡 prefix LM 和 causal LM 区别是什么？"
            },
            {
              "id": "llm_1401",
              "question": "💡 涌现能力是啥原因？"
            },
            {
              "id": "llm_1402",
              "question": "💡 大模型LLM的架构介绍？"
            },
            {
              "id": "llm_1403",
              "question": "💡 什么是 LLMs 复读机问题？"
            },
            {
              "id": "llm_1404",
              "question": "💡 为什么会出现 LLMs 复读机问题？"
            },
            {
              "id": "llm_1405",
              "question": "💡 如何缓解 LLMs 复读机问题？"
            },
            {
              "id": "llm_1406",
              "question": "💡 llama 输入句子长度理论上可以无限长吗？"
            },
            {
              "id": "llm_1407",
              "question": "💡 什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型，咋选？"
            },
            {
              "id": "llm_1408",
              "question": "💡 各个专业领域是否需要各自的大模型来服务？"
            },
            {
              "id": "llm_1409",
              "question": "💡 如何让大模型处理更长的文本？"
            },
            {
              "id": "llm_1410",
              "question": "💡 大模型怎么评测？"
            },
            {
              "id": "llm_1411",
              "question": "💡 大模型的honest原则是如何实现的？"
            },
            {
              "id": "llm_1412",
              "question": "💡 模型如何判断回答的知识是训练过的已知的知识，怎么训练这种能力？"
            },
            {
              "id": "llm_1413",
              "question": "💡 奖励模型需要和基础模型一致吗？"
            },
            {
              "id": "llm_1414",
              "question": "💡 RLHF 在实践过程中存在哪些不足？"
            },
            {
              "id": "llm_1415",
              "question": "💡 如何解决 人工产生的偏好数据集成本较高，很难量产问题？"
            },
            {
              "id": "llm_1416",
              "question": "💡 如何解决三个阶段的训练（SFT->RM->PPO）过程较长，更新迭代较慢问题？"
            },
            {
              "id": "llm_1417",
              "question": "💡 如何解决 PPO 的训练过程同时存在4个模型（2训练，2推理），对计算资源的要求较高 问题？"
            },
            {
              "id": "llm_1418",
              "question": "💡 如何给LLM注入领域知识？"
            },
            {
              "id": "llm_1419",
              "question": "💡 如果想要快速体验各种模型，该怎么办？"
            },
            {
              "id": "llm_1420",
              "question": "💡 建议的软件环境是什么？"
            },
            {
              "id": "llm_1421",
              "question": "💡 为什么大模型推理时显存涨的那么多还一直占着？"
            },
            {
              "id": "llm_1422",
              "question": "💡 大模型在gpu和cpu上推理速度如何？"
            },
            {
              "id": "llm_1423",
              "question": "💡 推理速度上，int8和fp16比起来怎么样？"
            },
            {
              "id": "llm_1424",
              "question": "💡 大模型有推理能力吗？"
            },
            {
              "id": "llm_1425",
              "question": "💡 大模型生成时的参数怎么设置？"
            },
            {
              "id": "llm_1426",
              "question": "💡 有哪些省内存的大语言模型训练/微调/推理方法？"
            },
            {
              "id": "llm_1427",
              "question": "解释模型决策过程：为了满足合规性要求，可以对模型的决策过程进行解释和解释。通过提供？"
            },
            {
              "id": "llm_1428",
              "question": "💡 如果想要在某个模型基础上做全参数微调，究竟需要多少显存？"
            },
            {
              "id": "llm_1429",
              "question": "💡 为什么SFT之后感觉LLM傻了？"
            },
            {
              "id": "llm_1430",
              "question": "💡 SFT 指令微调数据 如何构建？"
            },
            {
              "id": "llm_1431",
              "question": "💡 领域模型Continue PreTrain 数据选取？"
            },
            {
              "id": "llm_1432",
              "question": "💡 领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力？"
            },
            {
              "id": "llm_1433",
              "question": "💡 领域模型Continue PreTrain ，如何 让模型在预训练过程中就学习到更多的知识？"
            },
            {
              "id": "llm_1434",
              "question": "💡 进行SFT操作的时候，基座模型选用Chat还是Base？"
            },
            {
              "id": "llm_1435",
              "question": "💡 领域模型微调 指令&数据输入格式 要求？"
            },
            {
              "id": "llm_1436",
              "question": "💡 领域模型微调 领域评测集 构建？"
            },
            {
              "id": "llm_1437",
              "question": "💡 领域模型词表扩增是不是有必要的？"
            },
            {
              "id": "llm_1438",
              "question": "💡 如何训练自己的大模型？"
            },
            {
              "id": "llm_1439",
              "question": "💡 训练中文大模型有啥经验？"
            },
            {
              "id": "llm_1440",
              "question": "💡 指令微调的好处？"
            },
            {
              "id": "llm_1441",
              "question": "💡 预训练和微调哪个阶段注入知识的？"
            },
            {
              "id": "llm_1442",
              "question": "💡 想让模型学习某个领域或行业的知识，是应该预训练还是应该微调？"
            },
            {
              "id": "llm_1443",
              "question": "💡 多轮对话任务如何微调模型？"
            },
            {
              "id": "llm_1444",
              "question": "💡 微调后的模型出现能力劣化，灾难性遗忘是怎么回事？"
            },
            {
              "id": "llm_1445",
              "question": "💡 微调模型需要多大显存？"
            },
            {
              "id": "llm_1446",
              "question": "💡 大模型LLM进行SFT操作的时候在学习什么？"
            },
            {
              "id": "llm_1447",
              "question": "💡 大模型LLM进行SFT 如何对样本进行优化？"
            },
            {
              "id": "llm_1448",
              "question": "💡 SFT（有监督微调）的数据集格式？"
            },
            {
              "id": "llm_1449",
              "question": "💡 RM（奖励模型）的数据格式？"
            },
            {
              "id": "llm_1450",
              "question": "💡 PPO（强化学习）的数据格式？"
            },
            {
              "id": "llm_1451",
              "question": "💡 找数据集哪里找？"
            },
            {
              "id": "llm_1452",
              "question": "💡 微调需要多少条数据？"
            },
            {
              "id": "llm_1453",
              "question": "💡 有哪些大模型的训练集？"
            },
            {
              "id": "llm_1454",
              "question": "💡 进行领域大模型预训练应用哪些数据集比较好？"
            },
            {
              "id": "llm_1455",
              "question": "1 Layer Norm 的计算公式写一下？"
            },
            {
              "id": "llm_1456",
              "question": "1 RMS Norm 的计算公式写一下？"
            },
            {
              "id": "llm_1457",
              "question": "2 RMS Norm 相比于 Layer Norm 有什么特点？"
            },
            {
              "id": "llm_1458",
              "question": "1 Deep Norm 思路？"
            },
            {
              "id": "llm_1459",
              "question": "2 写一下 Deep Norm 代码实现？"
            },
            {
              "id": "llm_1460",
              "question": "LN 在 LLMs 中的不同位置有什么区别么？"
            },
            {
              "id": "llm_1461",
              "question": "如有能介绍一下区别么？"
            },
            {
              "id": "llm_1462",
              "question": "LLMs 各模型分别用了哪种 Layer normalization？"
            },
            {
              "id": "llm_1463",
              "question": "介绍一下 FFN 块 计算公式？"
            },
            {
              "id": "llm_1464",
              "question": "介绍一下 GeLU 计算公式？"
            },
            {
              "id": "llm_1465",
              "question": "介绍一下 Swish 计算公式？"
            },
            {
              "id": "llm_1466",
              "question": "介绍一下使用 GLU 线性门控单元的 FFN 块 计算公式？"
            },
            {
              "id": "llm_1467",
              "question": "介绍一下使用 GeLU 的 GLU 块 计算公式？"
            },
            {
              "id": "llm_1468",
              "question": "介绍一下 使用 Swish 的 GLU 块 计算公式？"
            },
            {
              "id": "llm_1469",
              "question": "各 LLMs 都使用哪种激活函数？"
            },
            {
              "id": "llm_1470",
              "question": "2 Attention 优化方向？"
            },
            {
              "id": "llm_1471",
              "question": "2 介绍一下 Multi-Query Attention？"
            },
            {
              "id": "llm_1472",
              "question": "3 对比一下 Multi-head Attention 和 Multi-Query Attention？"
            },
            {
              "id": "llm_1473",
              "question": "5 有哪些模型是使用 Multi-Query Attention？"
            },
            {
              "id": "llm_1474",
              "question": "2 有哪些大模型使用 Grouped-query Attention？"
            },
            {
              "id": "llm_1475",
              "question": "1 介绍一下 KL 散度？"
            },
            {
              "id": "llm_1476",
              "question": "2 交叉熵损失函数写一下，物理意义是什么？"
            },
            {
              "id": "llm_1477",
              "question": "3 KL 散度与交叉熵的区别？"
            },
            {
              "id": "llm_1478",
              "question": "4 多任务学习各 loss 差异过大怎样处理？"
            },
            {
              "id": "llm_1479",
              "question": "5 分类问题为什么用交叉熵损失函数不用均方误差（MSE）？"
            },
            {
              "id": "llm_1480",
              "question": "6 什么是信息增益？"
            },
            {
              "id": "llm_1481",
              "question": "7 多分类的分类损失函数(Softmax)？"
            },
            {
              "id": "llm_1482",
              "question": "8 softmax 和交叉熵损失怎么计算，二值交叉熵呢？"
            },
            {
              "id": "llm_1483",
              "question": "9 如果 softmax 的 e 次方超过 float 的值了怎么办？"
            },
            {
              "id": "llm_1484",
              "question": "2 了解对比学习嘛？"
            },
            {
              "id": "llm_1485",
              "question": "3 对比学习负样本是否重要？"
            },
            {
              "id": "llm_1486",
              "question": "1 分布式训练框架选择？"
            },
            {
              "id": "llm_1487",
              "question": "2 LLMs 训练时有哪些有用的建议？"
            },
            {
              "id": "llm_1488",
              "question": "3 模型大小如何选择？"
            },
            {
              "id": "llm_1489",
              "question": "4 加速卡如何选择？"
            },
            {
              "id": "llm_1490",
              "question": "1 为什么大模型需要外挂 (向量) 知识库？"
            },
            {
              "id": "llm_1491",
              "question": "如何将外部知识注入大模型，最直接的方法：利用外部知识对大模型进行微调。？"
            },
            {
              "id": "llm_1492",
              "question": "既然大模型微调不是将外部知识注入大模型的最优方案，那是否有其它可行方案？"
            },
            {
              "id": "llm_1493",
              "question": "2. RAG 思路是怎么样？"
            },
            {
              "id": "llm_1494",
              "question": "3. RAG 核心技术是什么？"
            },
            {
              "id": "llm_1495",
              "question": "RAG prompt 模板如何构建？"
            },
            {
              "id": "llm_1496",
              "question": "问题 1：如何让 LLM 简要、准确回答细粒度知识？"
            },
            {
              "id": "llm_1497",
              "question": "用户：2023 年我国上半年的国内生产总值是多少？"
            },
            {
              "id": "llm_1498",
              "question": "问题 2：如何让 LLM 回答出全面的粗粒度（跨段落）知识？"
            },
            {
              "id": "llm_1499",
              "question": "用户：根据文档内容，征信中心有几点声明？"
            },
            {
              "id": "llm_1500",
              "question": "如何构建关键信息？"
            },
            {
              "id": "llm_1501",
              "question": "句子、语义段、之间召回不会有包含关系吗，是否会造成冗余？"
            },
            {
              "id": "llm_1502",
              "question": "1 为什么需要对 RAG 进行评测？"
            },
            {
              "id": "llm_1503",
              "question": "2 RAG 有哪些评估方法？"
            },
            {
              "id": "llm_1504",
              "question": "3 RAG 有哪些关键指标和能力？"
            },
            {
              "id": "llm_1505",
              "question": "4 RAG 有哪些评估框架？"
            },
            {
              "id": "llm_1506",
              "question": "2 RAG 各模块有哪些优化策略？"
            },
            {
              "id": "llm_1507",
              "question": "3 RAG 架构优化有哪些优化策略？"
            },
            {
              "id": "llm_1508",
              "question": "1 如何利用 知识图谱（KG）进行上下文增强？"
            },
            {
              "id": "llm_1509",
              "question": "2 Self-RAG：如何让大模型对召回结果进行筛选？"
            },
            {
              "id": "llm_1510",
              "question": "比较常见）？"
            },
            {
              "id": "llm_1511",
              "question": "Self-RAG 的训练过程？"
            },
            {
              "id": "llm_1512",
              "question": "Self-RAG 的推理过程？"
            },
            {
              "id": "llm_1513",
              "question": "如何让 RAG 支持多模态数据格式？"
            },
            {
              "id": "llm_1514",
              "question": "3.1 如何让 RAG 支持半结构化 RAG（文本+表格）？"
            },
            {
              "id": "llm_1515",
              "question": "3.2 如何让 RAG 支持多模态 RAG（文本+表格+图片）？"
            },
            {
              "id": "llm_1516",
              "question": "3.3 如何让 RAG 支持私有化多模态 RAG（文本+表格+图片）？"
            },
            {
              "id": "llm_1517",
              "question": "9 Bert 在 RAG 中具体是起到了一个什么作用？"
            },
            {
              "id": "llm_1518",
              "question": "4 RAG 索引优化有哪些优化策略？"
            },
            {
              "id": "llm_1519",
              "question": "补充：尝试过不同大小的 chunk 和混合检索。效果都不太好，如何优化？"
            },
            {
              "id": "llm_1520",
              "question": "3 RAG 如何优化索引结构？"
            },
            {
              "id": "llm_1521",
              "question": "找到最佳块大小是要找到正确的平衡。如何高效地做到这一点？"
            },
            {
              "id": "llm_1522",
              "question": "4 如何通过混合检索提升 RAG 效果？"
            },
            {
              "id": "llm_1523",
              "question": "5 如何通过重新排名提升 RAG 效果？"
            },
            {
              "id": "llm_1524",
              "question": "5 RAG 索引数据优化有哪些优化策略？"
            },
            {
              "id": "llm_1525",
              "question": "1 RAG 如何提升索引数据的质量？"
            },
            {
              "id": "llm_1526",
              "question": "用正则表达式的老的 NLP 技术吗？"
            },
            {
              "id": "llm_1527",
              "question": "2 如何通过添加元数据 提升 RAG 效果？"
            },
            {
              "id": "llm_1528",
              "question": "3 如何通过输入查询与文档对齐提升 RAG 效果？"
            },
            {
              "id": "llm_1529",
              "question": "其实已经不错了，但还能做得更好吗？"
            },
            {
              "id": "llm_1530",
              "question": "--发动机的基本功能是什么？"
            },
            {
              "id": "llm_1531",
              "question": "--发动机如何将燃料转化为机械能？"
            },
            {
              "id": "llm_1532",
              "question": "--发动机运行涉及哪些关键部件，它们如何提高发动机的效率？"
            },
            {
              "id": "llm_1533",
              "question": "4 如何通过提示压缩提升 RAG 效果？"
            },
            {
              "id": "llm_1534",
              "question": "5 如何通过 查询重写和扩展 提升 RAG 效果？"
            }
          ]
        }
      ]
    },
    {
      "category": "Machine Learning",
      "subcategories": [
        {
          "subcategory": "Machine Learning-General",
          "questions": [
            {
              "id": "ml_0001",
              "question": "请问⼈⼯神经⽹络中为什么 ReLU 要好过于 tanh 和 Sigmoid function？"
            },
            {
              "id": "ml_0002",
              "question": "能写一下逻辑回归的损失函数吗？"
            },
            {
              "id": "ml_0003",
              "question": "逻辑回归和最大似然有什么关系？"
            },
            {
              "id": "ml_0004",
              "question": "逻辑回归用梯度下降优化，学习率对结果有什么影响？"
            },
            {
              "id": "ml_0005",
              "question": "逻辑回归中样本不均衡我们怎么处理？"
            },
            {
              "id": "ml_0006",
              "question": "lr 模型是线性模型还是非线性， 为什么？"
            },
            {
              "id": "ml_0007",
              "question": "能推导它的原理吗？"
            },
            {
              "id": "ml_0008",
              "question": "LR/SVM/softmax/Adaboost 损失函数之间的差别？"
            },
            {
              "id": "ml_0009",
              "question": "（天眼查）为什么逻辑回归做对数转换时选用以 e 为底？"
            },
            {
              "id": "ml_0010",
              "question": "（每日优鲜）lr 为什么不采用 mse 而是采用交叉熵损失？"
            },
            {
              "id": "ml_0011",
              "question": "SVM 的原理是什么？"
            },
            {
              "id": "ml_0012",
              "question": "SVM 为什么采用间隔最大化？"
            },
            {
              "id": "ml_0013",
              "question": "什么样的函数可以作为 SVM 的核函数？"
            },
            {
              "id": "ml_0014",
              "question": "SVM 对缺失数据敏感吗，为什么？"
            },
            {
              "id": "ml_0015",
              "question": "SVM 的损失函数是什么形式？"
            },
            {
              "id": "ml_0016",
              "question": "可以用梯度下降优化吗？"
            },
            {
              "id": "ml_0017",
              "question": "SVM 的高斯核为什么会把原始维度映射到无穷多维？"
            },
            {
              "id": "ml_0018",
              "question": "为什么要将求解 SVM 的原始问题转换为其对偶问题？"
            },
            {
              "id": "ml_0019",
              "question": "为什么 SVM 要引入核函数？"
            },
            {
              "id": "ml_0020",
              "question": "解释一下决策树的建模过程？"
            },
            {
              "id": "ml_0021",
              "question": "有几种不同的决策树，区别在哪？"
            },
            {
              "id": "ml_0022",
              "question": "决策树的缺失值和数值型特征分别是怎么处理的？"
            },
            {
              "id": "ml_0023",
              "question": "（滴滴）CART 树是如何做分类的，是如何做回归的，是如何处理多分类的？"
            },
            {
              "id": "ml_0024",
              "question": "决策树怎么控制过拟合？"
            },
            {
              "id": "ml_0025",
              "question": "决策树/随机森林的特征重要度是怎么获得的？"
            },
            {
              "id": "ml_0026",
              "question": "树形结构为什么不需要归⼀化？"
            },
            {
              "id": "ml_0027",
              "question": "树模型对于缺失值如何处理？"
            },
            {
              "id": "ml_0028",
              "question": "（百度） Xgboost 的原理能讲一下么？"
            },
            {
              "id": "ml_0029",
              "question": "一下原理么？"
            },
            {
              "id": "ml_0030",
              "question": "xgboost 的树生长时的精确分裂与近似分裂分别是怎么做的？"
            },
            {
              "id": "ml_0031",
              "question": "level-wise 的生长和 leaf-wise 的生长有什么不同，优缺点是什么？"
            },
            {
              "id": "ml_0032",
              "question": "能解释一下 LightGBM 里基于 histogram 的决策树算法吗？"
            },
            {
              "id": "ml_0033",
              "question": "什么 xgboost 的近似算法比 lightgbm 还是慢很多呢？"
            },
            {
              "id": "ml_0034",
              "question": "LightGBM 对类别型是怎么处理的？"
            },
            {
              "id": "ml_0035",
              "question": "lightgbm 哪些方面做了并行？"
            },
            {
              "id": "ml_0036",
              "question": "xgboost 和 LightGBM 有哪些控制过拟合的手段，通常需要调整的参数有哪些？"
            },
            {
              "id": "ml_0037",
              "question": "xgboost 对于缺失值，训练和预测的时候都是怎么处理的？"
            },
            {
              "id": "ml_0038",
              "question": "lgb,xgb 如何防止过拟合，有哪些参数？"
            },
            {
              "id": "ml_0039",
              "question": "lgb 二分类的损失函数是什么？"
            },
            {
              "id": "ml_0040",
              "question": "RF 与 boosting 之间差别？"
            },
            {
              "id": "ml_0041",
              "question": "GBDT 和 xgboost 之间差别？"
            },
            {
              "id": "ml_0042",
              "question": "xgboost 有哪些参数会影响模型的复杂度？"
            },
            {
              "id": "ml_0043",
              "question": "影响是怎样的？"
            },
            {
              "id": "ml_0044",
              "question": "xgboost 的并⾏化体现在哪？"
            },
            {
              "id": "ml_0045",
              "question": "xgboost 的多分类如何做？"
            },
            {
              "id": "ml_0046",
              "question": "怎么做的？"
            },
            {
              "id": "ml_0047",
              "question": "写一下贝叶斯公式？"
            },
            {
              "id": "ml_0048",
              "question": "朴素贝叶斯是一个什么算法，能解释一下吗？"
            },
            {
              "id": "ml_0049",
              "question": "如果出现计数为 0 导致概率为 0，这种情况在朴素贝叶斯计算里是怎么解决的？"
            },
            {
              "id": "ml_0050",
              "question": "（网易游戏）PCA 算法原理，跟 svd 的区别。特征值越大代表什么？"
            },
            {
              "id": "ml_0051",
              "question": "代表什么。特征向量有什么性质，是单位向量吗？"
            },
            {
              "id": "ml_0052",
              "question": "特征工程有什么作用？"
            },
            {
              "id": "ml_0053",
              "question": "请简述有哪些你知道的特征工程和他们的操作？"
            },
            {
              "id": "ml_0054",
              "question": "有哪些特征选择的方法？"
            },
            {
              "id": "ml_0055",
              "question": "有哪些异常值检测的⽅法？"
            },
            {
              "id": "ml_0056",
              "question": "有哪些处理异常值的⽅法？"
            },
            {
              "id": "ml_0057",
              "question": "常用的防止过拟合的技术手段有哪些， l1-norm 和 l2-norm 的区别是什么？"
            },
            {
              "id": "ml_0058",
              "question": "预测函数与代价函数的关系，在矩阵分解中如何体现的？"
            },
            {
              "id": "ml_0059",
              "question": "梯度下降中局部最优解和全局最优解的关系？"
            },
            {
              "id": "ml_0060",
              "question": "如何观察过拟合？"
            },
            {
              "id": "ml_0061",
              "question": "有哪些模型评估⽅法？"
            },
            {
              "id": "ml_0062",
              "question": "有哪些评估准则？"
            },
            {
              "id": "ml_0063",
              "question": "AUC 计算 auc 为什么稳定？"
            },
            {
              "id": "ml_0064",
              "question": "解释 balabala）。当然现在也有⼀些对 ReLU 的改进，⽐如 PReLU，random ReLU？"
            },
            {
              "id": "ml_0065",
              "question": "lr 使用注意事项，相比别的分类模型为什么使用它？"
            },
            {
              "id": "ml_0066",
              "question": "（欢聚时代 YY）LR 和 DNN 联系和区别是什么？"
            },
            {
              "id": "ml_0067",
              "question": "其中 a 是属性，aj 是该属性的所有取值。如果选择在某一节点上用哪个特征呢？"
            },
            {
              "id": "ml_0068",
              "question": ". 分类属性选择完成，对训练样本分类，发现属性缺失怎么办？"
            },
            {
              "id": "ml_0069",
              "question": "比如该节点是根据 a 属性划分，但是待分类样本 a 属性缺失，怎么办呢？"
            },
            {
              "id": "ml_0070",
              "question": ". 训练完成，给测试集样本分类，有缺失值怎么办？"
            },
            {
              "id": "ml_0071",
              "question": "Adaboost、SVM、LR、Knn、KMeans 之类则需要归⼀化呢？"
            },
            {
              "id": "ml_0072",
              "question": "为什么 xgboost 的近似算法比 lightgbm 还是慢很多呢？"
            },
            {
              "id": "ml_0073",
              "question": "汇总不同 worker 的不同 feature 的直方图(原理？"
            },
            {
              "id": "ml_0074",
              "question": "法怎么做的？"
            },
            {
              "id": "ml_0075",
              "question": "向量代表什么。特征向量有什么性质，是单位向量吗？"
            },
            {
              "id": "ml_0076",
              "question": "话概述，就是“投影后类内方差最小，类间方差最大”，什幺意思呢？"
            },
            {
              "id": "ml_0077",
              "question": "常用的防止过拟合的技术手段有哪些，l1-norm 和 l2-norm 的区别是什么？"
            },
            {
              "id": "ml_0078",
              "question": "比较合适，因为它能自动选择特征。而如果所有特征中，大部分特征都能起作用，？"
            },
            {
              "id": "ml_0079",
              "question": "合 （ over-fitting） 中 影 响 最 大？"
            },
            {
              "id": "ml_0080",
              "question": "证得到的均方误差是多少？"
            },
            {
              "id": "ml_0081",
              "question": "MLE） ， 说 法 正 确 的 是 （ 多 选 ）？"
            },
            {
              "id": "ml_0082",
              "question": "为零），则下面哪个说法是正确的？"
            },
            {
              "id": "ml_0083",
              "question": "合度。此时，如果增加一个特征，模型不变，则下面说法正确的是？"
            },
            {
              "id": "ml_0084",
              "question": "说明准确程度，只能大概定量。？"
            },
            {
              "id": "ml_0085",
              "question": "下 列 关 于 线 性 回 归 分 析 中 的 残 差 （ Residuals） 说 法 正 确 的 是？"
            },
            {
              "id": "ml_0086",
              "question": "下 列 关 于 异 方 差 （ Heteroskedasticity） 说 法 正 确 的 是？"
            },
            {
              "id": "ml_0087",
              "question": "下 列 哪 一 项 能 反 映 出 X 和 Y 之 间 的 强 相 关 性？"
            },
            {
              "id": "ml_0088",
              "question": "下 列 哪 些 假 设 是 我 们 推 导 线 性 回 归 参 数 时 遵 循 的 （ 多 选 ）？"
            },
            {
              "id": "ml_0089",
              "question": "哪种图形比较适合？"
            },
            {
              "id": "ml_0090",
              "question": "一 般 来 说 ， 下 列 哪 种 方 法 常 用 来 预 测 连 续 独 立 变 量？"
            },
            {
              "id": "ml_0091",
              "question": "下 列 哪 一 种 偏 移 ， 是 我 们 在 最 小 二 乘 直 线 拟 合 的 情 况 下 使 用 的？"
            },
            {
              "id": "ml_0092",
              "question": "能很好地拟合数据）。那么，下列说法正确的是（多选）？"
            },
            {
              "id": "ml_0093",
              "question": "关于这两句话，下列说法正确的是？"
            },
            {
              "id": "ml_0094",
              "question": "那么，下列说法正确的是？"
            },
            {
              "id": "ml_0095",
              "question": "式，而不使用 β 的平方约束呢？"
            },
            {
              "id": "ml_0096",
              "question": "关 于 特 征 选 择 ， 下 列 对 Ridge 回 归 和 Lasso 回 归 说 法 正 确 的 是？"
            },
            {
              "id": "ml_0097",
              "question": "（蓝色曲线）。那么，我们可以得出哪些结论（多选）？"
            },
            {
              "id": "ml_0098",
              "question": "下 列 哪 些 指 标 可 以 用 来 评 估 线 性 回 归 模 型 （ 多 选 ）？"
            },
            {
              "id": "ml_0099",
              "question": "系数。下列关于正规方程说法正确的是？"
            },
            {
              "id": "ml_0100",
              "question": "如 果 XTX 矩 阵 不 可 逆 ， 是 奇 异 矩 阵 怎 么 办 呢？"
            },
            {
              "id": "ml_0101",
              "question": "+ β2X2 + ··· + βnXn， 则 下 列 说 法 正 确 的 是 （ 多 选 ）？"
            },
            {
              "id": "ml_0102",
              "question": "构 建 一 个 最 简 单 的 线 性 回 归 模 型 需 要 几 个 系 数 （ 只 有 一 个 特 征 ）？"
            },
            {
              "id": "ml_0103",
              "question": "关于 A 和 B 各自的残差之和，下列说法正确的是？"
            },
            {
              "id": "ml_0104",
              "question": "如 果 两 个 变 量 相 关 ， 那 么 它 们 一 定 是 线 性 关 系 吗？"
            },
            {
              "id": "ml_0105",
              "question": "两 个 变 量 相 关 ， 它 们 的 相 关 系 数 r 可 能 为 0。 这 句 话 是 否 正 确？"
            },
            {
              "id": "ml_0106",
              "question": "变。然后重新训练测试。则下列说法正确的是？"
            },
            {
              "id": "ml_0107",
              "question": "值 与 真 实 值 的 残 差 。 计 算 SSE 为 多 少？"
            },
            {
              "id": "ml_0108",
              "question": "假 设 变 量 Var1 和 Var2 是 正 相 关 的 ， 那 么 下 面 那 张 图 是 正 确 的？"
            },
            {
              "id": "ml_0109",
              "question": "那 么 它 可 以 被 看 成 是 异 常 值 （ Outlier） 吗？"
            },
            {
              "id": "ml_0110",
              "question": "1分类算法常用哪些？"
            },
            {
              "id": "ml_0111",
              "question": "2 回归算法一般用哪些？"
            },
            {
              "id": "ml_0112",
              "question": "3 回归方法中使用的评价指标是哪些？"
            },
            {
              "id": "ml_0113",
              "question": "4 分类中使用的损失函数是哪些？"
            },
            {
              "id": "ml_0114",
              "question": "为什么是交叉熵而不是mae？"
            },
            {
              "id": "ml_0115",
              "question": "5对于小数据集一般怎么处理呢？"
            },
            {
              "id": "ml_0116",
              "question": "小数据集中如何防止过拟合？"
            },
            {
              "id": "ml_0117",
              "question": "6常用什么激活函数？"
            },
            {
              "id": "ml_0118",
              "question": "[链接](7. https://blog.csdn.net/weixin_42057852/article/details/84644348？"
            },
            {
              "id": "ml_0119",
              "question": "7 sigmoid函数有使用过吗？"
            },
            {
              "id": "ml_0120",
              "question": "与relu激活函数有什么不同？"
            },
            {
              "id": "ml_0121",
              "question": "22SVM中什么时候用线性核什么时候用高斯核？"
            },
            {
              "id": "ml_0122",
              "question": "23什么是支持向量机,SVM与LR的区别？"
            },
            {
              "id": "ml_0123",
              "question": "25机器学习中的距离计算方法？"
            },
            {
              "id": "ml_0124",
              "question": "26朴素贝叶斯（naive Bayes）法的要求是？"
            },
            {
              "id": "ml_0125",
              "question": "27训练集中类别不均衡，哪个参数最不准确？"
            },
            {
              "id": "ml_0126",
              "question": "35生成模型和判别模型基本形式，有哪些？"
            },
            {
              "id": "ml_0127",
              "question": "38分类算法列一下有多少种？"
            },
            {
              "id": "ml_0128",
              "question": "41SVM和全部数据有关还是和局部数据有关？"
            },
            {
              "id": "ml_0129",
              "question": "45Loss Function有哪些，怎么用？"
            },
            {
              "id": "ml_0130",
              "question": "59stacking和blending的区别？"
            },
            {
              "id": "ml_0131",
              "question": "67HMM隐马尔可夫模型的参数估计方法是？"
            },
            {
              "id": "ml_0132",
              "question": "68Bootstrap方法是什么？"
            },
            {
              "id": "ml_0133",
              "question": "69如何防止过拟合？"
            },
            {
              "id": "ml_0134",
              "question": "EM算法是否一定收敛？"
            },
            {
              "id": "ml_0135",
              "question": "73正负样本不平衡的解决办法？"
            },
            {
              "id": "ml_0136",
              "question": "评价指标的参考价值？"
            },
            {
              "id": "ml_0137",
              "question": "88循环神经网络，为什么好？"
            },
            {
              "id": "ml_0138",
              "question": "90训练过程中,若一个模型不收敛,那么是否说明这个模型无效？"
            },
            {
              "id": "ml_0139",
              "question": "导致模型不收敛的原因有哪些？"
            },
            {
              "id": "ml_0140",
              "question": "91VGG使用3*3卷积核的优势是什么？"
            },
            {
              "id": "ml_0141",
              "question": "92Relu比Sigmoid的效果好在哪里？"
            },
            {
              "id": "ml_0142",
              "question": "93神经网络中权重共享的是？"
            },
            {
              "id": "ml_0143",
              "question": "94神经网络激活函数？"
            },
            {
              "id": "ml_0144",
              "question": "97在深度学习中，通常会finetuning（微调）已有的成熟模型，再基于新数据，修改最后几层神经网络权值，为什么？"
            },
            {
              "id": "ml_0145",
              "question": "98微调时候网络参数是否更新？"
            },
            {
              "id": "ml_0146",
              "question": "103用过哪些移动端深度学习框架？"
            },
            {
              "id": "ml_0147",
              "question": "107RNN容易梯度消失，怎么解决？"
            },
            {
              "id": "ml_0148",
              "question": "谈谈自己投稿的论文，论文投稿级别，论文内容，用到的方法，对比方法等？"
            },
            {
              "id": "ml_0149",
              "question": "121模型压缩效果评价指标有哪些？"
            },
            {
              "id": "ml_0150",
              "question": "123压缩和加速方法如何选择？"
            },
            {
              "id": "ml_0151",
              "question": "133对一千万个整数排序,整数范围在[-1000,1000]间,用什么排序最快？"
            },
            {
              "id": "ml_0152",
              "question": "141什么是python的生成器？"
            },
            {
              "id": "ml_0153",
              "question": "142迭代器？"
            },
            {
              "id": "ml_0154",
              "question": "145如何判断两个dict是否一样,list头上删除元素,字符串拼接？"
            },
            {
              "id": "ml_0155",
              "question": "146pytorch中cuda()作用,两个Tensor,一个加了cuda(),一个没加,相加后很怎样？"
            },
            {
              "id": "ml_0156",
              "question": "151在程序里面智能指针的名字是啥？"
            },
            {
              "id": "ml_0157",
              "question": "155函数后面接const是什么意思？"
            },
            {
              "id": "ml_0158",
              "question": "157c＋＋的一些库吗？"
            },
            {
              "id": "ml_0159",
              "question": "什么是全局解释器锁？"
            },
            {
              "id": "ml_0160",
              "question": "什么是同步锁？"
            },
            {
              "id": "ml_0161",
              "question": "为什么用同步锁？"
            },
            {
              "id": "ml_0162",
              "question": "怎么使用同步锁？"
            },
            {
              "id": "ml_0163",
              "question": "什么是死锁？"
            },
            {
              "id": "ml_0164",
              "question": "死锁产生的必要条件？"
            },
            {
              "id": "ml_0165",
              "question": "处理死锁的基本方法？"
            },
            {
              "id": "ml_0166",
              "question": "四、什么是递归锁？"
            },
            {
              "id": "ml_0167",
              "question": "五、什么是乐观锁？"
            },
            {
              "id": "ml_0168",
              "question": "六、什么是悲观锁？"
            },
            {
              "id": "ml_0169",
              "question": "七、python常用的加锁方式？"
            },
            {
              "id": "ml_0170",
              "question": "160ip报文经过一个路由器改变哪些字段？"
            },
            {
              "id": "ml_0171",
              "question": "4 什么是监督学习？"
            },
            {
              "id": "ml_0172",
              "question": "什么是非监督学习？"
            },
            {
              "id": "ml_0173",
              "question": "1 什么是过拟合？"
            },
            {
              "id": "ml_0174",
              "question": "产生过拟合原因？"
            },
            {
              "id": "ml_0175",
              "question": "2 如何避免过拟合问题？"
            },
            {
              "id": "ml_0176",
              "question": "3 什么是机器学习的欠拟合？"
            },
            {
              "id": "ml_0177",
              "question": "4 如何避免欠拟合问题？"
            },
            {
              "id": "ml_0178",
              "question": "5 什么是交叉验证？"
            },
            {
              "id": "ml_0179",
              "question": "交叉验证的作用是什么？"
            },
            {
              "id": "ml_0180",
              "question": "6 交叉验证主要有哪几种方法？"
            },
            {
              "id": "ml_0181",
              "question": "7 什么是 K 折交叉验证？"
            },
            {
              "id": "ml_0182",
              "question": "8 如何在 K 折交叉验证中选择 K？"
            },
            {
              "id": "ml_0183",
              "question": "1 什么是准确率,精准率,召回率和 F1 分数？"
            },
            {
              "id": "ml_0184",
              "question": "2 模型常用的评估指标有哪些？"
            },
            {
              "id": "ml_0185",
              "question": "3 多标签分类怎么解决？"
            },
            {
              "id": "ml_0186",
              "question": "1 什么是正则化？"
            },
            {
              "id": "ml_0187",
              "question": "如何理解正则化？"
            },
            {
              "id": "ml_0188",
              "question": "2 L0、L1、L2 正则化？"
            },
            {
              "id": "ml_0189",
              "question": "3 L1 和 L2 正则化有什么区别？"
            },
            {
              "id": "ml_0190",
              "question": "4 L1 在 0 处不可导是怎么处理的？"
            },
            {
              "id": "ml_0191",
              "question": "5 L1 正则化产生稀疏性的原因？"
            },
            {
              "id": "ml_0192",
              "question": "对稀疏矩阵的理解？"
            },
            {
              "id": "ml_0193",
              "question": "6 为何要常对数据做归一化？"
            },
            {
              "id": "ml_0194",
              "question": "9 需要归一化的算法有哪些？"
            },
            {
              "id": "ml_0195",
              "question": "这些模型需要归一化的主要原因？"
            },
            {
              "id": "ml_0196",
              "question": "10 树形结构的不需要归一化的原因？"
            },
            {
              "id": "ml_0197",
              "question": "4 为什么要处理类别特征？"
            },
            {
              "id": "ml_0198",
              "question": "怎么处理？"
            },
            {
              "id": "ml_0199",
              "question": "5 什么是组合特征？"
            },
            {
              "id": "ml_0200",
              "question": "6 怎么有效地找到组合特征？"
            },
            {
              "id": "ml_0201",
              "question": "7 如何处理高维组合特征？"
            },
            {
              "id": "ml_0202",
              "question": "8 如何解决数据不平衡问题？"
            },
            {
              "id": "ml_0203",
              "question": "9 数据中有噪声如何处理？"
            },
            {
              "id": "ml_0204",
              "question": "1 简述一下 KNN 算法的原理？"
            },
            {
              "id": "ml_0205",
              "question": "2 如何理解 kNN 中的 k 的取值？"
            },
            {
              "id": "ml_0206",
              "question": "3 在 kNN 的样本搜索中，如何进行高效的匹配查找？"
            },
            {
              "id": "ml_0207",
              "question": "4 KNN 算法有哪些优点和缺点？"
            },
            {
              "id": "ml_0208",
              "question": "5 不平衡的样本可以给 KNN 的预测结果造成哪些问题，有没有什么好的解决方式？"
            },
            {
              "id": "ml_0209",
              "question": "7 如何优化 Kmeans？"
            },
            {
              "id": "ml_0210",
              "question": "曼哈顿距离？"
            },
            {
              "id": "ml_0211",
              "question": "1 SVM 的原理是什么？"
            },
            {
              "id": "ml_0212",
              "question": "2 SVM 为什么采用间隔最大化？"
            },
            {
              "id": "ml_0213",
              "question": "3 为什么 SVM 要引入核函数？"
            },
            {
              "id": "ml_0214",
              "question": "4 为什么 SVM 对缺失数据敏感？"
            },
            {
              "id": "ml_0215",
              "question": "6 SVM 如何处理多分类问题？"
            },
            {
              "id": "ml_0216",
              "question": "7 带核的 SVM 为什么能分类非线性问题？"
            },
            {
              "id": "ml_0217",
              "question": "8 RBF 核一定是线性可分的吗？"
            },
            {
              "id": "ml_0218",
              "question": "9 常用核函数及核函数的条件？"
            },
            {
              "id": "ml_0219",
              "question": "10 为什么要将求解 SVM 的原始问题转换为对偶问题？"
            },
            {
              "id": "ml_0220",
              "question": "11 SVM 怎么输出预测概率？"
            },
            {
              "id": "ml_0221",
              "question": "12 如何处理数据偏斜？"
            },
            {
              "id": "ml_0222",
              "question": "1 你是怎么理解偏差和方差的平衡的？"
            },
            {
              "id": "ml_0223",
              "question": "偏差的范围内。百分之多少的数据不会受到影响？"
            },
            {
              "id": "ml_0224",
              "question": "4 模型受到低偏差和高方差问题时，应该使用哪种算法来解决问题呢？"
            },
            {
              "id": "ml_0225",
              "question": "5 怎么理解偏差方差的平衡的？"
            },
            {
              "id": "ml_0226",
              "question": "6 协方差和相关性有什么区别？"
            },
            {
              "id": "ml_0227",
              "question": "7 把分类变量当成连续型变量会更得到一个更好的预测模型吗？"
            },
            {
              "id": "ml_0228",
              "question": "8 机器学习中分类器指的是什么？"
            },
            {
              "id": "ml_0229",
              "question": "10 请简要说说一个完整机器学习项目的流程？"
            },
            {
              "id": "ml_0230",
              "question": "1.1 什么是回归？"
            },
            {
              "id": "ml_0231",
              "question": "哪些模型可用于解决回归问题？"
            },
            {
              "id": "ml_0232",
              "question": "1.2 线性回归的损失函数为什么是均方差？"
            },
            {
              "id": "ml_0233",
              "question": "1.3 什么是线性回归？"
            },
            {
              "id": "ml_0234",
              "question": "什么时候使用它？"
            },
            {
              "id": "ml_0235",
              "question": "1.4 什么是梯度下降？"
            },
            {
              "id": "ml_0236",
              "question": "SGD 的推导？"
            },
            {
              "id": "ml_0237",
              "question": "1.5 什么是最小二乘法（最小平方法）？"
            },
            {
              "id": "ml_0238",
              "question": "1.6 常见的损失函数有哪些？"
            },
            {
              "id": "ml_0239",
              "question": "1.7 有哪些评估回归模型的指标？"
            },
            {
              "id": "ml_0240",
              "question": "1.8 什么是正规方程？"
            },
            {
              "id": "ml_0241",
              "question": "1.9 梯度下降法找到的一定是下降最快的方向吗？"
            },
            {
              "id": "ml_0242",
              "question": "1.10 MBGD 需要注意什么？"
            },
            {
              "id": "ml_0243",
              "question": "如何选择 m？"
            },
            {
              "id": "ml_0244",
              "question": "什么是正态分布？"
            },
            {
              "id": "ml_0245",
              "question": "为什么要重视它？"
            },
            {
              "id": "ml_0246",
              "question": "如何检查变量是否遵循正态分布？"
            },
            {
              "id": "ml_0247",
              "question": "如何建立价格预测模型？"
            },
            {
              "id": "ml_0248",
              "question": "价格是否正态分布？"
            },
            {
              "id": "ml_0249",
              "question": "需要对价格进行预处理吗？"
            },
            {
              "id": "ml_0250",
              "question": "2.1 为什么 LR 要使用 sigmoid 函数？"
            },
            {
              "id": "ml_0251",
              "question": "2.2 为什么常常要做特征组合（特征交叉）？"
            },
            {
              "id": "ml_0252",
              "question": "2.3 为什么 LR 比线性回归要好？"
            },
            {
              "id": "ml_0253",
              "question": "2.4 LR 参数求解的优化方法？"
            },
            {
              "id": "ml_0254",
              "question": "2.5 工程上，怎么实现 LR 的并行化？"
            },
            {
              "id": "ml_0255",
              "question": "有哪些并行化的工具？"
            },
            {
              "id": "ml_0256",
              "question": "2.6 LR 如何解决低维不可分问题？"
            },
            {
              "id": "ml_0257",
              "question": "2.7 LR 与最大熵模型 MaxEnt 的关系？"
            },
            {
              "id": "ml_0258",
              "question": "2.8 为什么 LR 用交叉熵损失而不是平方损失（MSE）？"
            },
            {
              "id": "ml_0259",
              "question": "2.9 LR 能否解决非线性分类问题？"
            },
            {
              "id": "ml_0260",
              "question": "2.10 用什么来评估 LR 模型？"
            },
            {
              "id": "ml_0261",
              "question": "2.11 LR 如何解决多分类问题？"
            },
            {
              "id": "ml_0262",
              "question": "100 遍，会造成怎样的影响？"
            },
            {
              "id": "ml_0263",
              "question": "2.13 为什么在训练的过程当中将高度相关的特征去掉？"
            },
            {
              "id": "ml_0264",
              "question": "2.5.1 什么是 ROC 曲线？"
            },
            {
              "id": "ml_0265",
              "question": "如何判断 ROC 曲线的好坏？"
            },
            {
              "id": "ml_0266",
              "question": "2.5.2 什么是 AUC？"
            },
            {
              "id": "ml_0267",
              "question": "2.5.3 如何解释 AU ROC 分数？"
            },
            {
              "id": "ml_0268",
              "question": "1.1 什么是特征选择？"
            },
            {
              "id": "ml_0269",
              "question": "为什么需要它？"
            },
            {
              "id": "ml_0270",
              "question": "特征选择的目标？"
            },
            {
              "id": "ml_0271",
              "question": "1.2 有哪些特征选择技术？"
            },
            {
              "id": "ml_0272",
              "question": "2.1 既然信息增益可以计算，为什么 C4.5 还使用信息增益比？"
            },
            {
              "id": "ml_0273",
              "question": "3.1 基尼指数和信息熵都表示数据不确定性，为什么 CART 使用基尼指数？"
            },
            {
              "id": "ml_0274",
              "question": "3.2 基尼系数(Gini)存在的问题？"
            },
            {
              "id": "ml_0275",
              "question": "5.1 决策树的数据 split 原理或者流程？"
            },
            {
              "id": "ml_0276",
              "question": "5.2 构造决策树的步骤？"
            },
            {
              "id": "ml_0277",
              "question": "5.3 决策树算法中如何避免过拟合和欠拟合？"
            },
            {
              "id": "ml_0278",
              "question": "5.4 决策树怎么剪枝？"
            },
            {
              "id": "ml_0279",
              "question": "5.5 决策树的优缺点？"
            },
            {
              "id": "ml_0280",
              "question": "5.6 决策树和条件概率分布的关系？"
            },
            {
              "id": "ml_0281",
              "question": "立最优的决策树？"
            },
            {
              "id": "ml_0282",
              "question": "5.8 如果特征很多，决策树中最后没有用到的特征一定是无用吗？"
            },
            {
              "id": "ml_0283",
              "question": "5.9 决策树怎么做回归？"
            },
            {
              "id": "ml_0284",
              "question": "5.10 决策树算法的停止条件？"
            },
            {
              "id": "ml_0285",
              "question": "5.11 为什么决策树之前用 PCA 会好一点？"
            },
            {
              "id": "ml_0286",
              "question": "交叉熵代价函数是如何产生的？"
            },
            {
              "id": "ml_0287",
              "question": "6.1 为什么信息增益偏向取值较多的特征(缺点)？"
            },
            {
              "id": "ml_0288",
              "question": "比较 偏向取值较多的特征。？"
            },
            {
              "id": "ml_0289",
              "question": "7.1 如何使用信息增益比？"
            },
            {
              "id": "ml_0290",
              "question": "么不用曼哈顿距离？"
            },
            {
              "id": "ml_0291",
              "question": "1.1.1 GBDT 是训练过程如何选择特征？"
            },
            {
              "id": "ml_0292",
              "question": "1.1.2 GBDT 如何防止过拟合？"
            },
            {
              "id": "ml_0293",
              "question": "决定性的作用，如何改进这个问题？"
            },
            {
              "id": "ml_0294",
              "question": "1.1.3 梯度提升的如何调参？"
            },
            {
              "id": "ml_0295",
              "question": "1.1.4 GBDT 对标量特征要不要 one-hot 编码？"
            },
            {
              "id": "ml_0296",
              "question": "1.1.5 为什么 GBDT 用负梯度当做残差？"
            },
            {
              "id": "ml_0297",
              "question": "1.2.1 为什么 Adaboost 方式能够提高整体模型的学习精度？"
            },
            {
              "id": "ml_0298",
              "question": "1.2.2 使用 m 个基学习器和加权平均使用 m 个学习器之间有什么不同？"
            },
            {
              "id": "ml_0299",
              "question": "1.2.3 adaboost 的迭代次数(基学习器的个数)如何控制？"
            },
            {
              "id": "ml_0300",
              "question": "1.2.4 adaboost 算法中基学习器是否很重要，应该怎么选择基学习器？"
            },
            {
              "id": "ml_0301",
              "question": "1.3.1 XGBoost 使用泰勒二阶展开的原因？"
            },
            {
              "id": "ml_0302",
              "question": "1.3.2 XGBoost 可以并行训练的原因？"
            },
            {
              "id": "ml_0303",
              "question": "1.3.3 XGBoost 为什么快？"
            },
            {
              "id": "ml_0304",
              "question": "1.3.4 XGBoost 防止过拟合的方法？"
            },
            {
              "id": "ml_0305",
              "question": "1.3.5 XGBoost 如何处理缺失值？"
            },
            {
              "id": "ml_0306",
              "question": "1.3.6 为什么 XGBoost 相比某些模型对缺失值不敏感？"
            },
            {
              "id": "ml_0307",
              "question": "1.3.7 XGBoost 如何处理不平衡数据？"
            },
            {
              "id": "ml_0308",
              "question": "1.3.8 XGBoost 中叶子结点的权重如何计算出来？"
            },
            {
              "id": "ml_0309",
              "question": "1.3.9 XGBoost 中的一棵树的停止生长条件？"
            },
            {
              "id": "ml_0310",
              "question": "1.3.10 比较 LR 和 GBDT，说说什么情景下 GBDT 不如 LR？"
            },
            {
              "id": "ml_0311",
              "question": "为什么此时树模型就过拟合的更严重呢？"
            },
            {
              "id": "ml_0312",
              "question": "1.3.11 XGBoost 在什么地方做的剪枝？"
            },
            {
              "id": "ml_0313",
              "question": "如何进行剪枝？"
            },
            {
              "id": "ml_0314",
              "question": "1.3.12 XGBoost 如何选择最佳分裂点？"
            },
            {
              "id": "ml_0315",
              "question": "1.3.13 XGBoost 的 Scalable 性如何体现？"
            },
            {
              "id": "ml_0316",
              "question": "1.3.14 XGBooost 参数调优的一般步骤？"
            },
            {
              "id": "ml_0317",
              "question": "1.3.15 XGBoost 模型如果过拟合了怎么解决？"
            },
            {
              "id": "ml_0318",
              "question": "1.3.16 XGBoost 如何寻找最优特征？"
            },
            {
              "id": "ml_0319",
              "question": "是有放回还是无放回？"
            },
            {
              "id": "ml_0320",
              "question": "1.3.17 XGBoost 如何分布式？"
            },
            {
              "id": "ml_0321",
              "question": "特征分布式和数据分布式？"
            },
            {
              "id": "ml_0322",
              "question": "各有什么问题？"
            },
            {
              "id": "ml_0323",
              "question": "1.3.18 为什么 XGBoost 的近似算法比 lightgbm 慢很多呢？"
            },
            {
              "id": "ml_0324",
              "question": "2.1.1 随机森林的随机性指的是？"
            },
            {
              "id": "ml_0325",
              "question": "2.1.2 为什么随机抽样？"
            },
            {
              "id": "ml_0326",
              "question": "2.1.3 为什么要有放回的抽样？"
            },
            {
              "id": "ml_0327",
              "question": "2.1.4 为什么不用全样本训练？"
            },
            {
              "id": "ml_0328",
              "question": "2.1.5 为什么要随机特征？"
            },
            {
              "id": "ml_0329",
              "question": "2.1.6 需要剪枝吗？"
            },
            {
              "id": "ml_0330",
              "question": "2.1.7 随机森林如何处理缺失值？"
            },
            {
              "id": "ml_0331",
              "question": "2.1.8 随机森林如何评估特征重要性？"
            },
            {
              "id": "ml_0332",
              "question": "2.1.9 RF 与决策树的区别？"
            },
            {
              "id": "ml_0333",
              "question": "2.1.10 RF 为什么比 bagging 效率高？"
            },
            {
              "id": "ml_0334",
              "question": "2.1.11 RF 为什么能够更鲁棒？"
            },
            {
              "id": "ml_0335",
              "question": "2.1.12 RF 分类和回归问题如何预测 y 值？"
            },
            {
              "id": "ml_0336",
              "question": "2.1.13 为什么 RF 的树比 GBDT 的要深一点？"
            },
            {
              "id": "ml_0337",
              "question": "1.1.1 K 值的如何选取？"
            },
            {
              "id": "ml_0338",
              "question": "1.1.2 K-means 算法中初始点的选择对最终结果的影响？"
            },
            {
              "id": "ml_0339",
              "question": "1.1.3 K-means 不适用哪些数据？"
            },
            {
              "id": "ml_0340",
              "question": "1.1.4 K-means 中常用的距离度量？"
            },
            {
              "id": "ml_0341",
              "question": "1.1.5 为什么在计算 K-means 之前要将数据点在各维度上归一化？"
            },
            {
              "id": "ml_0342",
              "question": "1.1.6 聚类和分类区别？"
            },
            {
              "id": "ml_0343",
              "question": "2.1.1 PCA 其优化目标是什么？"
            },
            {
              "id": "ml_0344",
              "question": "2.1.2 PCA 白化是什么？"
            },
            {
              "id": "ml_0345",
              "question": "2.2.1 为什么要用 SVD 进行降维？"
            },
            {
              "id": "ml_0346",
              "question": "2.5 降维的作用是什么？"
            },
            {
              "id": "ml_0347",
              "question": "2.6 矩阵的特征值和特征向量的物理意义是什么？"
            },
            {
              "id": "ml_0348",
              "question": "维度灾难是什么？"
            },
            {
              "id": "ml_0349",
              "question": "为什么要关心它？"
            },
            {
              "id": "ml_0350",
              "question": "1.2 为什么朴素贝叶斯如此“朴素”？"
            },
            {
              "id": "ml_0351",
              "question": "1.3 朴素贝叶斯的优缺点？"
            },
            {
              "id": "ml_0352",
              "question": "1.4 为什么引入条件独立性假设？"
            },
            {
              "id": "ml_0353",
              "question": "1.5 在估计条件概率 P(X|Y)时出现概率为 0 的情况怎么办？"
            },
            {
              "id": "ml_0354",
              "question": "1.6 为什么属性独立性假设在实际情况中很难成立，但 NB 仍能取得较好效果？"
            },
            {
              "id": "ml_0355",
              "question": "如何对贝叶斯网络进行采样？"
            },
            {
              "id": "ml_0356",
              "question": "什么是深度学习？"
            },
            {
              "id": "ml_0357",
              "question": "深度学习的训练过程是什么？"
            },
            {
              "id": "ml_0358",
              "question": "深度学习与机器学习有什么区别？"
            },
            {
              "id": "ml_0359",
              "question": "有限。你会怎么做？"
            },
            {
              "id": "ml_0360",
              "question": "个标准偏差的范围内。百分之多少的数据不会受到影响？"
            },
            {
              "id": "ml_0361",
              "question": "“买了这个的客户，也买了......”亚马逊的建议是哪种算法的结果？"
            },
            {
              "id": "ml_0362",
              "question": "对统计这一块了解吗？"
            },
            {
              "id": "ml_0363",
              "question": "p 值是什么？"
            },
            {
              "id": "ml_0364",
              "question": "无监督和有监督算法的区别？"
            },
            {
              "id": "ml_0365",
              "question": "SVM 的推导，特性？"
            },
            {
              "id": "ml_0366",
              "question": "多分类怎么处理？"
            },
            {
              "id": "ml_0367",
              "question": "LR 的推导，特性？"
            },
            {
              "id": "ml_0368",
              "question": "决策树的特性？"
            },
            {
              "id": "ml_0369",
              "question": "SVM、LR、决策树的对比？"
            },
            {
              "id": "ml_0370",
              "question": "GBDT 和随机森林的区别？"
            },
            {
              "id": "ml_0371",
              "question": "如何判断函数凸或非凸？"
            },
            {
              "id": "ml_0372",
              "question": "什么是凸优化？"
            },
            {
              "id": "ml_0373",
              "question": "如何解决类别不平衡问题？"
            },
            {
              "id": "ml_0374",
              "question": "解释对偶的概念。？"
            },
            {
              "id": "ml_0375",
              "question": "如何进行特征选择？"
            },
            {
              "id": "ml_0376",
              "question": "为什么会产生过拟合，有哪些方法可以预防或克服过拟合？"
            },
            {
              "id": "ml_0377",
              "question": "什么是偏差与方差？"
            },
            {
              "id": "ml_0378",
              "question": "神经网络的原理，如何进行训练？"
            },
            {
              "id": "ml_0379",
              "question": "介绍卷积神经网络，和 DBN 有什么区别？"
            },
            {
              "id": "ml_0380",
              "question": "采用 EM 算法求解的模型有哪些，为什么不用牛顿法或梯度下降法？"
            },
            {
              "id": "ml_0381",
              "question": "聚类算法中的距离度量有哪些？"
            },
            {
              "id": "ml_0382",
              "question": "解释贝叶斯公式和朴素贝叶斯分类。？"
            },
            {
              "id": "ml_0383",
              "question": "解释 L1 和 L2 正则化的作用。？"
            },
            {
              "id": "ml_0384",
              "question": "TF-IDF 是什么？"
            },
            {
              "id": "ml_0385",
              "question": "文本中的余弦距离是什么，有哪些作用？"
            }
          ]
        }
      ]
    },
    {
      "category": "Multimodal",
      "subcategories": [
        {
          "subcategory": "Multimodal-General",
          "questions": [
            {
              "id": "mm_0001",
              "question": "一、最近关注的论文，多模态视觉大模型(CLIP,DALLE)？"
            },
            {
              "id": "mm_0002",
              "question": "二、blip2的架构，优势和之前多模态模型的区别？"
            },
            {
              "id": "mm_0003",
              "question": "三、多模态融合后，怎样知道最终结果受哪种模态影响更大？"
            },
            {
              "id": "mm_0004",
              "question": "四、多模态中常见的SOTA模型有哪些？"
            },
            {
              "id": "mm_0005",
              "question": "五、介绍一下stable diffusion的原理？"
            }
          ]
        }
      ]
    },
    {
      "category": "NLP",
      "subcategories": [
        {
          "subcategory": "NLP-General",
          "questions": [
            {
              "id": "nlp_0001",
              "question": "• [x] 1 介绍一下 FFN 块 计算公式？"
            },
            {
              "id": "nlp_0002",
              "question": "• [x] 2 介绍一下 GeLU 计算公式？"
            },
            {
              "id": "nlp_0003",
              "question": "• [x] 3 介绍一下 Swish 计算公式？"
            },
            {
              "id": "nlp_0004",
              "question": "块 计算公式？"
            },
            {
              "id": "nlp_0005",
              "question": "• [ ] 各 LLMs 都使用哪种激活函数？"
            },
            {
              "id": "nlp_0006",
              "question": "2 prefix LM 和 causal LM 区别是什么？"
            },
            {
              "id": "nlp_0007",
              "question": "3 涌现能力是啥原因？"
            },
            {
              "id": "nlp_0008",
              "question": "4 大模型 LLM 的架构介绍？"
            },
            {
              "id": "nlp_0009",
              "question": "i. 什么是 LLMs 复读机问题？"
            },
            {
              "id": "nlp_0010",
              "question": "ii. 为什么会出现 LLMs 复读机问题？"
            },
            {
              "id": "nlp_0011",
              "question": "iii. 如何缓解 LLMs 复读机问题？"
            },
            {
              "id": "nlp_0012",
              "question": "i. llama 输入句子长度理论上可以无限长吗？"
            },
            {
              "id": "nlp_0013",
              "question": "4 各个专业领域是否需要各自的大模型来服务？"
            },
            {
              "id": "nlp_0014",
              "question": "5 如何让大模型处理更长的文本？"
            },
            {
              "id": "nlp_0015",
              "question": "💡 为什么 SFT 之后感觉 LLM 傻了？"
            },
            {
              "id": "nlp_0016",
              "question": "💡 领域模型 Continue PreTrain 数据选取？"
            },
            {
              "id": "nlp_0017",
              "question": "💡 进行 SFT 操作的时候，基座模型选用 Chat 还是 Base？"
            },
            {
              "id": "nlp_0018",
              "question": "💡 大模型 LLM 进行 SFT 操作的时候在学习什么？"
            },
            {
              "id": "nlp_0019",
              "question": "💡 大模型 LLM 进行 SFT 如何对样本进行优化？"
            },
            {
              "id": "nlp_0020",
              "question": "ntry}？"
            },
            {
              "id": "nlp_0021",
              "question": "France？"
            },
            {
              "id": "nlp_0022",
              "question": "请注意，Chat Message History 的具体用法和实现细节可？"
            },
            {
              "id": "nlp_0023",
              "question": "请注意，您可以根据需要自定义 LLM 的参数，例如温度？"
            },
            {
              "id": "nlp_0024",
              "question": "name？"
            },
            {
              "id": "nlp_0025",
              "question": "请注意，您可以根据需要添加、删除和修改聊天消息提？"
            },
            {
              "id": "nlp_0026",
              "question": "请注意，您可以根据需要添加、删除和修改组件。**Chain**？"
            },
            {
              "id": "nlp_0027",
              "question": "LangChain 如何 Embedding & vector store？"
            },
            {
              "id": "nlp_0028",
              "question": "请注意，您可以根据需要添加、删除和修改嵌入向量。？"
            },
            {
              "id": "nlp_0029",
              "question": "板 如何构建？"
            },
            {
              "id": "nlp_0030",
              "question": "• 1 Byte-Pair Encoding(BPE) 如何构建词典？"
            },
            {
              "id": "nlp_0031",
              "question": "• 1 WordPiece 与 BPE 异同点是什么？"
            },
            {
              "id": "nlp_0032",
              "question": "• 简单介绍一下 SentencePiece 思路？"
            },
            {
              "id": "nlp_0033",
              "question": "• 1 举例 介绍一下 不同 大模型 LLMs 的分词方式？"
            },
            {
              "id": "nlp_0034",
              "question": "• 2 介绍一下 不同 大模型 LLMs 的分词方式 的区别？"
            },
            {
              "id": "nlp_0035",
              "question": "• [[#Deep Norm 篇#Deep Norm 思路？"
            },
            {
              "id": "nlp_0036",
              "question": "• [[#Deep Norm 篇#写一下 Deep Norm 代码实现？"
            },
            {
              "id": "nlp_0037",
              "question": "Deep Norm 代码实现？"
            },
            {
              "id": "nlp_0038",
              "question": "式写一下？"
            },
            {
              "id": "nlp_0039",
              "question": "|RMS Norm 的计算公式写一下？"
            },
            {
              "id": "nlp_0040",
              "question": "Layer Norm 有什么特点？"
            },
            {
              "id": "nlp_0041",
              "question": "有什么特点？"
            },
            {
              "id": "nlp_0042",
              "question": "• [[#Layer Norm 篇#Layer Norm 的计算公式写一下？"
            },
            {
              "id": "nlp_0043",
              "question": "Norm 的计算公式写一下？"
            },
            {
              "id": "nlp_0044",
              "question": "• [[#Layer normalization-方法篇#Deep Norm 有什么优点？"
            },
            {
              "id": "nlp_0045",
              "question": "|Deep Norm 有什么优点？"
            },
            {
              "id": "nlp_0046",
              "question": "置 有什么区别么？"
            },
            {
              "id": "nlp_0047",
              "question": "如果有，能介绍一下区别么？"
            },
            {
              "id": "nlp_0048",
              "question": "LLMs 中的不同位置 有什么区别么？"
            },
            {
              "id": "nlp_0049",
              "question": "种 Layer normalization？"
            },
            {
              "id": "nlp_0050",
              "question": "Layer normalization？"
            },
            {
              "id": "nlp_0051",
              "question": "Layer Norm 的计算公式写一下？"
            },
            {
              "id": "nlp_0052",
              "question": "RMS Norm 的计算公式写一下？"
            },
            {
              "id": "nlp_0053",
              "question": "RMS Norm 相比于 Layer Norm 有什么特点？"
            },
            {
              "id": "nlp_0054",
              "question": "写一下 Deep Norm 代码实现？"
            },
            {
              "id": "nlp_0055",
              "question": "层归一化 Layer Norm 在 大语言模型 LLMs 中的不同位置 有什么区别么？"
            },
            {
              "id": "nlp_0056",
              "question": "LLMs 各模型分别用了 哪种 Layer normalization？"
            },
            {
              "id": "nlp_0057",
              "question": "• [[#1 介绍一下 FFN 块 计算公式？"
            },
            {
              "id": "nlp_0058",
              "question": "• [[#2 介绍一下 GeLU 计算公式？"
            },
            {
              "id": "nlp_0059",
              "question": "• [[#3 介绍一下 Swish 计算公式？"
            },
            {
              "id": "nlp_0060",
              "question": "• [[#5 介绍一下 使用 GeLU 的 GLU 块 计算公式？"
            },
            {
              "id": "nlp_0061",
              "question": "• [[#6 介绍一下 使用 Swish 的 GLU 块 计算公式？"
            },
            {
              "id": "nlp_0062",
              "question": "• [[#各 LLMs 都使用哪种激活函数？"
            },
            {
              "id": "nlp_0063",
              "question": "1 介绍一下 FFN 块 计算公式？"
            },
            {
              "id": "nlp_0064",
              "question": "2 介绍一下 GeLU 计算公式？"
            },
            {
              "id": "nlp_0065",
              "question": "3 介绍一下 Swish 计算公式？"
            },
            {
              "id": "nlp_0066",
              "question": "4 介绍一下 使用 GLU 线性门控单元的 FFN 块 计算公式？"
            },
            {
              "id": "nlp_0067",
              "question": "5 介绍一下 使用 GeLU 的 GLU 块 计算公式？"
            },
            {
              "id": "nlp_0068",
              "question": "介绍一下 使用 GeLU 作为激活函数的 GLU 块 计算公式？"
            },
            {
              "id": "nlp_0069",
              "question": "6 介绍一下 使用 Swish 的 GLU 块 计算公式？"
            },
            {
              "id": "nlp_0070",
              "question": "介绍一下 使用 Swish 作为激活函数的 GLU 块 计算公式？"
            },
            {
              "id": "nlp_0071",
              "question": "词转化为其基本形式？"
            },
            {
              "id": "nlp_0072",
              "question": "下列哪些技术能被用于计算两个词向量之间的距离？"
            },
            {
              "id": "nlp_0073",
              "question": "文本语料库的可能特征是什么？"
            },
            {
              "id": "nlp_0074",
              "question": "term matrix）。以下哪项可用于减少数据维度？"
            },
            {
              "id": "nlp_0075",
              "question": "下列哪项是关键词归一化技术？"
            },
            {
              "id": "nlp_0076",
              "question": "下面哪个是 NLP 用例？"
            },
            {
              "id": "nlp_0077",
              "question": "TF（词频）和 IDF（逆文档频率）的乘积的正确值是多少？"
            },
            {
              "id": "nlp_0078",
              "question": "从句子中删除“and”、“is”、“a”、“an”、“the” 这样的词的过程被称为？"
            },
            {
              "id": "nlp_0079",
              "question": "从给定的句子、段落中识别人名、组织名的过程称为？"
            },
            {
              "id": "nlp_0080",
              "question": "下列哪一项不是预处理技术？"
            },
            {
              "id": "nlp_0081",
              "question": "转换为整数或浮点向量的操作？"
            },
            {
              "id": "nlp_0082",
              "question": "将词表示成向量被称为神经词嵌入（Neural Word Embeddings）？"
            },
            {
              "id": "nlp_0083",
              "question": "下列哪种词嵌入支持上下文建模（Context Modeling）？"
            },
            {
              "id": "nlp_0084",
              "question": "下列哪种嵌入方式支持双向上下文（Bidirectional Context）？"
            },
            {
              "id": "nlp_0085",
              "question": "下列哪种词嵌入可以自定义训练特定主题？"
            },
            {
              "id": "nlp_0086",
              "question": "词嵌入捕获多维数据，并表示为向量？"
            },
            {
              "id": "nlp_0087",
              "question": "词嵌入向量有助于确定 2 个 tokens 之间的距离？"
            },
            {
              "id": "nlp_0088",
              "question": "Transformer 架构首先是由下列哪项引入的？"
            },
            {
              "id": "nlp_0089",
              "question": "以下哪种架构可以更快地训练，且需要更少的训练数据？"
            },
            {
              "id": "nlp_0090",
              "question": "相同的词可以通过___________来实现多个词嵌入？"
            },
            {
              "id": "nlp_0091",
              "question": "关。这是哪种架构？"
            },
            {
              "id": "nlp_0092",
              "question": "以下哪种 NLP 模型的准确性最高？"
            },
            {
              "id": "nlp_0093",
              "question": "排列语言模型（Permutation Language Models）是下列哪项的特点？"
            }
          ]
        }
      ]
    },
    {
      "category": "RAG",
      "subcategories": [
        {
          "subcategory": "RAG-Query 改写/扩写",
          "questions": [
            {
              "id": "rag_0001",
              "question": "为什么进行 Query 扩写？"
            },
            {
              "id": "rag_0002",
              "question": "Query 扩写的 Prompt 如何构建？"
            },
            {
              "id": "rag_0003",
              "question": "介绍下 HyDE？"
            }
          ]
        },
        {
          "subcategory": "RAG-向量化与检索",
          "questions": [
            {
              "id": "rag_0004",
              "question": "什么是检索增强的语言模型（Retrieval-based LMs）？"
            },
            {
              "id": "rag_0005",
              "question": "为什么要使用检索增强的语言模型（Retrieval-based LMs）？"
            },
            {
              "id": "rag_0006",
              "question": "长文本如何存储用于检索？"
            },
            {
              "id": "rag_0007",
              "question": "向量化模型都有哪些？"
            },
            {
              "id": "rag_0008",
              "question": "如何构造微调 Bert 相似度向量模型数据？"
            },
            {
              "id": "rag_0009",
              "question": "检索模块的评估指标有哪些？"
            }
          ]
        },
        {
          "subcategory": "RAG-基础与对比",
          "questions": [
            {
              "id": "rag_0010",
              "question": "RAG 和 SFT 微调有什么不同？"
            },
            {
              "id": "rag_0011",
              "question": "长篇知识如何产生问答对？"
            },
            {
              "id": "rag_0012",
              "question": "使用什么相似度模型？"
            },
            {
              "id": "rag_0013",
              "question": "为什么要微调 Bert 模型？"
            },
            {
              "id": "rag_0014",
              "question": "如何提高搜索质量和大语言模型的推理能力？"
            },
            {
              "id": "rag_0015",
              "question": "什么是交互型模型？"
            },
            {
              "id": "rag_0016",
              "question": "输入文档的顺序对大模型是否有影响？"
            },
            {
              "id": "rag_0017",
              "question": "基于 Retrieval-based LMs 的对话流程是怎么样？"
            },
            {
              "id": "rag_0018",
              "question": "RAG 调用模式有几种？"
            },
            {
              "id": "rag_0019",
              "question": "分别是什么？"
            },
            {
              "id": "rag_0020",
              "question": "如何判断上下文是否关联？"
            },
            {
              "id": "rag_0021",
              "question": "上下文长度过长怎么办？"
            },
            {
              "id": "rag_0022",
              "question": "介绍下 RAG-Fusion？"
            },
            {
              "id": "rag_0023",
              "question": "介绍下 SELF-RAG？"
            },
            {
              "id": "rag_0024",
              "question": "生成对应的问题：“什么是大模型？"
            },
            {
              "id": "rag_0025",
              "question": "○ 问题：“什么是大模型？"
            },
            {
              "id": "rag_0026",
              "question": "■ （“什么是冠心病？"
            },
            {
              "id": "rag_0027",
              "question": "”， “冠心病怎么治疗？"
            },
            {
              "id": "rag_0028",
              "question": "■ （“冠心病的症状是什么？"
            },
            {
              "id": "rag_0029",
              "question": "”， “冠心病的治疗方法有哪些？"
            },
            {
              "id": "rag_0030",
              "question": "”， “什么是糖尿病？"
            },
            {
              "id": "rag_0031",
              "question": "请说 “根据已知信息无法回答该问题” 或 “没有提供足够的相关信息”，不允许在？"
            },
            {
              "id": "rag_0032",
              "question": "请添加微信：nlpdreampai，获得专属职业咨询服务。？"
            },
            {
              "id": "rag_0033",
              "question": "2 RAG 有哪些评其方法？"
            },
            {
              "id": "rag_0034",
              "question": "Self-RAG 的训练过中？"
            },
            {
              "id": "rag_0035",
              "question": "Self-RAG 的推理过中？"
            }
          ]
        },
        {
          "subcategory": "RAG-排序与过滤（Rerank）",
          "questions": [
            {
              "id": "rag_0036",
              "question": "介绍下 Rerank 模型？"
            }
          ]
        },
        {
          "subcategory": "RAG-生成与Prompt",
          "questions": [
            {
              "id": "rag_0037",
              "question": "prompt 模板如何构建？"
            }
          ]
        },
        {
          "subcategory": "RAG-知识库构建与切块",
          "questions": [
            {
              "id": "rag_0038",
              "question": "为什么要进行文本切块？"
            },
            {
              "id": "rag_0039",
              "question": "选择分块策略时，需要考虑哪些要素？"
            },
            {
              "id": "rag_0040",
              "question": "分块策略都有哪些？"
            },
            {
              "id": "rag_0041",
              "question": "语义分块模型都有哪些？"
            },
            {
              "id": "rag_0042",
              "question": "什么是句子窗口检索？"
            },
            {
              "id": "rag_0043",
              "question": "什么是父文档检索器？"
            }
          ]
        },
        {
          "subcategory": "RAG-评估与挑战",
          "questions": [
            {
              "id": "rag_0044",
              "question": "如何评估 RAG 系统的准确率上下限问题？"
            },
            {
              "id": "rag_0045",
              "question": "4 RAG 有哪些评其框架？"
            }
          ]
        }
      ]
    },
    {
      "category": "Reinforcement Learning",
      "subcategories": [
        {
          "subcategory": "Reinforcement Learning-General",
          "questions": [
            {
              "id": "rl_0001",
              "question": "一、大语言模型RLHF中的PPO主要分哪些步骤？"
            },
            {
              "id": "rl_0002",
              "question": "二、举例描述一下 大语言模型的RLHF？"
            },
            {
              "id": "rl_0003",
              "question": "1 什么是 PPO 中 采样过程？"
            },
            {
              "id": "rl_0004",
              "question": "2 介绍一下 PPO 中 采样策略？"
            },
            {
              "id": "rl_0005",
              "question": "3 PPO 中 采样策略中，如何评估“收益”？"
            },
            {
              "id": "rl_0006",
              "question": "一、介绍一下 LLM的经典预训练Pipeline？"
            },
            {
              "id": "rl_0007",
              "question": "1 具体介绍一下 预训练（Pre-training）？"
            },
            {
              "id": "rl_0008",
              "question": "1 具体介绍一下 有监督微调（Supervised Tinetuning）？"
            },
            {
              "id": "rl_0009",
              "question": "2 有监督微调（Supervised Tinetuning）的训练数据格式是什么样？"
            },
            {
              "id": "rl_0010",
              "question": "3 预训练（Pre-training） vs 有监督微调（Supervised Tinetuning）区别？"
            },
            {
              "id": "rl_0011",
              "question": "1 简单介绍一下 对齐（Alignment）？"
            },
            {
              "id": "rl_0012",
              "question": "1 简单介绍一下 RLHF 流程？"
            },
            {
              "id": "rl_0013",
              "question": "2 如何在在预训练好的模型上进行有监督微调？"
            },
            {
              "id": "rl_0014",
              "question": "3 如何在有监督微调模型基础上创建一个RM模型？"
            },
            {
              "id": "rl_0015",
              "question": "4 如何基于RM模型使用PPO算法微调SFT模型？"
            },
            {
              "id": "rl_0016",
              "question": "5 instructGPT的原理，讲讲rlhf和reward？"
            },
            {
              "id": "rl_0017",
              "question": "1 介绍一下 LLaMA 2 的 RLHF？"
            },
            {
              "id": "rl_0018",
              "question": "2 LLaMA 2 中 Margin Loss 的 实现逻辑？"
            },
            {
              "id": "rl_0019",
              "question": "3 LLaMA 2 中 两个RM模型 的 实现逻辑？"
            },
            {
              "id": "rl_0020",
              "question": "4 LLaMA 2 中 拒绝采样 逻辑？"
            },
            {
              "id": "rl_0021",
              "question": "1 为什么需要 RLHF 替代方案？"
            },
            {
              "id": "rl_0022",
              "question": "2 RLHF 有哪些替代方案？"
            },
            {
              "id": "rl_0023",
              "question": "1 RLHF 训练过程，怎么选取最优 checkpoint？"
            },
            {
              "id": "rl_0024",
              "question": "Tinetuning）区别？"
            },
            {
              "id": "rl_0025",
              "question": "HIR是如何工作的？"
            },
            {
              "id": "rl_0026",
              "question": "但，现在的问题是：根本无法获得「真实分数」，我们该如何找到这个「最高点」呢？"
            },
            {
              "id": "rl_0027",
              "question": "<!--蒙特卡洛、TD、动态规划的关系？"
            },
            {
              "id": "rl_0028",
              "question": "<!--DQN的几个变种以及各自解决了那些问题？"
            },
            {
              "id": "rl_0029",
              "question": "<!--深度强化学习中的DQN和A3C区别与联系？"
            },
            {
              "id": "rl_0030",
              "question": "<!--策略梯度的推导过程？"
            },
            {
              "id": "rl_0031",
              "question": "<!--策略梯度和actor-critic的关系与对比？"
            },
            {
              "id": "rl_0032",
              "question": "<!--A3C和DDPG区别和共同点？"
            },
            {
              "id": "rl_0033",
              "question": "<!--value-based和policy-based关系？"
            },
            {
              "id": "rl_0034",
              "question": "<!--off-policy和on-policy的好与坏？"
            },
            {
              "id": "rl_0035",
              "question": "<!--表格式到函数近似的理解？"
            },
            {
              "id": "rl_0036",
              "question": "<!--Actor-Critic的优点？"
            },
            {
              "id": "rl_0037",
              "question": "<!--Actor和Critic两者的区别？"
            },
            {
              "id": "rl_0038",
              "question": "<!--advantage(优势函数)推导过程，如何计算？"
            },
            {
              "id": "rl_0039",
              "question": "<!--DPG、DDPG、D3PG、D4PG之间的区别？"
            },
            {
              "id": "rl_0040",
              "question": "<!--强化学习是什么？"
            },
            {
              "id": "rl_0041",
              "question": "和有监督学习的异同？"
            },
            {
              "id": "rl_0042",
              "question": "SL靠的是样本标签训练模型，RL依靠的是什么？"
            },
            {
              "id": "rl_0043",
              "question": "<!--强化学习用来解决什么问题？"
            },
            {
              "id": "rl_0044",
              "question": "<!--强化学习的损失函数是什么？"
            },
            {
              "id": "rl_0045",
              "question": "<!--蒙特卡洛和时间差分的对比：MC和TD分别是无偏估计吗，为什么？"
            },
            {
              "id": "rl_0046",
              "question": "MC、TD谁的方差大，为什么？"
            },
            {
              "id": "rl_0047",
              "question": "<!--简述值函数逼近的想法？"
            },
            {
              "id": "rl_0048",
              "question": "<!--RL的马尔科夫性质？"
            },
            {
              "id": "rl_0049",
              "question": "<!--RL不同于其它学习算法的原因？"
            },
            {
              "id": "rl_0050",
              "question": "<!--Model-based和model-free的区别？"
            },
            {
              "id": "rl_0051",
              "question": "<!--确定性策略和 随机性策略的区别与联系？"
            },
            {
              "id": "rl_0052",
              "question": "<!--on-policy 和off-policy的区别与联系？"
            },
            {
              "id": "rl_0053",
              "question": "<!--重要性采样的推导过程、作用？"
            },
            {
              "id": "rl_0054",
              "question": "<!--Q-learning是off-policy的方法，为什么不使用重要性采样？"
            },
            {
              "id": "rl_0055",
              "question": "从Q-learning的算法中可以看出，其行为策略为-greedy策略，目标策略是greedy策略，因此属于off-policy方法。那么为什么没有用重要性采样呢？"
            },
            {
              "id": "rl_0056",
              "question": "<!--有哪些方法可以使得RL训练稳定？"
            },
            {
              "id": "rl_0057",
              "question": "<!--写出贝尔曼期望方程和贝尔曼最优方程？"
            },
            {
              "id": "rl_0058",
              "question": "<!--贝尔曼期望方程和贝尔曼最优方程什么时候用？"
            },
            {
              "id": "rl_0059",
              "question": "<!--策略梯度算法的目标函数和策略梯度计算？"
            },
            {
              "id": "rl_0060",
              "question": "<!--DQN的原理？"
            },
            {
              "id": "rl_0061",
              "question": "<!--DQN和Sarsa的区别？"
            },
            {
              "id": "rl_0062",
              "question": "<!--为什么使用优势函数？"
            },
            {
              "id": "rl_0063",
              "question": "<!--常见的平衡探索与利用的方法？"
            },
            {
              "id": "rl_0064",
              "question": "<!--TD3如何解决过估计？"
            },
            {
              "id": "rl_0065",
              "question": "<!--TD3和DDPG的区别？"
            },
            {
              "id": "rl_0066",
              "question": "<!--多臂老虎机和强化学习算法的差别？"
            },
            {
              "id": "rl_0067",
              "question": "<!--多臂老虎机算法的分类？"
            },
            {
              "id": "rl_0068",
              "question": "<!--有那几种Bandit算法？"
            },
            {
              "id": "rl_0069",
              "question": "<!--简述UCB算法 （Upper Confidence Bound)？"
            },
            {
              "id": "rl_0070",
              "question": "<!--简述重要性采样，Thompson sampling采样？"
            },
            {
              "id": "rl_0071",
              "question": "<!--什么是强化学习？"
            },
            {
              "id": "rl_0072",
              "question": "<!--强化学习和监督学习、无监督学习的区别是什么？"
            },
            {
              "id": "rl_0073",
              "question": "<!--强化学习适合解决什么样子的问题？"
            },
            {
              "id": "rl_0074",
              "question": "<!--强化学习的损失函数（loss function）是什么？"
            },
            {
              "id": "rl_0075",
              "question": "和深度学习的损失函数有何关系？"
            },
            {
              "id": "rl_0076",
              "question": "<!--POMDP是什么？"
            },
            {
              "id": "rl_0077",
              "question": "马尔科夫过程是什么？"
            },
            {
              "id": "rl_0078",
              "question": "马尔科夫决策过程是什么？"
            },
            {
              "id": "rl_0079",
              "question": "里面的“马尔科夫”体现了什么性质？"
            },
            {
              "id": "rl_0080",
              "question": "<!--贝尔曼方程的具体数学表达式是什么？"
            },
            {
              "id": "rl_0081",
              "question": "<!--最优值函数和最优策略为什么等价？"
            },
            {
              "id": "rl_0082",
              "question": "<!--值迭代和策略迭代的区别？"
            },
            {
              "id": "rl_0083",
              "question": "<!--如果不满足马尔科夫性怎么办？"
            },
            {
              "id": "rl_0084",
              "question": "当前时刻的状态和它之前很多很多个状态都有关之间关系？"
            },
            {
              "id": "rl_0085",
              "question": "<!--求解马尔科夫决策过程都有哪些方法？"
            },
            {
              "id": "rl_0086",
              "question": "有模型用什么方法？"
            },
            {
              "id": "rl_0087",
              "question": "动态规划是怎么回事？"
            },
            {
              "id": "rl_0088",
              "question": "<!--简述动态规划(DP)算法？"
            },
            {
              "id": "rl_0089",
              "question": "<!--MC和TD分别是无偏估计吗？"
            },
            {
              "id": "rl_0090",
              "question": "<!--MC、TD谁的方差大，为什么？"
            },
            {
              "id": "rl_0091",
              "question": "<!--写出用第n步的值函数更新当前值函数的公式（1-step，2-step，n-step的意思）。当n的取值变大时，期望和方差分别变大、变小？"
            },
            {
              "id": "rl_0092",
              "question": "<!--TD（λ）方法：当λ=0时实际上与哪种方法等价，λ=1呢？"
            },
            {
              "id": "rl_0093",
              "question": "<!--写出蒙特卡洛、TD和TD（λ）这三种方法更新值函数的公式？"
            },
            {
              "id": "rl_0094",
              "question": "<!--value-based和policy-based的区别是什么？"
            },
            {
              "id": "rl_0095",
              "question": "<!--DQN的两个关键trick分别是什么？"
            },
            {
              "id": "rl_0096",
              "question": "<!--阐述目标网络和experience replay的作用？"
            },
            {
              "id": "rl_0097",
              "question": "<!--手工推导策略梯度过程？"
            },
            {
              "id": "rl_0098",
              "question": "<!--描述随机策略和确定性策略的特点？"
            },
            {
              "id": "rl_0099",
              "question": "<!--不打破数据相关性，神经网络的训练效果为什么就不好？"
            },
            {
              "id": "rl_0100",
              "question": "<!--画出DQN玩Flappy Bird的流程图。在这个游戏中，状态是什么，状态是怎么转移的？"
            },
            {
              "id": "rl_0101",
              "question": "奖赏函数如何设计，有没有奖赏延迟问题？"
            },
            {
              "id": "rl_0102",
              "question": "<!--DQN都有哪些变种？"
            },
            {
              "id": "rl_0103",
              "question": "引入状态奖励的是哪种？"
            },
            {
              "id": "rl_0104",
              "question": "<!--简述double DQN原理？"
            },
            {
              "id": "rl_0105",
              "question": "恰好，如果我们用上Fixed Q-targets，我们不就是有两个Q网络了吗？"
            },
            {
              "id": "rl_0106",
              "question": "<!--策略梯度方法中基线baseline如何确定？"
            },
            {
              "id": "rl_0107",
              "question": "<!--什么是DDPG，并画出DDPG框架结构图？"
            },
            {
              "id": "rl_0108",
              "question": "<!--Actor-Critic两者的区别是什么？"
            },
            {
              "id": "rl_0109",
              "question": "<!--actor-critic框架中的critic起了什么作用？"
            },
            {
              "id": "rl_0110",
              "question": "<!--DDPG是on-policy还是off-policy，为什么？"
            },
            {
              "id": "rl_0111",
              "question": "<!--是否了解过D4PG算法？"
            },
            {
              "id": "rl_0112",
              "question": "<!--简述A3C算法？"
            },
            {
              "id": "rl_0113",
              "question": "A3C是on-policy还是off-policy，为什么？"
            },
            {
              "id": "rl_0114",
              "question": "<!--A3C算法是如何异步更新的？"
            },
            {
              "id": "rl_0115",
              "question": "是否能够阐述GA3C和A3C的区别？"
            },
            {
              "id": "rl_0116",
              "question": "<!--简述A3C的优势函数？"
            },
            {
              "id": "rl_0117",
              "question": "<!--什么是重要性采样？"
            },
            {
              "id": "rl_0118",
              "question": "<!--为什么TRPO能保证新策略的回报函数单调不减？"
            },
            {
              "id": "rl_0119",
              "question": "<!--如何理解利用平均KL散度代替最大KL散度？"
            },
            {
              "id": "rl_0120",
              "question": "<!--简述PPO算法？"
            },
            {
              "id": "rl_0121",
              "question": "<!--简述DPPO和PPO的关系？"
            },
            {
              "id": "rl_0122",
              "question": "<!--强化学习如何用在推荐系统中？"
            },
            {
              "id": "rl_0123",
              "question": "<!--推荐场景中奖赏函数如何设计？"
            },
            {
              "id": "rl_0124",
              "question": "<!--场景中状态是什么，当前状态怎么转移到下一状态？"
            },
            {
              "id": "rl_0125",
              "question": "<!--自动驾驶和机器人的场景如何建模成强化学习问题？"
            },
            {
              "id": "rl_0126",
              "question": "MDP各元素对应真实场景中的哪些变量？"
            },
            {
              "id": "rl_0127",
              "question": "<!--强化学习需要大量数据，如何生成或采集到这些数据？"
            },
            {
              "id": "rl_0128",
              "question": "<!--是否用某种DRL算法玩过Torcs游戏？"
            },
            {
              "id": "rl_0129",
              "question": "具体怎么解决？"
            },
            {
              "id": "rl_0130",
              "question": "<!--是否了解过奖励函数的设置(reward shaping)？"
            },
            {
              "id": "rl_0131",
              "question": "<!--强化学习中如何处理归一化？"
            },
            {
              "id": "rl_0132",
              "question": "<!--强化学习如何观察收敛曲线？"
            },
            {
              "id": "rl_0133",
              "question": "<!--强化学习如何如何确定收敛？"
            },
            {
              "id": "rl_0134",
              "question": "<!--影响强化学习算法收敛的因素有哪些，如何调优？"
            },
            {
              "id": "rl_0135",
              "question": "<!--多智能体强化学习算法有哪些？"
            },
            {
              "id": "rl_0136",
              "question": "<!--简述Model Based Learning？"
            },
            {
              "id": "rl_0137",
              "question": "有什么新的进展？"
            },
            {
              "id": "rl_0138",
              "question": "比如World Model？"
            },
            {
              "id": "rl_0139",
              "question": "Dream？"
            },
            {
              "id": "rl_0140",
              "question": "MuZero？"
            },
            {
              "id": "rl_0141",
              "question": "<!--简述Meta Reinforcement Learning？"
            },
            {
              "id": "rl_0142",
              "question": "<!--为什么Reptile应用的效果并不好？"
            },
            {
              "id": "rl_0143",
              "question": "<!--Meta RL不好应用的原因有哪些？"
            },
            {
              "id": "rl_0144",
              "question": "<!--简述Meta Gradient Reinforcement Learning？"
            },
            {
              "id": "rl_0145",
              "question": "<!--简述Imitation Learning？"
            },
            {
              "id": "rl_0146",
              "question": "GAIL？"
            },
            {
              "id": "rl_0147",
              "question": "Deepminic？"
            },
            {
              "id": "rl_0148",
              "question": "<!--简述DRL的一些最新改进？"
            },
            {
              "id": "rl_0149",
              "question": "R2D3？"
            },
            {
              "id": "rl_0150",
              "question": "LASER？"
            },
            {
              "id": "rl_0151",
              "question": "<!--简述Multi-Agent Reinforcement Learning？"
            },
            {
              "id": "rl_0152",
              "question": "比如MADDPG比较早的，思想是什么？"
            },
            {
              "id": "rl_0153",
              "question": "和一般的DRL有什么区别？"
            },
            {
              "id": "rl_0154",
              "question": "<!--简述seed rl？"
            },
            {
              "id": "rl_0155",
              "question": "对于大规模分布式强化学习，还有更好的提高throughput的方法吗？"
            },
            {
              "id": "rl_0156",
              "question": "<!--简述AI-GAs？"
            },
            {
              "id": "rl_0157",
              "question": "你对这个理论有什么看法？"
            },
            {
              "id": "rl_0158",
              "question": "<!--简述Out-of-Distributon Generalization？"
            },
            {
              "id": "rl_0159",
              "question": "Modularity？"
            },
            {
              "id": "rl_0160",
              "question": "<!--DRL要实现足够的泛化Generalization有哪些做法？"
            },
            {
              "id": "rl_0161",
              "question": "Randomization？"
            },
            {
              "id": "rl_0162",
              "question": "<!--简述Neural-Symbolic Learning的方法？"
            },
            {
              "id": "rl_0163",
              "question": "怎么看待？"
            },
            {
              "id": "rl_0164",
              "question": "<!--简述unsupervised reinforcement learning？"
            },
            {
              "id": "rl_0165",
              "question": "Diversity is all you need？"
            },
            {
              "id": "rl_0166",
              "question": "<!--简述offline reinforcement learning？"
            },
            {
              "id": "rl_0167",
              "question": "<!--简述Multi-Task Reinforcement Learning？"
            },
            {
              "id": "rl_0168",
              "question": "Policy Distillation？"
            },
            {
              "id": "rl_0169",
              "question": "<!--简述sim2real？"
            },
            {
              "id": "rl_0170",
              "question": "有哪些方法？"
            },
            {
              "id": "rl_0171",
              "question": "<!--对于drl在机器人上的应用怎么看？"
            },
            {
              "id": "rl_0172",
              "question": "<!--简述go-explore？"
            },
            {
              "id": "rl_0173",
              "question": "<!--对于hard exploration的问题，要怎么处理？"
            },
            {
              "id": "rl_0174",
              "question": "<!--简述Transformer？"
            },
            {
              "id": "rl_0175",
              "question": "能否具体介绍一下实现方法？"
            },
            {
              "id": "rl_0176",
              "question": "<!--简述Pointer Network？"
            },
            {
              "id": "rl_0177",
              "question": "和一般的Attention有什么不同？"
            },
            {
              "id": "rl_0178",
              "question": "<!--什么是Importance Sampling？"
            },
            {
              "id": "rl_0179",
              "question": "为什么PPO和IMPALA要使用？"
            },
            {
              "id": "rl_0180",
              "question": "两者在使用方式上有何不同？"
            },
            {
              "id": "rl_0181",
              "question": "能否结合？"
            },
            {
              "id": "rl_0182",
              "question": "<!--PPO在实现上是怎么采样的？"
            },
            {
              "id": "rl_0183",
              "question": "<!--PPO里使用的GAE是怎么实现的？"
            },
            {
              "id": "rl_0184",
              "question": "能否写出计算过程？"
            },
            {
              "id": "rl_0185",
              "question": "<!--是否理解Entropy，KL divergence和Mutual Information的含义？"
            },
            {
              "id": "rl_0186",
              "question": "<!--AlphaStar的scatter connection？"
            },
            {
              "id": "rl_0187",
              "question": "怎么实现的？"
            },
            {
              "id": "rl_0188",
              "question": "<!--对于多个entity的observation，你会怎么预处理？"
            },
            {
              "id": "rl_0189",
              "question": "神经网络要怎么构建？"
            },
            {
              "id": "rl_0190",
              "question": "<!--AlphaStar的League，能否解释一下？"
            },
            {
              "id": "rl_0191",
              "question": "如何让agent足够diverse？"
            },
            {
              "id": "rl_0192",
              "question": "<!--Inverse RL 能否解决奖励问题，如何解决的？"
            },
            {
              "id": "rl_0193",
              "question": "<!--分层强化学习的原理是什么？"
            },
            {
              "id": "rl_0194",
              "question": "<!--简述分层强化学习中基于目标的(goal-reach)和基于目标的(goal-reach）的区别与联系？"
            },
            {
              "id": "rl_0195",
              "question": "<!--请简述IQL（independent Q-learning算法过程？"
            },
            {
              "id": "rl_0196",
              "question": "<!--是否了解α−Rank算法？"
            },
            {
              "id": "rl_0197",
              "question": "<!--请简述QMIX算法？"
            },
            {
              "id": "rl_0198",
              "question": "<!--简述模仿学习与强化学习的区别、联系？"
            },
            {
              "id": "rl_0199",
              "question": "<!--简述MADDPG算法的过程和伪代码？"
            },
            {
              "id": "rl_0200",
              "question": "<!--多智能体之间如何通信、如何竞争？"
            },
            {
              "id": "rl_0201",
              "question": "<!--你熟悉的多智能体环境有哪些？"
            },
            {
              "id": "rl_0202",
              "question": "<!--你做过的强化学习项目有哪些，遇到的难点有哪些？"
            },
            {
              "id": "rl_0203",
              "question": "<!--请简述造成强化学习inefficient的原因？"
            },
            {
              "id": "rl_0204",
              "question": "<!--sarsa的公式以及和Q-leaning的区别？"
            },
            {
              "id": "rl_0205",
              "question": "<!--是否了解RLlib？"
            },
            {
              "id": "rl_0206",
              "question": "Coach？"
            },
            {
              "id": "rl_0207",
              "question": "<!--Ray怎么做梯度并行运算的？"
            },
            {
              "id": "rl_0208",
              "question": "<!--A3C中多线程如何更新梯度？"
            },
            {
              "id": "rl_0209",
              "question": "<!--GA3C算法的queue如何实现？"
            },
            {
              "id": "rl_0210",
              "question": "<!--强化学习的动作、状态以及奖励如何定义的，指标有哪些，包括状态和动作的维度是多少，那些算法效果比较好？"
            },
            {
              "id": "rl_0211",
              "question": "<!--DQN的trick有哪些？"
            },
            {
              "id": "rl_0212",
              "question": "<!--PPO算法中的clip如何实现的？"
            },
            {
              "id": "rl_0213",
              "question": "<!--MADDPG如何解决离散action的？"
            },
            {
              "id": "rl_0214",
              "question": "<!--强化学习在机器人的局限性有哪些？"
            },
            {
              "id": "rl_0215",
              "question": "<!--强化学习中如何解决高纬度输入输出问题？"
            },
            {
              "id": "rl_0216",
              "question": "<!--基于值函数方法的算法有哪些？"
            },
            {
              "id": "rl_0217",
              "question": "其损失函数是什么？"
            },
            {
              "id": "rl_0218",
              "question": "<!--TD(λ)方法：当λ=0时实际上与哪种方法等价，λ=1呢？"
            },
            {
              "id": "rl_0219",
              "question": "<!--为什么Policy中输出的动作需要sample，而不是直接使用呢？"
            },
            {
              "id": "rl_0220",
              "question": "<!--为什么连续动作环境下使用DDPG的表现还没有直接动作离散化后Q-learning表现好？"
            },
            {
              "id": "rl_0221",
              "question": "<!--PPO算法中的损失函由那些组成？"
            },
            {
              "id": "rl_0222",
              "question": "<!--你在强化学习模型调试中，有哪些调优技巧？"
            },
            {
              "id": "rl_0223",
              "question": "<!--简述PPO、DPPO算法？"
            },
            {
              "id": "rl_0224",
              "question": "<!--简述PER算法、HER算法？"
            },
            {
              "id": "rl_0225",
              "question": "<!--离散action和连续action在处理上有什么相似和不同的地方？"
            }
          ]
        }
      ]
    },
    {
      "category": "Transformer",
      "subcategories": [
        {
          "subcategory": "Transformer-General",
          "questions": [
            {
              "id": "tr_0001",
              "question": "什么是 transformer 呢？"
            },
            {
              "id": "tr_0002",
              "question": "transformer 这个黑盒子里面都有什么呢？"
            },
            {
              "id": "tr_0003",
              "question": "为什么要引入 Transformer？"
            },
            {
              "id": "tr_0004",
              "question": "Transformer 有何特点？"
            },
            {
              "id": "tr_0005",
              "question": "你怎么理解注意力机制？"
            },
            {
              "id": "tr_0006",
              "question": "你了解哪些 attention 机制？"
            },
            {
              "id": "tr_0007",
              "question": "为什么要做 softmax 标准化？"
            },
            {
              "id": "tr_0008",
              "question": "为什么后续有不少工作尝试对 softmax 进行替换？"
            },
            {
              "id": "tr_0009",
              "question": "transformer 的输入是什么样的？"
            },
            {
              "id": "tr_0010",
              "question": "为什么要做 position embedding/encoding？"
            },
            {
              "id": "tr_0011",
              "question": "为什么要使用 qeury，value，key 矩阵？"
            },
            {
              "id": "tr_0012",
              "question": "self attention 部分怎么计算的？"
            },
            {
              "id": "tr_0013",
              "question": "scaled dot production 为什么要除以一个根号 dk？"
            },
            {
              "id": "tr_0014",
              "question": "Position-wise feed-forward networks 具体是怎么设计的？"
            },
            {
              "id": "tr_0015",
              "question": "为什么要加 ffn？"
            },
            {
              "id": "tr_0016",
              "question": "transformer 其它的一些细节问题？"
            },
            {
              "id": "tr_0017",
              "question": "Transformer 为何使用多头注意力机制？"
            },
            {
              "id": "tr_0018",
              "question": "自身的点乘？"
            },
            {
              "id": "tr_0019",
              "question": "Transformer 计算 attention 的时候为何选择点乘而不是加法？"
            },
            {
              "id": "tr_0020",
              "question": "果上有什么区别？"
            },
            {
              "id": "tr_0021",
              "question": "为什么在进行 softmax 之前需要对 attention 进行 scaled（为什么除以 dk 的平方？"
            },
            {
              "id": "tr_0022",
              "question": "为什么需要进行 scaled？"
            },
            {
              "id": "tr_0023",
              "question": "在计算 attention score 的时候如何对 padding 做 mask 操作？"
            },
            {
              "id": "tr_0024",
              "question": "为什么在进行多头注意力的时候需要对每个 head 进行降维？"
            },
            {
              "id": "tr_0025",
              "question": "意力而不用单头注意力？"
            },
            {
              "id": "tr_0026",
              "question": "大概讲一下 Transformer 的 Encoder 模块？"
            },
            {
              "id": "tr_0027",
              "question": "为何在获取输入词向量之后需要对矩阵乘以 embedding size 的开方？"
            },
            {
              "id": "tr_0028",
              "question": "意义是什么？"
            },
            {
              "id": "tr_0029",
              "question": "简单介绍一下 Transformer 的位置编码？"
            },
            {
              "id": "tr_0030",
              "question": "有什么意义和优缺点？"
            },
            {
              "id": "tr_0031",
              "question": "你还了解哪些关于位置编码的技术，各自的优缺点是什么？"
            },
            {
              "id": "tr_0032",
              "question": "Encoder 端和 Decoder 端是如何进行交互的？"
            },
            {
              "id": "tr_0033",
              "question": "Decoder 阶段的多头自注意力和 encoder 的多头自注意力有什么区别？"
            },
            {
              "id": "tr_0034",
              "question": "为什么需要 Decoder 自注意力进行序列掩码？"
            },
            {
              "id": "tr_0035",
              "question": "Transformer 的并行化提现在哪个地方？"
            },
            {
              "id": "tr_0036",
              "question": "Decoder 端可以做并行化吗？"
            },
            {
              "id": "tr_0037",
              "question": "解释性 ................................................................................................................................................ 35？"
            },
            {
              "id": "tr_0038",
              "question": "Transformer 是如何工作的？"
            },
            {
              "id": "tr_0039",
              "question": "Transformer 的优势是什么？"
            },
            {
              "id": "tr_0040",
              "question": "Transformer 的局限性是什么？"
            },
            {
              "id": "tr_0041",
              "question": "什么是 Transformer 及其架构？"
            },
            {
              "id": "tr_0042",
              "question": "它与传统神经网络有何不同？"
            },
            {
              "id": "tr_0043",
              "question": "Transformer 是如何训练的？"
            },
            {
              "id": "tr_0044",
              "question": "Transformer 中的自注意力机制是什么？"
            },
            {
              "id": "tr_0045",
              "question": "训练和实现 Transformer 时有哪些常见挑战，如何改进其性能？"
            },
            {
              "id": "tr_0046",
              "question": "如何决定 Transformer 中的层数和注意力头的数量？"
            },
            {
              "id": "tr_0047",
              "question": "如何处理 Transformer 中的不同长度的输入序列？"
            },
            {
              "id": "tr_0048",
              "question": "如何处理 Transformer 中的缺失/损坏数据并解决过拟合问题？"
            },
            {
              "id": "tr_0049",
              "question": "如何微调预训练的 Transformer 以适应特定任务？"
            },
            {
              "id": "tr_0050",
              "question": "如何确定 Transformer 的适当容量水平？"
            },
            {
              "id": "tr_0051",
              "question": "transformer 中的前馈神经网络？"
            },
            {
              "id": "tr_0052",
              "question": "其中使用了什么激活函数？"
            },
            {
              "id": "tr_0053",
              "question": "Encoder 和 Decoder 端是如何进行交互的？"
            },
            {
              "id": "tr_0054",
              "question": "decoder 阶段的多头注意力机制和 encoder 的多头注意力机制有什么区别？"
            },
            {
              "id": "tr_0055",
              "question": "Transformer 的并行化体现在什么地方？"
            },
            {
              "id": "tr_0056",
              "question": "Decoder 端可以做并行化么？"
            },
            {
              "id": "tr_0057",
              "question": "为什么 decoder 需要 sequence mask？"
            },
            {
              "id": "tr_0058",
              "question": "transformer 哪里做了权重共享？"
            },
            {
              "id": "tr_0059",
              "question": "为什么可以做权重共享？"
            },
            {
              "id": "tr_0060",
              "question": "好处是什么？"
            },
            {
              "id": "tr_0061",
              "question": "bert 的 mask 为何不学习 transformer 在 attention 处进行屏蔽 score 的操作？"
            },
            {
              "id": "tr_0062",
              "question": "什么是深度学习，它与传统机器学习有什么不同？"
            },
            {
              "id": "tr_0063",
              "question": "权重初始化如何影响深度学习模型的性能？"
            },
            {
              "id": "tr_0064",
              "question": "你能解释感知器和S形神经元的区别吗？"
            },
            {
              "id": "tr_0065",
              "question": "你能解释反向传播在神经网络中是如何工作的吗？"
            },
            {
              "id": "tr_0066",
              "question": "神经网络中常用的激活函数有哪些？"
            },
            {
              "id": "tr_0067",
              "question": "如何防止神经网络的过拟合？"
            },
            {
              "id": "tr_0068",
              "question": "如何决定神经网络的层数和神经元数？"
            },
            {
              "id": "tr_0069",
              "question": "如何处理神经网络中缺失的数据？"
            },
            {
              "id": "tr_0070",
              "question": "你能解释一下深度学习中迁移学习的概念吗？"
            },
            {
              "id": "tr_0071",
              "question": "你如何评估一个深度学习模型的性能？"
            },
            {
              "id": "tr_0072",
              "question": "过拟合和欠拟合的区别是什么？"
            },
            {
              "id": "tr_0073",
              "question": "神经网络中Dropout 的目的是什么？"
            },
            {
              "id": "tr_0074",
              "question": "批归一化是如何工作的？"
            },
            {
              "id": "tr_0075",
              "question": "激活函数在神经网络中的作用是什么？"
            },
            {
              "id": "tr_0076",
              "question": "深度神经网络和浅层神经网络的区别是什么？"
            },
            {
              "id": "tr_0077",
              "question": "梯度下降法和随机梯度下降法有什么区别？"
            },
            {
              "id": "tr_0078",
              "question": "什么是 Transformer 模型？"
            },
            {
              "id": "tr_0079",
              "question": "它与传统的 RNN 和 CNN 有何不同？"
            },
            {
              "id": "tr_0080",
              "question": "Transformer 中的自注意力机制是如何工作的？"
            },
            {
              "id": "tr_0081",
              "question": "Transformer 模型中的多头注意力机制有什么优点？"
            },
            {
              "id": "tr_0082",
              "question": "什么是位置编码（Positional Encoding），为什么在 Transformer 中需要它？"
            },
            {
              "id": "tr_0083",
              "question": "Transformer 模型的编码器和解码器结构是怎样的？"
            },
            {
              "id": "tr_0084",
              "question": "在 Transformer 模型的训练过程中，为什么要使用掩码（Mask）？"
            },
            {
              "id": "tr_0085",
              "question": "什么是 BERT 模型？"
            },
            {
              "id": "tr_0086",
              "question": "它与原始的 Transformer 有何不同？"
            },
            {
              "id": "tr_0087",
              "question": "GPT 模型的架构和特点是什么？"
            },
            {
              "id": "tr_0088",
              "question": "Transformer 中的前馈神经网络有什么作用？"
            },
            {
              "id": "tr_0089",
              "question": "什么是多任务学习，Transformer 如何应用于多任务学习？"
            },
            {
              "id": "tr_0090",
              "question": "什么是注意力矩阵，如何计算？"
            },
            {
              "id": "tr_0091",
              "question": "在 Transformer 模型中，如何处理长序列数据？"
            },
            {
              "id": "tr_0092",
              "question": "什么是层归一化（Layer Normalization）？"
            },
            {
              "id": "tr_0093",
              "question": "，它在 Transformer 中有什么作用？"
            },
            {
              "id": "tr_0094",
              "question": "如何理解 Transformer 模型中的残差连接（Residual Connection）？"
            },
            {
              "id": "tr_0095",
              "question": "什么是预训练和微调（Fine-tuning）？"
            },
            {
              "id": "tr_0096",
              "question": "，在 Transformer 模型中如何应用？"
            },
            {
              "id": "tr_0097",
              "question": "Transformer 在机器翻译任务中的应用是怎样的？"
            },
            {
              "id": "tr_0098",
              "question": "什么是 Transformer 模型中的头（Head），它的作用是什么？"
            },
            {
              "id": "tr_0099",
              "question": "在自然语言生成任务中，Transformer 如何确保生成文本的连贯性和一致性？"
            },
            {
              "id": "tr_0100",
              "question": "如何评估和改进 Transformer 模型的性能？"
            },
            {
              "id": "tr_0101",
              "question": "说明）,这样我们就得到了一个(1,3,256)的无序的 sequence data 了；？"
            },
            {
              "id": "tr_0102",
              "question": "马东什么：kaggle 上的 attention layer 到底实现的是啥？"
            },
            {
              "id": "tr_0103",
              "question": "self attention 无法学习到序列信息？"
            },
            {
              "id": "tr_0104",
              "question": "为何不能使用同一个值进行自身的点乘？"
            },
            {
              "id": "tr_0105",
              "question": "两者计算复杂度和效果上有什么区别？"
            },
            {
              "id": "tr_0106",
              "question": "为什么在进行 softmax 之前需要对 attention 进行？"
            },
            {
              "id": "tr_0107",
              "question": "为什么在进行多头注意力的时候需要对每个 head 进行降？"
            },
            {
              "id": "tr_0108",
              "question": "为什么要设计多头注意力而不用单头注意力？"
            },
            {
              "id": "tr_0109",
              "question": "size 的开方？"
            },
            {
              "id": "tr_0110",
              "question": "力有什么区别？"
            },
            {
              "id": "tr_0111",
              "question": "以做并行化吗？"
            },
            {
              "id": "tr_0112",
              "question": "解释性？"
            },
            {
              "id": "tr_0113",
              "question": "如何处理 Transformer 中的缺失/损坏数据并解决过拟合？"
            },
            {
              "id": "tr_0114",
              "question": "力机制有什么区别？"
            },
            {
              "id": "tr_0115",
              "question": "以做并行化么？"
            },
            {
              "id": "tr_0116",
              "question": "mutil_head_self_attention 里，为什么这样做？"
            },
            {
              "id": "tr_0117",
              "question": "进行屏蔽 score 的操作？"
            },
            {
              "id": "tr_0118",
              "question": "你能解释卷积神经网络和循环神经网络的区别 吗？"
            },
            {
              "id": "tr_0119",
              "question": "什么是位置编码（Positional Encoding），为什么在？"
            },
            {
              "id": "tr_0120",
              "question": "Transformer 中需要它？"
            },
            {
              "id": "tr_0121",
              "question": "（Mask）？"
            },
            {
              "id": "tr_0122",
              "question": "什么是多任务学习，Transformer 如何应用于多任务学？"
            },
            {
              "id": "tr_0123",
              "question": "什么是层归一化（Layer Normalization），它在？"
            },
            {
              "id": "tr_0124",
              "question": "Transformer 中有什么作用？"
            },
            {
              "id": "tr_0125",
              "question": "如何理解 Transformer 模型中的残差连接（Residual？"
            },
            {
              "id": "tr_0126",
              "question": "Connection）？"
            },
            {
              "id": "tr_0127",
              "question": "什么是预训练和微调（Fine-tuning），在 Transformer？"
            },
            {
              "id": "tr_0128",
              "question": "模型中如何应用？"
            },
            {
              "id": "tr_0129",
              "question": "什么是 Transformer 模型中的头（Head），它的作用是？"
            },
            {
              "id": "tr_0130",
              "question": "的连贯性和一致性？"
            },
            {
              "id": "tr_0131",
              "question": "c.self-attention一定要这样表达吗？"
            },
            {
              "id": "tr_0132",
              "question": "d.有其他方法不用除根号 吗？"
            },
            {
              "id": "tr_0133",
              "question": "e.为什么transformer用Layer Norm？"
            },
            {
              "id": "tr_0134",
              "question": "有什么用？"
            },
            {
              "id": "tr_0135",
              "question": "任何norm的意义都是为了让使用norm的网络的输入的数据分布变得更好，也就是转换为标准正态分布，数值进入敏感度区间，以减缓梯度消失，从而更容易训练。当然，这也意味着舍弃了除此维度之外其他维度的其他信息。为什么能舍弃呢？"
            },
            {
              "id": "tr_0136",
              "question": "f.为什么不用BN？"
            },
            {
              "id": "tr_0137",
              "question": "既然前面说了是CV中用BN，那为什么NLP中不用BN，而用LN呢？"
            },
            {
              "id": "tr_0138",
              "question": "“为”、“我”、“母”归一到同一分布后，第一句话中的“为”和“中”就没有可比性了，何谈同一句子之间的注意力机制？"
            },
            {
              "id": "tr_0139",
              "question": "g.Bert为什么要搞一个position embedding？"
            },
            {
              "id": "tr_0140",
              "question": "h.Bert为什么三个embedding可以相加？"
            },
            {
              "id": "tr_0141",
              "question": "为什么 Bert 的三个 Embedding 可以进行相加？"
            },
            {
              "id": "tr_0142",
              "question": "由此可以再深入想一想，在一串文本中，如果每个词的特征都可以用叠加波来表示，整个序列又可以进一步叠加。哪些是低频信号（比如词性？"
            },
            {
              "id": "tr_0143",
              "question": "），哪些是高频信号（比如语义？"
            },
            {
              "id": "tr_0144",
              "question": "i.transformer为什么要用三个不一样的QKV？"
            },
            {
              "id": "tr_0145",
              "question": "j.为什么要多头？"
            },
            {
              "id": "tr_0146",
              "question": "k.为什么Bert中要用WordPiece/BPE这样的subword Token？"
            },
            {
              "id": "tr_0147",
              "question": "l.Bert中为什么要在开头加个[CLS]？"
            },
            {
              "id": "tr_0148",
              "question": "那关键点就在于，为什么[CLS]可以建模整句话的语义表征呢？"
            },
            {
              "id": "tr_0149",
              "question": "——为什么无明显语义？"
            },
            {
              "id": "tr_0150",
              "question": "因为训练的时候BERT发现每个句子头都有，这样他能学到什么语义呢？"
            },
            {
              "id": "tr_0151",
              "question": "——为什么要公平？"
            },
            {
              "id": "tr_0152",
              "question": "当然，不放在句子开头的其他位置是否可行？"
            },
            {
              "id": "tr_0153",
              "question": "m.不用[CLS]的语义输出，有其他方式可以代替吗？"
            },
            {
              "id": "tr_0154",
              "question": "n.Bert中有哪些地方用到了mask？"
            },
            {
              "id": "tr_0155",
              "question": "o.预训练阶段的mask有什么用？"
            },
            {
              "id": "tr_0156",
              "question": "p.attention中的mask有什么用？"
            },
            {
              "id": "tr_0157",
              "question": "t.Bert是如何处理传统方法难以搞定的溢出词表词(oov)的语义学习的？"
            },
            {
              "id": "tr_0158",
              "question": "u.中文是如何处理溢出词表词(oov)的语义学习的？"
            },
            {
              "id": "tr_0159",
              "question": "x.为什么说GPT是单向的Bert是双向的？"
            },
            {
              "id": "tr_0160",
              "question": "y.Bert如何处理一词多义？"
            },
            {
              "id": "tr_0161",
              "question": "z.Bert中的transformer和原生的transformer有什么区别？"
            },
            {
              "id": "tr_0162",
              "question": "z+.Albert是通过什么方法压缩网络参数的？"
            },
            {
              "id": "tr_0163",
              "question": "有什么问题？"
            },
            {
              "id": "tr_0164",
              "question": "预训练和微调是哪个阶段注入知识的？"
            },
            {
              "id": "tr_0165",
              "question": "想让模型学习垂直领域的知识，是应该预训练还是微调？"
            },
            {
              "id": "tr_0166",
              "question": "微调后的大模型出现灾难性遗忘是什么原因？"
            },
            {
              "id": "tr_0167",
              "question": "什么是LLM的复读机问题？"
            },
            {
              "id": "tr_0168",
              "question": "出现复读机问题的可能原因有哪些？"
            },
            {
              "id": "tr_0169",
              "question": "解决大模型复读机问题可用哪些策略？"
            },
            {
              "id": "tr_0170",
              "question": "如何缓解大模型的复读机问题是一个复杂的任务，并没有一个通用的解决方案。不同的方法可能适用于不同的业务场景和任务，需要根据具体的情况进行选择和调整。下面是几种用于缓解大模型复读机问题的几种解决方案。？"
            },
            {
              "id": "tr_0171",
              "question": "LoRA怎么做的，讲一下？"
            },
            {
              "id": "tr_0172",
              "question": "为什么可以用LoRA？"
            },
            {
              "id": "tr_0173",
              "question": "国外开源的LLaMA的词表实际上兼容中文效果可能会大打折扣，那么扩充词表该怎么做？"
            }
          ]
        }
      ]
    }
  ]
}
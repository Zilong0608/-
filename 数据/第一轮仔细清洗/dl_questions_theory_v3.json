{
  "source": "深度学习面试题.docx",
  "mode": "theory_only_clean_regroup_v3",
  "question_count": 131,
  "questions": [
    "1各个激活函数的优缺点？",
    "2为什么ReLU常用于神经网络的激活函数？",
    "3 梯度消失和梯度爆炸的解决方案？梯度爆炸引发的问题？",
    "4如何确定是否出现梯度爆炸？",
    "5神经网络中有哪些正则化技术？",
    "6批量归一化(BN) 如何实现？作用？",
    "7神经网络中权值共享的理解？",
    "8 对fine-tuning(微调模型)的理解？为什么要修改最后几层神经网络权值？",
    "9 什么是Dropout？为什么有用？它是如何工作的？",
    "10如何选择dropout 的概率？",
    "11什么是Adam？Adam和SGD之间的主要区别是什么？",
    "12为什么Momentum可以加速训练？",
    "13什么时候使用Adam和SGD？‍？",
    "什么是14 batch size和epoch的平衡？",
    "15 SGD每步做什么，为什么能online learning？",
    "16学习率太大(太小)时会发生什么？如何设置学习率？‍？",
    "17神经网络为什么不用拟牛顿法而是用梯度下降？",
    "18 BN和Dropout在训练和测试时的差别？",
    "19若网络初始化为0的话有什么问题？",
    "20 sigmoid和softmax的区别？softmax的公式？",
    "21改进的softmax损失函数有哪些？",
    "22深度学习调参有哪些技巧？",
    "23神经网络调参，要往哪些方向想？",
    "24深度学习训练中是否有必要使用L1获得稀疏解？",
    "25神经网络数据预处理方法有哪些？中心化/零均值，归一化？",
    "26如何初始化神经网络的权重？‍神经网络怎样进行参数初始化？",
    "27为什么构建深度学习模型需要使用GPU？",
    "28前馈神经网络(FNN),递归神经网络(RNN)和 CNN 区别？",
    "29神经网络可以解决哪些问题？",
    "30如何提高小型网络的精度？",
    "32什么是鞍点问题？梯度为0，海森矩阵不定的点，不是极值点。？",
    "33网络设计中，为什么卷积核设计尺寸都是奇数？",
    "什么是2 Keras搭建CNN？",
    "什么是1 LeNet？",
    "什么是2 AlexNet？",
    "什么是4 Inception(GoogLeNet)？",
    "什么是5 ResNet？",
    "什么是6 DenseNet？",
    "3卷积层有哪些基本参数？",
    "4如何计算卷积层的输出的大小？",
    "5如何计算卷积层参数数量？",
    "7 1*1卷积的作用？",
    "8卷积层和池化层有什么区别？",
    "9卷积核是否一定越大越好？",
    "10卷积在图像中有什么直观作用？",
    "11 CNN中空洞卷积的作用是什么？",
    "12怎样才能减少卷积层参数量？",
    "13在进行卷积操作时，必须同时考虑通道和区域吗？",
    "14采用宽卷积,窄卷积的好处有什么？",
    "16如何提高卷积神经网络的泛化能力？",
    "17卷积神经网络在NLP与CV领域应用的区别？",
    "18全连接、局部连接、全卷积与局部卷积的区别？",
    "19卷积层和全连接层的区别？",
    "20 Max pooling如何工作？还有其他池化技术吗？‍？",
    "21卷积神经网络的优点？为什么用小卷积核？",
    "22 CNN拆成3x1 1x3的优点？",
    "23 BN、LN、IN、GN和SN的区别？",
    "1 RNNs训练和传统ANN训练异同点？",
    "2 为什么RNN 训练的时候Loss波动很大？",
    "3 RNN中为什么会出现梯度消失？",
    "4如何解决RNN中的梯度消失问题？",
    "什么是5 CNN VS RNN？",
    "什么是6 Keras搭建RNN？",
    "1 LSTM结构推导，为什么比RNN好？",
    "2为什么LSTM模型中既存在sigmoid又存在tanh两种激活函数，而不是选择统一一种sigmoid或者tanh？",
    "3 LSTM中为什么经常是两层双向LSTM？",
    "什么是4 RNN扩展改进？",
    "什么是1 Bidirectional RNNs？",
    "什么是2 CNN-LSTMs？",
    "什么是3 Bidirectional LSTMs？",
    "5 LSTM、RNN、GRU区别？",
    "6 LSTM是如何实现长短期记忆功能的？",
    "7 LSTM的原理、写LSTM的公式、手推LSTM的梯度反向传播？",
    "1 什么是反向传播？‍？",
    "2反向传播是如何工作的？",
    "3 为什么需要反向传播？",
    "1神经网络中包含哪些超参数？",
    "2为什么要进行超参数调优？",
    "4极端批样本数量下，如何训练网络？",
    "什么是5 合理使用预训练网络？",
    "1 什么是微调（fine-tune）？",
    "2微调有哪些不同方法？",
    "3 微调先冻结底层，训练顶层的原因？",
    "4不同的数据集特性下如何微调？",
    "5目标检测中使用预训练模型的优劣？",
    "6目标检测中如何从零开始训练(train from scratch)？",
    "6自动化超参数搜索方法有哪些？",
    "为什么需要激活功能？",
    "另一种解释：当反向传播进行很多层的时候，由于每一层都对前一层梯度乘以了一个小数，因此越往前传递，梯度就会越小，训练越慢。？",
    "为什么dropout可以解决过拟合？",
    "Dropout 在训练和测试的区别？",
    "12一阶优化和二阶优化的方法有哪些？为什么不使用二阶优化？",
    "牛顿法原理：使用函数f(x)的泰勒级数的前面几项来寻找方程f(x)= 0的根。？",
    "什么是1 保证有大量、高质量并且带有干净标签的数据;？",
    "什么是2 样本要随机化，防止大数据淹没小数据;？",
    "什么是3 样本要做归一化.？",
    "什么是1 用一个一般的lr开始，然后逐渐的减小它;？",
    "什么是2 建议值是0.01，适用于很多NN的问题，一般倾向于小一点;？",
    "使用验证集，可以知道什么时候开始降低学习率，和什么时候停止训练。？",
    "什么是1 用高斯分布乘上一个很小的数？",
    "1 AlexNet 对比LeNet 的优势？",
    "1 VGG使用2个3*3卷积的优势在哪里？",
    "2每层卷积是否只能用一种尺寸的卷积核？",
    "1 inception结构能不能缓解梯度消失？",
    "1 ResNet为什么不用Dropout？",
    "2 ResNet网络越来越深，准确率会不会提升？",
    "3 ResNet v1 与 ResNet v2的区别？",
    "1 DenseNet 比 ResNet 好？",
    "DensetNet连接有正则化的作用，可以减少过拟合。？DenseNet直接连接不同层的特征图，而不是像ResNet一样element-wise sum。2.2.6.2为什么 DenseNet 比 ResNet 更耗显存？",
    "9卷积核是否一定越大越好？它的优点有哪些？",
    "为什么需要卷积？不能使用全连接层吗？‍？",
    "为什么降采用使用max pooling，而分类使用average pooling？",
    "CNN是否抗旋转？如果旋转图像，CNN的预测会怎样？",
    "什么是数据增强？为什么需要它们？你知道哪种增强？",
    "如何选择要使用的增强？‍？",
    "什么是迁移学习？它是如何工作的？‍？",
    "什么是目标检测？你知道有哪些框架吗？",
    "什么是对象分割？你知道有哪些框架吗？",
    "RNN 和 传统神经网络 最大的区别？",
    "文本生成 2.语音识别 3.机器翻译 4.生成图像描述 5.视频标记？它的缺点有哪些？",
    "RNN可以用于描述时间上连续状态的输出，有记忆功能，CNN用于静态输出。？",
    "​ 作用对象：细胞状态 。？",
    "​ 作用：将细胞状态中的信息选择性的遗忘。？",
    "Ft和Ct-1做点积操作，Ft确保Ct-1有哪些东西需要被遗忘调？",
    "作用对象：细胞状态？",
    "​ 作用：将新的信息选择性的记录到细胞状态中。？",
    "​ 作用对象：隐层ht 作用：确定输出什么值。？",
    "1 LSTM结构推导，为什么比RNN好？推导forget gate，input gate，cell state， hidden information等的变化；因为LSTM有进有出且当前的cell informaton是通过input gate控制之后叠加的，RNN是叠乘，因此LSTM可以防止梯度消失或者爆炸。？",
    "类比几个人站成一排，第一个人看一幅画（输入数据），描述给第二个人（隐层）……依此类推，到最后一个人（输出）的时候，画出来的画肯定不能看了（误差较大）。？",
    "反向传播就是：把画拿给最后一个人看（求取误差），然后最后一个人就会告诉前面的人下次描述时需要注意哪里（权值修正）？",
    "推导：链式求导法则反复用？"
  ]
}
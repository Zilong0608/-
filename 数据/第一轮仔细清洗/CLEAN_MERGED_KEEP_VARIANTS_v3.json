{
  "version": "1.0-merged-exactdedup-keepvariants",
  "note": "仅做精确去重（保留同义改写为 variants）；删除明显截断/残句问题。",
  "dropped_truncated": 230,
  "total_questions": 5458,
  "categories": [
    {
      "category": "Data Analysis",
      "count": 5,
      "subcategories": [
        {
          "subcategory": "DA-基础",
          "count": 5,
          "questions": [
            {
              "id": "da_0001",
              "question": "什么是指标体系？如何建立？业务应用场景？",
              "variants": [],
              "sources": [
                "数据分析常考面试题101题_only_questions_theory.json"
              ]
            },
            {
              "id": "da_0002",
              "question": "什么是漏斗分析？有哪些注意的点？",
              "variants": [],
              "sources": [
                "数据分析常考面试题101题_only_questions_theory.json"
              ]
            },
            {
              "id": "da_0003",
              "question": "你是怎么理解数据分析的？流程如何？",
              "variants": [],
              "sources": [
                "数据分析常考面试题101题_only_questions_theory.json"
              ]
            },
            {
              "id": "da_0004",
              "question": "如何理解假设检验中的 P 值和显著性水平 α？？",
              "variants": [],
              "sources": [
                "数据分析常考面试题101题_only_questions_theory.json"
              ]
            },
            {
              "id": "da_0005",
              "question": "如何评估一场活动的效果？请结合以下方面说明：活动效果怎么样？要不要继续做、如果可以继续做，活动的做的好的方面是哪些？问题或者瓶颈环节在哪、针对问题环节的改进方案是什么、活动关键指标达成分析、活动关键流程漏斗分析、活动的渠道、用户分析、活动策略、节奏分析。？",
              "variants": [],
              "sources": [
                "数据分析常考面试题101题_only_questions_theory.json"
              ]
            }
          ]
        }
      ]
    },
    {
      "category": "Database",
      "count": 69,
      "subcategories": [
        {
          "subcategory": "DB-事务与并发",
          "count": 22,
          "questions": [
            {
              "id": "db_0001",
              "question": "Dubbo⽀支持分布式事务吗？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0002",
              "question": "EJB 的生命周期，以及如何管理事务？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0003",
              "question": "Linux 中主要有哪几种内核锁？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0004",
              "question": "MYSQL 支持事务吗？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0005",
              "question": "MySQL 的MyISAM 与InnoDB 两种存储引擎在，事务、锁级别，各自的适用场景？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0006",
              "question": "Mysql 中有哪几种锁？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0007",
              "question": "Redis 事务相关的命令有哪几个？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0008",
              "question": "Spring 的事务传播行为？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0009",
              "question": "Zookeeper 是如何保证事务的顺序一直性的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0010",
              "question": "python常用的加锁方式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0011",
              "question": "产生死锁的条件？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0012",
              "question": "什么是乐观锁和悲观锁？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0013",
              "question": "什么是乐观锁？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0014",
              "question": "什么是事务隔离级别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0015",
              "question": "什么是悲观锁？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0016",
              "question": "什么是递归锁？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0017",
              "question": "列举spring 支持的事务管理类型？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0018",
              "question": "如何使用Redis 做分布式锁？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0019",
              "question": "如何执行事务/加锁？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0020",
              "question": "怎么理解Redis 事务？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0021",
              "question": "死锁的原因？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0022",
              "question": "请简洁描述Mysql 中InnoDB 支持的四种事务隔离级别名称，以及逐级之间的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            }
          ]
        },
        {
          "subcategory": "DB-基础",
          "count": 24,
          "questions": [
            {
              "id": "db_0023",
              "question": "MYSQL 数据库服务器性能分析的方法命令有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0024",
              "question": "MYSQL 数据表在什么情况下容易损坏？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0025",
              "question": "MongoDB 成为最好NoSQL 数据库的原因是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0026",
              "question": "MySQL 与MongoDB 之间最基本的差别是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0027",
              "question": "MySQL 中控制内存分配的全局参数，有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0028",
              "question": "MySQL 当记录不存在时insert,当记录存在时update，语句怎么写？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0029",
              "question": "NoSQL 数据库有哪些类型？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0030",
              "question": "Oracle 19c 中数据库的默认字符集是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0031",
              "question": "Oracle 19c 中自动内存管理是如何工作的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0032",
              "question": "Oracle 19c 的内存结构有哪些改进？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0033",
              "question": "Oracle 系统进程主要有哪些，作用是个啥？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0034",
              "question": "SQL 注入是如何攻击的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0035",
              "question": "SQL 注入防护方法？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0036",
              "question": "与Oracle 相比，Mysql 有什么优势？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0037",
              "question": "主要是通过什么方法来判断某些SQL 语句需要进行优化的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0038",
              "question": "什么是 SQL 注入攻击？如何防止 SQL 注入攻击？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0039",
              "question": "什么是 SQL 注入攻击？如何防范？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0040",
              "question": "什么是通用SQL 函数？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0041",
              "question": "如何在Oracle 19c 中创建物化视图？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0042",
              "question": "如何在Oracle 19c 中配置分布式数据库？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0043",
              "question": "如何防范 SQL 注入攻击？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0044",
              "question": "怎么优化复杂的SQL 查询？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0045",
              "question": "渗透测试中的 SQL 注入攻击是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0046",
              "question": "聊聊SQL 注入？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            }
          ]
        },
        {
          "subcategory": "DB-索引与性能",
          "count": 20,
          "questions": [
            {
              "id": "db_0047",
              "question": "1 如何 构建 数据索引？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0048",
              "question": "4.2 什么是倒排索引呢？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0049",
              "question": "Hash 索引和B+树索引的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0050",
              "question": "MongoDB 在A:{B,C}上建立索引，查询A:{B,C}和A:{C,B}都会使用索引吗？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0051",
              "question": "为什么不都用Hash 索引而使用B+树索引？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0052",
              "question": "为什么说B+比B 树更适合实际应用中操作系统的文件索引和数据库索引？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0053",
              "question": "什么是索引分裂？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0054",
              "question": "什么是索引？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0055",
              "question": "什么样的字段适合建索引？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0056",
              "question": "以及在mysql 数据库中索引的工作机制是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0057",
              "question": "你怎么看到为表格定义的所有索引？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0058",
              "question": "可以使用多少列创建索引？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0059",
              "question": "在MongoDb 中什么是索引？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0060",
              "question": "如何在Oracle 19c 中启用自动索引？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0061",
              "question": "如何添加索引？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0062",
              "question": "索引的作用？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0063",
              "question": "聚集索引和非聚集索引区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0064",
              "question": "请简述常用的索引有哪些种类？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0065",
              "question": "请解释一下索引的工作原理？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "db_0066",
              "question": "除了使用索引以外，还有什么方法可以加快查询速度？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            }
          ]
        },
        {
          "subcategory": "General AI-General",
          "count": 3,
          "questions": [
            {
              "id": "db_0067",
              "question": "2 Faiss 的索引Index有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "db_0068",
              "question": "3 Faiss 的索引Index都怎么用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "db_0069",
              "question": "为什么要创建索引？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            }
          ]
        }
      ]
    },
    {
      "category": "Deep Learning",
      "count": 330,
      "subcategories": [
        {
          "subcategory": "DL-CNN",
          "count": 16,
          "questions": [
            {
              "id": "dl_0001",
              "question": "20 Max pooling如何工作？还有其他池化技术吗？‍？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0002",
              "question": "9卷积核是否一定越大越好？它的优点有哪些？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0003",
              "question": "CNN中的卷积到底指什么？举个例子？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "dl_0004",
              "question": "CNN是否抗旋转？如果旋转图像，CNN的预测会怎样？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0005",
              "question": "RNN可以用于描述时间上连续状态的输出，有记忆功能，CNN用于静态输出。？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0006",
              "question": "TextCNN 的结构可以说一下吗？卷积核的层数应该怎么取？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "dl_0007",
              "question": "为什么需要卷积？不能使用全连接层吗？‍？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0008",
              "question": "五、TextCNN可以调整哪些参数？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "dl_0009",
              "question": "什么是2 CNN-LSTMs？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0010",
              "question": "什么是2 Keras搭建CNN？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0011",
              "question": "什么是5 CNN VS RNN？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0012",
              "question": "卷积神经网络的优点？为什么用小卷积核？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "dl_0013",
              "question": "四、TextCNN进行文本分类的过程？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "dl_0014",
              "question": "在卷积神经网络中，卷积操作的数学表达式是什么？请解释卷积核、步长和填充在其中的作用？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "dl_0015",
              "question": "当网络里出现转置卷积 stride=2 做上采样时，如何防止棋盘效应？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "dl_0016",
              "question": "说说RCNN、fase-RCNN 和faster-RCNN 的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            }
          ]
        },
        {
          "subcategory": "DL-RNN系列",
          "count": 16,
          "questions": [
            {
              "id": "dl_0017",
              "question": "1 LSTM结构推导，为什么比RNN好？推导forget gate，input gate，cell state， hidden information等的变化；因为LSTM有进有出且当前的cell informaton是通过input gate控制之后叠加的，RNN是叠乘，因此LSTM可以防止梯度消失或者爆炸。？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0018",
              "question": "7 LSTM的原理、写LSTM的公式、手推LSTM的梯度反向传播？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0019",
              "question": "LSTM 与GRU 关系是怎样的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "dl_0020",
              "question": "LSTM 与GRU 区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "dl_0021",
              "question": "LSTM 和GRU 的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "dl_0022",
              "question": "LSTM和RNN有什么区别？解决什么问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "dl_0023",
              "question": "RNN 原理？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "dl_0024",
              "question": "RNN 和 传统神经网络 最大的区别？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0025",
              "question": "elmo 为什么用的是两层单向的LSTM 而不是bi-LSTM？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "dl_0026",
              "question": "什么是1 Bidirectional RNNs？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0027",
              "question": "什么是3 Bidirectional LSTMs？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0028",
              "question": "什么是4 RNN扩展改进？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0029",
              "question": "什么是6 Keras搭建RNN？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0030",
              "question": "循环神经网络RNN 怎么解决长期依赖问题？LSTM 的结构是怎样的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "dl_0031",
              "question": "讲讲LSTM 和实习项目中做的改进？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "dl_0032",
              "question": "请解释LSTM中遗忘门的计算公式以及其作用？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            }
          ]
        },
        {
          "subcategory": "DL-基础",
          "count": 29,
          "questions": [
            {
              "id": "dl_0033",
              "question": "1 什么是反向传播？‍？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0034",
              "question": "1 什么是微调（fine-tune）？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0035",
              "question": "12一阶优化和二阶优化的方法有哪些？为什么不使用二阶优化？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0036",
              "question": "25神经网络数据预处理方法有哪些？中心化/零均值，归一化？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0037",
              "question": "DensetNet连接有正则化的作用，可以减少过拟合。？DenseNet直接连接不同层的特征图，而不是像ResNet一样element-wise sum。2.2.6.2为什么 DenseNet 比 ResNet 更耗显存？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0038",
              "question": "Ft和Ct-1做点积操作，Ft确保Ct-1有哪些东西需要被遗忘调？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0039",
              "question": "​ 作用对象：细胞状态 。？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0040",
              "question": "​ 作用对象：隐层ht 作用：确定输出什么值。？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0041",
              "question": "​ 作用：将新的信息选择性的记录到细胞状态中。？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0042",
              "question": "​ 作用：将细胞状态中的信息选择性的遗忘。？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0043",
              "question": "什么是1 LeNet？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0044",
              "question": "什么是1 用高斯分布乘上一个很小的数？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0045",
              "question": "什么是14 batch size和epoch的平衡？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0046",
              "question": "什么是2 AlexNet？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0047",
              "question": "什么是3 样本要做归一化.？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0048",
              "question": "什么是4 Inception(GoogLeNet)？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0049",
              "question": "什么是5 ResNet？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0050",
              "question": "什么是5 合理使用预训练网络？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0051",
              "question": "什么是6 DenseNet？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0052",
              "question": "什么是对象分割？你知道有哪些框架吗？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0053",
              "question": "什么是数据增强？为什么需要它们？你知道哪种增强？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0054",
              "question": "什么是目标检测？你知道有哪些框架吗？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0055",
              "question": "什么是迁移学习？它是如何工作的？‍？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0056",
              "question": "作用对象：细胞状态？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0057",
              "question": "反向传播就是：把画拿给最后一个人看（求取误差），然后最后一个人就会告诉前面的人下次描述时需要注意哪里（权值修正）？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0058",
              "question": "如何选择要使用的增强？‍？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0059",
              "question": "推导：链式求导法则反复用？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0060",
              "question": "文本生成 2.语音识别 3.机器翻译 4.生成图像描述 5.视频标记？它的缺点有哪些？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0061",
              "question": "牛顿法原理：使用函数f(x)的泰勒级数的前面几项来寻找方程f(x)= 0的根。？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            }
          ]
        },
        {
          "subcategory": "DL-损失函数",
          "count": 6,
          "questions": [
            {
              "id": "dl_0062",
              "question": "LR（逻辑回归）损失函数？怎么来的？为什么这么定义？里面取log 是为了什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "dl_0063",
              "question": "七、多分类的分类损失函数(Softmax)？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "dl_0064",
              "question": "五、分类问题为什么用交叉熵损失函数不用均方误差（MSE）？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "dl_0065",
              "question": "分类的分类损失函数？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "dl_0066",
              "question": "深度学习的损失函数有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "dl_0067",
              "question": "题2、深度学习中，常见的损失函数有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            }
          ]
        },
        {
          "subcategory": "DL-训练与优化",
          "count": 11,
          "questions": [
            {
              "id": "dl_0068",
              "question": "11什么是Adam？Adam和SGD之间的主要区别是什么？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0069",
              "question": "13什么时候使用Adam和SGD？‍？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0070",
              "question": "16学习率太大(太小)时会发生什么？如何设置学习率？‍？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0071",
              "question": "3 梯度消失和梯度爆炸的解决方案？梯度爆炸引发的问题？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0072",
              "question": "32什么是鞍点问题？梯度为0，海森矩阵不定的点，不是极值点。？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0073",
              "question": "6批量归一化(BN) 如何实现？作用？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0074",
              "question": "9 什么是Dropout？为什么有用？它是如何工作的？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0075",
              "question": "什么是梯度下降？SGD 的推导？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "dl_0076",
              "question": "使用验证集，可以知道什么时候开始降低学习率，和什么时候停止训练。？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0077",
              "question": "另一种解释：当反向传播进行很多层的时候，由于每一层都对前一层梯度乘以了一个小数，因此越往前传递，梯度就会越小，训练越慢。？",
              "variants": [],
              "sources": [
                "dl_questions_theory_v3.json"
              ]
            },
            {
              "id": "dl_0078",
              "question": "神经网络中的梯度消失和梯度膨胀是什么，怎么解决？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            }
          ]
        },
        {
          "subcategory": "Deep Learning-General",
          "count": 190,
          "questions": [
            {
              "id": "dl_0079",
              "question": "1 2 . 模 型 的 FLO P s（ 计 算 量 ） 指 的 是 什 么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0080",
              "question": "1 7 . 神 经 网 络 中 1×1 卷 积 有 什 么 作 用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0081",
              "question": "1 LSTM 结构推导，为什么比 RNN 好？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0082",
              "question": "1 RNNs 训练和传统 ANN 训练异同点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0083",
              "question": "1 什么是反向传播？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0084",
              "question": "1 什么是深度学习，它与传统机器学习有什么不同？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0085",
              "question": "1 各个激活函数的优缺点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0086",
              "question": "1 神经网络中包含哪些超参数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0087",
              "question": "10 你如何评估一个深度学习模型的性能？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0088",
              "question": "10 卷积在图像中有什么直观作用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0089",
              "question": "10 如何选择 dropout 的概率？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0090",
              "question": "11 CNN 中空洞卷积的作用是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0091",
              "question": "11 什么是 Adam？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0092",
              "question": "11 你能解释卷积神经网络和循环神经网络的区别吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0093",
              "question": "12 一阶优化和二阶优化的方法有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0094",
              "question": "12 为什么 Momentum 可以加速训练？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0095",
              "question": "12 怎样才能减少卷积层参数量？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0096",
              "question": "12 过拟合和欠拟合的区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0097",
              "question": "13 什么时候使用 Adam 和 SGD？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0098",
              "question": "13 在进行卷积操作时，必须同时考虑通道和区域吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0099",
              "question": "13 神经网络中 Dropout 的目的是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0100",
              "question": "14 批归一化是如何工作的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0101",
              "question": "14 采用宽卷积,窄卷积的好处有什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0102",
              "question": "15 SGD 每步做什么，为什么能 online learning？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0103",
              "question": "15 激活函数在神经网络中的作用是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0104",
              "question": "16 如何提高卷积神经网络的泛化能力？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0105",
              "question": "16 学习率太大(太小)时会发生什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0106",
              "question": "16 深度神经网络和浅层神经网络的区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0107",
              "question": "17 卷积神经网络在 NLP 与 CV 领域应用的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0108",
              "question": "17 梯度下降法和随机梯度下降法有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0109",
              "question": "17 神经网络为什么不用拟牛顿法而是用梯度下降？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0110",
              "question": "18 BN 和 Dropout 在训练和测试时的差别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0111",
              "question": "18 什么是递归神经网络？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0112",
              "question": "18 全连接、局部连接、全卷积与局部卷积的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0113",
              "question": "19 卷积层和全连接层的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0114",
              "question": "19 深度学习中验证集和测试集的区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0115",
              "question": "19 若网络初始化为 0 的话有什么问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0116",
              "question": "2 5 . 为 什 么 神 经 网 络 种 常 用 relu 作 为 激 活 函 数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0117",
              "question": "2 7 . 什 么 是 正 则 化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0118",
              "question": "2 9 . dr opo ut 为 什 么 能 解 决 过 拟 合？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0119",
              "question": "2 为什么 LSTM 模型中既存在 sigmoid 又存在 tanh 两种激活函数，而不是选择统一一种 sigmoid 或者 tanh？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0120",
              "question": "2 为什么 RNN 训练的时候 Loss 波动很大？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0121",
              "question": "2 为什么 ReLU 常用于神经网络的激活函数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0122",
              "question": "2 为什么要进行超参数调优？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0123",
              "question": "2 反向传播是如何工作的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0124",
              "question": "2 微调有哪些不同方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0125",
              "question": "2 权重初始化如何影响深度学习模型的性能？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0126",
              "question": "2.1 AlexNet 对比 LeNet 的优势？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0127",
              "question": "20 Max pooling 如何工作？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0128",
              "question": "20 sigmoid 和 softmax 的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0129",
              "question": "20 前馈神经网络和循环神经网络的区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0130",
              "question": "21 你能解释一下深度学习中权重初始化的概念吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0131",
              "question": "21 卷积神经网络的优点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0132",
              "question": "21 改进的 softmax 损失函数有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0133",
              "question": "22 CNN 拆成 3x1 1x3 的优点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0134",
              "question": "22 深度学习调参有哪些技巧？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0135",
              "question": "22softmax 函数和 sigmoid 函数的区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0136",
              "question": "23 BN、LN、IN、GN 和 SN 的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0137",
              "question": "23 你能解释一下深度学习中集成学习的概念吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0138",
              "question": "23 神经网络调参，要往哪些方向想？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0139",
              "question": "24 你能解释一下深度学习中权重衰减的概念吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0140",
              "question": "24 深度学习训练中是否有必要使用 L1 获得稀疏解？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0141",
              "question": "25 深度学习中 L1 和 L2 正则化的区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0142",
              "question": "25 神经网络数据预处理方法有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0143",
              "question": "26 如何初始化神经网络的权重？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0144",
              "question": "26 学习率在深度学习中的作用是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0145",
              "question": "27 为什么构建深度学习模型需要使用 GPU？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0146",
              "question": "27 你能解释一下深度学习中早停法的概念吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0147",
              "question": "28 你能解释一下深度学习中数据增强的概念吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0148",
              "question": "28 前馈神经网络(FNN),递归神经网络(RNN)和 CNN 区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0149",
              "question": "29 批归一化和层归一化有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0150",
              "question": "29 神经网络可以解决哪些问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0151",
              "question": "3 1 . P yT orch 和 TensorFl ow 的 特 点 分 别 是 什 么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0152",
              "question": "3 4 . R eLU 函 数 在 0 处 不 可 导 ， 为 什 么 还 能 用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0153",
              "question": "3 6 . 为 什 么 m ax p o ol ing 要 更 常 用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0154",
              "question": "3 LSTM 中为什么经常是两层双向 LSTM？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0155",
              "question": "3 RNN 中为什么会出现梯度消失？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0156",
              "question": "3 为什么需要反向传播？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0157",
              "question": "3 你能解释感知器和 S 形神经元的区别吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0158",
              "question": "3 卷积层有哪些基本参数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0159",
              "question": "3 微调先冻结底层，训练顶层的原因？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0160",
              "question": "3 梯度消失和梯度爆炸的解决方案？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0161",
              "question": "3.1 VGG 使用 2 个 3*3 卷积的优势在哪里？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0162",
              "question": "3.2 每层卷积是否只能用一种尺寸的卷积核？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0163",
              "question": "30 你能解释一下长短期记忆(LSTM)网络的概念吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0164",
              "question": "30 如何提高小型网络的精度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0165",
              "question": "31 深度学习中生成模型和判别模型的区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0166",
              "question": "32 什么是鞍点问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0167",
              "question": "32 你能解释一下生成对抗网络(GANs)的概念吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0168",
              "question": "33 深度信念网络和深度神经网络的区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0169",
              "question": "33 网络设计中，为什么卷积核设计尺寸都是奇数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0170",
              "question": "34 你能解释一下深度学习中强化学习的概念吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0171",
              "question": "35 你能解释一下深度学习中注意机制的概念吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0172",
              "question": "4 1 . S oft m ax+Cr o ss E nt rop y 如 何 反 向 求 导？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0173",
              "question": "4 3 . 为 什 么 在 模 型 训 练 开 始 会 有 warm u p？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0174",
              "question": "4 4 . V GG 使 用 3*3 卷 积 核 的 优 势 是 什 么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0175",
              "question": "4 7 . R elu 比 Si gm o id 的 效 果 好 在 哪 里？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0176",
              "question": "4 不同的数据集特性下如何微调？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0177",
              "question": "4 你能解释反向传播在神经网络中是如何工作的吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0178",
              "question": "4 如何确定是否出现梯度爆炸？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0179",
              "question": "4 如何解决 RNN 中的梯度消失问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0180",
              "question": "4 如何计算卷积层的输出的大小？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0181",
              "question": "4 极端批样本数量下，如何训练网络？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0182",
              "question": "4.1 inception 结构能不能缓解梯度消失？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0183",
              "question": "5 3 . D NN 的 梯 度 是 如 何 更 新 的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0184",
              "question": "5 LSTM、RNN、GRU 区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0185",
              "question": "5 如何计算卷积层参数数量？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0186",
              "question": "5 目标检测中使用预训练模型的优劣？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0187",
              "question": "5 神经网络中常用的激活函数有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0188",
              "question": "5 神经网络中有哪些正则化技术？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0189",
              "question": "5.1 ResNet 为什么不用 Dropout？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0190",
              "question": "5.2 ResNet 网络越来越深，准确率会不会提升？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0191",
              "question": "5.3 ResNet v1 与 ResNet v2 的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0192",
              "question": "6 LSTM 是如何实现长短期记忆功能的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0193",
              "question": "6 如何防止神经网络的过拟合？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0194",
              "question": "6 批量归一化(BN) 如何实现？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0195",
              "question": "6 有哪些池化方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0196",
              "question": "6 目标检测中如何从零开始训练(train from scratch)？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0197",
              "question": "6 自动化超参数搜索方法有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0198",
              "question": "6.1 DenseNet 比 ResNet 好？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0199",
              "question": "7 1*1 卷积的作用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0200",
              "question": "7 如何决定神经网络的层数和神经元数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0201",
              "question": "7 神经网络中权值共享的理解？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0202",
              "question": "8 卷积层和池化层有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0203",
              "question": "8 如何处理神经网络中缺失的数据？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0204",
              "question": "8 对 fine-tuning(微调模型)的理解？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0205",
              "question": "9 什么是 Dropout？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0206",
              "question": "9 你能解释一下深度学习中迁移学习的概念吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0207",
              "question": "9 卷积核是否一定越大越好？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0208",
              "question": "Adam 和 SGD 之间的主要区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0209",
              "question": "CNN 是否抗旋转？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0210",
              "question": "DenseNet直接连接不同层的特征图，而不是像ResNet一样element-wise sum。2.2.6.2为什么 DenseNet 比 ResNet 更耗显存？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0211",
              "question": "Dropout 在训练和测试的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0212",
              "question": "a ver ag e p o oling 比 m ax p oo l ing 更 合 适？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0213",
              "question": "softmax 的公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0214",
              "question": "‍神经网络怎样进行参数初始化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0215",
              "question": "「 参 考 资 料 」 ： CNN 模 型 所 需 的 计 算 力 flops 是 什 么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0216",
              "question": "「 参 考 资 料 」 ： N， LN， IN， GN 都 是 什 么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0217",
              "question": "「 参 考 资 料 」 ： l1 正 则 与 l2 正 则 的 特 点 是 什 么 ， 各 有 什 么 优 势？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0218",
              "question": "「 参 考 资 料 」 ： 深 度 学 习 中 的 batch 的 大 小 对 学 习 效 果 有 何 影 响？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0219",
              "question": "「参考资料」：深度卷积网络中如何进行上采样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0220",
              "question": "一 化 RNN（ 2015） 有 什 么 区 别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0221",
              "question": "下 采 样 的 作 用 是 什 么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0222",
              "question": "不能使用全连接层吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0223",
              "question": "个 模 型 无 效？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0224",
              "question": "个网络能正常训练嘛？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0225",
              "question": "为 什 么 要 反 向 传 播？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0226",
              "question": "为 什 么 需 要 做 特 征 归 一 化 、 标 准 化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0227",
              "question": "为什么 dropout 可以解决过拟合？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0228",
              "question": "为什么不使用二阶优化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0229",
              "question": "为什么有效；有什么理论解释么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0230",
              "question": "为什么有用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0231",
              "question": "为什么用小卷积核？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0232",
              "question": "为什么要修改最后几层神经网络权值？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0233",
              "question": "为什么降采用使用 max pooling，而分类使用 average pooling？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0234",
              "question": "为什么需要卷积？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0235",
              "question": "为什么需要它们？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0236",
              "question": "为什么需要激活功能？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0237",
              "question": "什么是对象分割？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0238",
              "question": "什么是数据增强？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0239",
              "question": "什么是目标检测？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0240",
              "question": "什么是迁移学习？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0241",
              "question": "你 用 过 哪 些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0242",
              "question": "你知道哪种增强？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0243",
              "question": "你知道有哪些框架吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0244",
              "question": "卷 积 层 和 全 连 接 层 的 区 别 是 什 么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0245",
              "question": "增 大 感 受 野 的 方 法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0246",
              "question": "如何设置学习率？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0247",
              "question": "如何选择要使用的增强？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0248",
              "question": "如果旋转图像，CNN 的预测会怎样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0249",
              "question": "它是如何工作的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0250",
              "question": "导 致 模 型 不 收 敛 的 原 因 有 哪 些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0251",
              "question": "常 用 常 用 的 归 一 化 和 标 准 化 的 方 法 有 哪 些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0252",
              "question": "常 见 的 损 失 函 数 有 哪 些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0253",
              "question": "有 什 么 数 据 增 强 的 方 式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0254",
              "question": "有 哪 些 经 典 的 卷 积 类 型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0255",
              "question": "梯 度 消 失 和 梯 度 爆 炸 的 原 因 是 什 么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0256",
              "question": "梯度爆炸引发的问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0257",
              "question": "模型的参数量指的是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0258",
              "question": "比 ResNet 更耗显存？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0259",
              "question": "深 度 学 习 为 什 么 在 计 算 机 视 觉 领 域 这 么 好？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0260",
              "question": "激 活 函 数 是 什 么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0261",
              "question": "神 经 网 络 的 优 缺 点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0262",
              "question": "神 经 网 络 的 正 则 化 方 法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0263",
              "question": "神 经 网 络 的 深 度 和 宽 度 分 别 指 的 是 什 么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0264",
              "question": "者 好 处 呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0265",
              "question": "过 拟 合 的 解 决 方 法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0266",
              "question": "还有其他池化技术吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0267",
              "question": "通 常 有 哪 些 方 式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0268",
              "question": "随 机 梯 度 下 降 相 比 全 局 梯 度 下 降 好 处 是 什 么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            }
          ]
        },
        {
          "subcategory": "General AI-General",
          "count": 62,
          "questions": [
            {
              "id": "dl_0269",
              "question": "1 CNN-CRF vs BiLSTM-CRF vs IDCNN-CRF？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0270",
              "question": "1 LSTM结构推导，为什么⽐RNN好？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0271",
              "question": "1 TextCNN进行文本分类的过程？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0272",
              "question": "1 【BERT】BERT的两个预训练任务对应的损失函数是什么(用公式形式展示)？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0273",
              "question": "1 归⼀化为什么能提⾼梯度下降法求解最优解的速度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0274",
              "question": "1 简要介绍TextRCNN相较于TextCNN的改进？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0275",
              "question": "1 ）是否找到合适的损失函数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0276",
              "question": "1.1 什么是 BiLSTM-CRF？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0277",
              "question": "1.2 为什么要用 BiLSTM？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0278",
              "question": "10 CNN究竟是怎样⼀步⼀步⼯作的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0279",
              "question": "2 GRU是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0280",
              "question": "2 TextCNN可以调整哪些参数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0281",
              "question": "2 简单介绍DPCNN模型相较于TextCNN的改进？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0282",
              "question": "2.1 什么是 Dilated CNN？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0283",
              "question": "2.2 为什么会有 Dilated CNN？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0284",
              "question": "2.3 Dilated CNN 的优点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0285",
              "question": "3 LSTM神经⽹络输⼊输出究竟是怎样的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0286",
              "question": "3 什么是 Lattice LSTM ，存在什么问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0287",
              "question": "3 什么是 WC-LSTM ，存在什么问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0288",
              "question": "3 使用CNN作为文本分类器时，不同通道channels对应着文本的什么信息？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0289",
              "question": "3 分类问题使用的损失函数还有有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0290",
              "question": "4 TextCNN中卷积核的长与宽代表了什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0291",
              "question": "4 循环神经网络RNN层 介绍？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0292",
              "question": "5 【BERT】 Bert 损失函数篇？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0293",
              "question": "5 在TextCNN中的pooling操作与一般CNN的pooling操作有何不同？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0294",
              "question": "5 如何修复梯度爆炸问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0295",
              "question": "6 LSTM相关的典型⾯试题：https://www.julyedu.com/search？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0296",
              "question": "6 TextCNN的局限性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0297",
              "question": "6 如何解决RNN梯度爆炸和弥散的问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0298",
              "question": "7 RNN是怎么从单层⽹络⼀步⼀步构造的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0299",
              "question": "7 基于(Bi-)LSTM的词性标注 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0300",
              "question": "BERT是怎么缓解梯度消失的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0301",
              "question": "CNN中参数量和计算量怎么算？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0302",
              "question": "CNN中的卷积到底指什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0303",
              "question": "Dropout的原理、在训练和测试时的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0304",
              "question": "GRU对LSTM做了哪些改动？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0305",
              "question": "LSTM和RNN有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0306",
              "question": "R-CNN与Fast R-CNN的区别有哪些呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0307",
              "question": "ReLU如何解决梯度消失问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0308",
              "question": "ReLU激活函数是如何解决梯度消失和梯度爆炸问题的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0309",
              "question": "• 本质上来讲，这就是⼀个⼆次函数最优化问题！但要是损失函数不是⼆次函数咋办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0310",
              "question": "与传统RNN和LSTM相比有哪些优势？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0311",
              "question": "二分类的分类损失函数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0312",
              "question": "什么单元更容易出现梯度消失梯度爆炸的问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0313",
              "question": "什么是梯度消失和梯度爆炸？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0314",
              "question": "介绍一下CNN？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0315",
              "question": "卷积神经网络中常见的层有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0316",
              "question": "多分类的分类损失函数(Softmax)？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0317",
              "question": "层⽹络的神经元进⾏归⼀化。既然BN是对单个神经元的运算，那么在CNN中卷积层上要怎么搞？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0318",
              "question": "我们是否可以采取一个轻量级的模型，比如TextCNN，去逼近BERT的效果呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0319",
              "question": "描述的⽅法，另外⼀种就是梯度下降法。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0320",
              "question": "更加⼀般的，损失函数不是⼆次函数咋办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0321",
              "question": "注意，为何gbdt可以⽤⽤负梯度近似残差呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0322",
              "question": "深度可分离卷积是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0323",
              "question": "深度可分离卷积的参数量和计算量是多少？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0324",
              "question": "类下第36题『CNN究竟是怎样⼀步⼀步⼯作的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0325",
              "question": "经常在机器学习中的优化问题中看到⼀个算法，即梯度下降法，那到底什么是梯度下降法呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0326",
              "question": "解释：所有导致有的同学认为LSTM+CRF中其实并没有实际意义的CRF。其实按刚才说的，Hi？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0327",
              "question": "说明：特征缩放其实并不需要太精确，其⽬的只是为了让梯度下降能够运⾏得更快⼀点，让梯度下降收？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0328",
              "question": "还记得4.2节开头对⽬标函数的说明吧（损失函数揭⽰训练误差 + 正则化定义复杂度）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0329",
              "question": "问题2、深度学习中，常见的损失函数有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "dl_0330",
              "question": "额，问题又来了，什么是梯度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            }
          ]
        }
      ]
    },
    {
      "category": "General Tech",
      "count": 1406,
      "subcategories": [
        {
          "subcategory": "General AI-General",
          "count": 744,
          "questions": [
            {
              "id": "gen_0001",
              "question": "(Tom,0.6)(Chase,0.2) (Jerry,0.2) 是如何得到的呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0002",
              "question": "1 CRF 的 优点在哪里？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0003",
              "question": "1 CRF模型 和 HMM 和 MEMM 模型 区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0004",
              "question": "1 Elmo 存在的问题是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0005",
              "question": "1 Elmo 的 特点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0006",
              "question": "1 FAQ 检索式问答系统 是 什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0007",
              "question": "1 Faiss 如何安装？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0008",
              "question": "1 HMM 和 MEMM 存在什么问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0009",
              "question": "1 HMM 存在 什么问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0010",
              "question": "1 NLG（自然语言生成）是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0011",
              "question": "1 QQ 匹配的优点有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0012",
              "question": "1 Q： 知识表示相对于one-hot表示的优势是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0013",
              "question": "1 Word2vec 中 霍夫曼树 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0014",
              "question": "1 Wordvec 指什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0015",
              "question": "1 fastText的分类过程？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0016",
              "question": "1 https://www.jianshu.com/p/7b6bb28c1753？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0017",
              "question": "1 neo4j模块：执行CQL ( cypher ) 语句是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0018",
              "question": "1 softmax函数是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0019",
              "question": "1 word-level Model 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0020",
              "question": "1 word2vec和NNLM对比有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0021",
              "question": "1 word2vec训练trick，window设置多大？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0022",
              "question": "1 【对比】多义词问题是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0023",
              "question": "1 【演变史】one-hot 存在问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0024",
              "question": "1 中文命名实体识别 与 英文命名实体识别的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0025",
              "question": "1 为什么会有 Elmo？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0026",
              "question": "1 为什么有 one-hot？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0027",
              "question": "1 为什么要用 多轮对话系统？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0028",
              "question": "1 为什么需要 P-tuning v2？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0029",
              "question": "1 为什么需要 P-tuning？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0030",
              "question": "1 为什么需要 前缀微调（Prefix-tining）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0031",
              "question": "1 为什么需要 指示微调（Prompt-tuning）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0032",
              "question": "1 事件抽取中常见的英文数据集有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0033",
              "question": "1 事件抽取和命名实体识别（即实体抽取）有什么异同？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0034",
              "question": "1 二分类问题使用的激活函数sigmoid简介？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0035",
              "question": "1 什么是 Adaptive Embedding 范式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0036",
              "question": "1 什么是 CRF？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0037",
              "question": "1 什么是 DST（对话状态跟踪）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0038",
              "question": "1 什么是 Dynamic Architecture？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0039",
              "question": "1 什么是 Faiss？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0040",
              "question": "1 什么是 TF-IDF？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0041",
              "question": "1 什么是 词汇/实体类型信息增强？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0042",
              "question": "1 什么是 词汇增强？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0043",
              "question": "1 什么是 语言理解（SLU）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0044",
              "question": "1 什么是 马尔可夫过程？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0045",
              "question": "1 什么是n元语法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0046",
              "question": "1 什么是事件？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0047",
              "question": "1 什么是任务型对话系统？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0048",
              "question": "1 什么是低秩因式分解？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0049",
              "question": "1 什么是剪枝？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0050",
              "question": "1 什么是图（Graph）呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0051",
              "question": "1 什么是实体嵌套？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0052",
              "question": "1 什么是归⼀化，它与标准化的区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0053",
              "question": "1 什么是文本摘要？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0054",
              "question": "1 什么是概率图模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0055",
              "question": "1 什么是类型识别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0056",
              "question": "1 什么是蒸馏？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0057",
              "question": "1 什么是角色识别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0058",
              "question": "1 什么是触发词检测？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0059",
              "question": "1 什么是论元检测？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0060",
              "question": "1 什么是量化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0061",
              "question": "1 介绍一下主题建模任务？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0062",
              "question": "1 传统的相似度算法所存在的问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0063",
              "question": "1 你了解哪些预训练模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0064",
              "question": "1 分类任务有哪些类别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0065",
              "question": "1 刷分的奇技淫巧？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0066",
              "question": "1 压缩式摘要是怎么做的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0067",
              "question": "1 句子重要性评估算法有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0068",
              "question": "1 命名实体识别 评价指标 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0069",
              "question": "1 在C++ 程序中调⽤被C 编译器编译后的函数，为什么要加extern“C”？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0070",
              "question": "1 基于规则的命名实体识别方法是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0071",
              "question": "1 如何发现 FAQ 中标准问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0072",
              "question": "1 如何计算两段文本之间的距离？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0073",
              "question": "1 对话系统有哪几种？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0074",
              "question": "1 常用 方案有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0075",
              "question": "1 抽取式摘要是怎么做的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0076",
              "question": "1 摘要质量的评估方法有哪些类型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0077",
              "question": "1 最大熵马尔科夫模型（MEMM） 是什么样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0078",
              "question": "1 标签解码器是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0079",
              "question": "1 模式匹配方法怎么用在事件抽取中？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0080",
              "question": "1 生成式摘要是怎么做的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0081",
              "question": "1 知识图谱的数据来源于哪里？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0082",
              "question": "1 问答系统的动机？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0083",
              "question": "1 隐马尔科夫算法 序列概率计算过程 是什么样的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0084",
              "question": "1 隐马尔科夫算法 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0085",
              "question": "10 ⽹卡或者硬盘有问题时，我们可以通过使⽤哪个命令查看相关信息？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0086",
              "question": "10亿个参数的模型，部署后占用多大显存？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0087",
              "question": "11 什么是⾮极⼤值抑制（NMS）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0088",
              "question": "11 如何进⾏特征选择？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0089",
              "question": "12 什么是lambda函数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0090",
              "question": "12 什么是深度学习中的anchor？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0091",
              "question": "12 机器学习和统计⾥⾯的auc的物理意义是啥？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0092",
              "question": "12什么是TF-IDF算法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0093",
              "question": "13 常见的分类算法有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0094",
              "question": "13 当神经⽹络的调参效果不好时，从哪些⾓度思考？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0095",
              "question": "15 Python⾥⾯如何拷贝⼀个对象？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0096",
              "question": "18 如何降低数据集的维度以减少模型计算时间？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0097",
              "question": "18 怎么解决推荐系统中的冷启动问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0098",
              "question": "19 如何提升已经达到96%精度的分类模型性能？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0099",
              "question": "1、什么是最⼩⼆乘法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0100",
              "question": "2 9 .发明l i b s v m的台湾林智仁教授0 6年的机器学习讲义S V M：h t t p : / / w e n k u . b a i d u . c o m / l i n k？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0101",
              "question": "2 CRF 的 主要思想是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0102",
              "question": "2 CRF 的 缺点在哪里？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0103",
              "question": "2 DST（对话状态跟踪）的输入输出是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0104",
              "question": "2 Elmo 的 思想是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0105",
              "question": "2 FAQ 如何做拆分？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0106",
              "question": "2 Faiss 如何使用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0107",
              "question": "2 MLP+softmax层 介绍？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0108",
              "question": "2 NLG（自然语言生成）的输入输出是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0109",
              "question": "2 Neo4J 怎么下载？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0110",
              "question": "2 Neo4j 怎么创建节点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0111",
              "question": "2 P-tuning v2 思路是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0112",
              "question": "2 P-tuning 思路是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0113",
              "question": "2 QQ 匹配的语义空间是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0114",
              "question": "2 Q：有哪些文本表示模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0115",
              "question": "2 Sigmod的缺点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0116",
              "question": "2 TF-IDF 如何评估词的重要程度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0117",
              "question": "2 Word2vec 中 为什么要使用 霍夫曼树？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0118",
              "question": "2 Wordvec 中 CBOW 指什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0119",
              "question": "2 fastText 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0120",
              "question": "2 fastText的优点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0121",
              "question": "2 h t t p s : / / m p . w e i x i n . q q . c o m / s？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0122",
              "question": "2 n-gram算法的局限性是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0123",
              "question": "2 one-hot 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0124",
              "question": "2 py2neo模块是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0125",
              "question": "2 query 匹配标准 QA 的核心是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0126",
              "question": "2 softmax函数怎么求导？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0127",
              "question": "2 word-level Model 存在什么问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0128",
              "question": "2 word2vec和tf-idf 在相似度计算时的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0129",
              "question": "2 【MLM】Bert 预训练任务 Masked LM 怎么做？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0130",
              "question": "2 【对比】word2vec 为什么解决不了多义词问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0131",
              "question": "2 为什么 CRF模型 会比 HMM 被普遍使用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0132",
              "question": "2 为什么 DNN 后面要加 CRF？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0133",
              "question": "2 为什么 QQ 匹配比较常用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0134",
              "question": "2 事件抽取中常见的中文数据集有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0135",
              "question": "2 事件抽取和关系抽取有什么异同？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0136",
              "question": "2 什么是 Schema 呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0137",
              "question": "2 什么是 随机场？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0138",
              "question": "2 什么是jaccard距离？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0139",
              "question": "2 什么是事件抽取？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0140",
              "question": "2 什么是多线程，多线程与多任务有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0141",
              "question": "2 什么是知识图谱呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0142",
              "question": "2 什么是跨层参数共享？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0143",
              "question": "2 任务型对话系统的流程是怎么样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0144",
              "question": "2 使用 模型蒸馏 的论文？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0145",
              "question": "2 信息抽取的难点在哪里？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0146",
              "question": "2 前缀微调（Prefix-tining）思路是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0147",
              "question": "2 基于无监督学习的命名实体识别方法是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0148",
              "question": "2 基于深度学习的命名实体识别方法 的 结构是怎么样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0149",
              "question": "2 基于约束的摘要生成方法有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0150",
              "question": "2 常用方法有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0151",
              "question": "2 常见的多轮对话系统解决方案是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0152",
              "question": "2 抽取式摘要的可读性问题是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0153",
              "question": "2 指示微调（Prompt-tuning）思路是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0154",
              "question": "2 文本摘要技术有哪些类型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0155",
              "question": "2 最大熵马尔科夫模型（MEMM） 如何解决 HMM 问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0156",
              "question": "2 树形结构为什么不需要归⼀化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0157",
              "question": "2 检索的方法 的 训练阶段 如何做？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0158",
              "question": "2 生成式摘要存在哪些问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0159",
              "question": "2 简单介绍混淆矩阵和kappa？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0160",
              "question": "2 类型识别有哪些方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0161",
              "question": "2 统计机器学习方法怎么用在事件抽取中？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0162",
              "question": "2 角色识别有哪些方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0163",
              "question": "2 触发词检测有哪些方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0164",
              "question": "2 论元检测有哪些方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0165",
              "question": "2 语言理解（SLU）的输入输出是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0166",
              "question": "2 请问⼈⼯神经⽹络中为什么ReLU要好过于tanh和Sigmoid function？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0167",
              "question": "2 这几种对话系统的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0168",
              "question": "2 问答系统 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0169",
              "question": "2 隐马尔科夫算法 中 两个序列 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0170",
              "question": "2 隐马尔科夫算法 学习训练过程 是什么样的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0171",
              "question": "2 马尔可夫过程 的核心思想 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0172",
              "question": "20 如何在⼀个数据集上选择重要的变量？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0173",
              "question": "2013年在微博上关于极⼤似然估计的讨论：http://weibo.com/1580904460/zfUsAgCl2？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0174",
              "question": "21 什么是wide&deep模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0175",
              "question": "22 怎样将知识图谱引⼊推荐系统？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0176",
              "question": "23 阿⾥最新开源的X-Deep Learning为Online Learning提供了哪些解决⽅案？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0177",
              "question": "26 芝⿇信⽤分的主要计算维度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0178",
              "question": "27 为什么我们做评分卡的时候要⽤woe编码？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0179",
              "question": "28 深度学习的风控模型，从经验上看，样本量⼤概要多少条啊？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0180",
              "question": "2【演变史】wordvec 存在问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0181",
              "question": "2）Gradient呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0182",
              "question": "2）batch size是否合适？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0183",
              "question": "3 ACE测评中事件抽取涉及的几个基本术语及任务是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0184",
              "question": "3 CRF 的定义是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0185",
              "question": "3 Character-Level Model 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0186",
              "question": "3 DST（对话状态跟踪）存在问题和解决方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0187",
              "question": "3 Dice系数和Jaccard系数的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0188",
              "question": "3 GPT 处理的 有监督任务有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0189",
              "question": "3 HMM模型三个基本问题的联系？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0190",
              "question": "3 NLG（自然语言生成）的实现方式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0191",
              "question": "3 Neo4J 怎么安装？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0192",
              "question": "3 Neo4j 怎么创建关系？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0193",
              "question": "3 P-tuning v2 优点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0194",
              "question": "3 P-tuning 优点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0195",
              "question": "3 Pointer-generator network解决了什么问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0196",
              "question": "3 QQ 匹配一般处理流程是怎么样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0197",
              "question": "3 QQ 匹配的语料的稳定性是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0198",
              "question": "3 Q：word2vec与LDA模型之间的区别和联系？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0199",
              "question": "3 TF-IDF 的思想是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0200",
              "question": "3 TF-IDF算法是做什么的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0201",
              "question": "3 TextTeaser算法是怎么抽取摘要的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0202",
              "question": "3 Word2vec 中使用 霍夫曼树 的好处？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0203",
              "question": "3 Wordvec 中 Skip-gram 指什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0204",
              "question": "3 fastText 的结构是什么样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0205",
              "question": "3 one-hot 有什么特点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0206",
              "question": "3 事件抽取的评价指标是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0207",
              "question": "3 什么是事理图谱？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0208",
              "question": "3 什么样的数据集不适合⽤深度学习？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0209",
              "question": "3 分布式输入层 是什么，有哪些方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0210",
              "question": "3 前缀微调（Prefix-tining）的优点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0211",
              "question": "3 基于特征的监督学习的命名实体识别方法是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0212",
              "question": "3 怎么通俗易懂地解释EM算法并且举个例⼦？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0213",
              "question": "3 指示微调（Prompt-tuning）优点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0214",
              "question": "3 指针网路层 介绍？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0215",
              "question": "3 条件随机场CRF层 介绍？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0216",
              "question": "3 构建知识图谱所涉及的技术？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0217",
              "question": "3 检索的方法 的 预测阶段 如何做？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0218",
              "question": "3 深度学习方法怎么用在事件抽取中？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0219",
              "question": "3 知识图谱的类别有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0220",
              "question": "3 能不能简单介绍下词袋模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0221",
              "question": "3 词汇/实体类型信息增强 方法有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0222",
              "question": "3 词汇增强 方法有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0223",
              "question": "3 语言理解（SLU）所使用的技术是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0224",
              "question": "3 隐马尔科夫算法 中 三个矩阵 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0225",
              "question": "3 隐马尔科夫算法 序列标注（解码）过程 是什么样的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0226",
              "question": "3【NSP】Bert 预训练任务 Next Sentence Prediction 怎么做？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0227",
              "question": "3【演变史】fastText 存在问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0228",
              "question": "3图像边缘检测的原理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0229",
              "question": "3）是否选择了合适的激活函数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0230",
              "question": "3）这不是boosting吧？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0231",
              "question": "4 CBOW vs Skip-gram 哪一个好？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0232",
              "question": "4 CRF 复现？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0233",
              "question": "4 CRF 的 三个基本问题 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0234",
              "question": "4 Character-Level Model 优点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0235",
              "question": "4 DST（对话状态跟踪）实现方式是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0236",
              "question": "4 FAQ 标准库如何实时更新？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0237",
              "question": "4 Faiss 然后使用 GPU？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0238",
              "question": "4 Neo4j 怎么创建 出生地关系？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0239",
              "question": "4 P-tuning v2 缺点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0240",
              "question": "4 P-tuning 缺点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0241",
              "question": "4 QQ 匹配的业务回答与算法模型的解耦是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0242",
              "question": "4 S root 1 0 0 80 0 - 725 - 10:43？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0243",
              "question": "4 TF-IDF 的计算公式是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0244",
              "question": "4 TextRank算法是怎么抽取摘要的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0245",
              "question": "4 milter：如何感性地理解EM算法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0246",
              "question": "4 one-hot 存在哪些问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0247",
              "question": "4 tf-idf高意味着什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0248",
              "question": "4 【对比】为什么 elmo、GPT、Bert能够解决多义词问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0249",
              "question": "4 为什么 Word2vec 中会用到 负采样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0250",
              "question": "4 事件抽取怎么发展的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0251",
              "question": "4 什么是 FLAT ，存在什么问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0252",
              "question": "4 前缀微调（Prefix-tining）的缺点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0253",
              "question": "4 同样是编辑距离，莱文斯坦距离和汉明距离的区别在哪里？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0254",
              "question": "4 在⼀个严格单调递增的整数数组中找到a[x] == x的位置？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0255",
              "question": "4 指示微调（Prompt-tuning）缺点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0256",
              "question": "4 知识图谱的价值在哪呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0257",
              "question": "4 隐马尔科夫算法 中 两个假设 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0258",
              "question": "4【演变史】elmo 存在问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0259",
              "question": "4图像中的⾓点(Harris⾓点)是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0260",
              "question": "5 CRF 的 流程是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0261",
              "question": "5 Character-Level Model 存在问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0262",
              "question": "5 Cypher查询语言是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0263",
              "question": "5 Neo4j 怎么查询？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0264",
              "question": "5 QQ 匹配的新问题发现与去重是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0265",
              "question": "5 Q：简要介绍下TransE模型的思想及优点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0266",
              "question": "5 TF-IDF 怎么描述？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0267",
              "question": "5 Word2vec 中会用到 负采样 是什么样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0268",
              "question": "5 事件抽取存在什么问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0269",
              "question": "5 写一下计算编辑距离（莱温斯坦距离）的编程题吧？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0270",
              "question": "5 可以⽤for循环直接删除ArrayList的特定元素吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0271",
              "question": "5 指示微调（Prompt-tuning）与 Prefix-tuning 区别 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0272",
              "question": "5 隐马尔科夫算法 中 工作流程 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0273",
              "question": "5为什么说神经⽹络是端到端的⽹络？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0274",
              "question": "5）是否选择了合适的优化算法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0275",
              "question": "6 Character-Level Model 问题的解决方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0276",
              "question": "6 Neo4j 怎么删除和修改？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0277",
              "question": "6 QQ 匹配的上线运行速度是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0278",
              "question": "6 TF-IDF 的优点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0279",
              "question": "6 Word2vec 中 负采样 的采样方式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0280",
              "question": "6 fastText 词内的n-gram信息 的 训练过程？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0281",
              "question": "6 为什么xgboost要⽤泰勒展开，优势在哪⾥？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0282",
              "question": "6 初期标注数据不足问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0283",
              "question": "6 如何看当前Linux系统有⼏颗物理CPU和每颗CPU的核数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0284",
              "question": "6 指示微调（Prompt-tuning）与 fine-tuning 区别 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0285",
              "question": "6什么是感受野？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0286",
              "question": "7 TF-IDF 的缺点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0287",
              "question": "7 fastText 词内的n-gram信息 存在问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0288",
              "question": "7 使⽤top查看系统资源占⽤情况时，哪⼀列表⽰内存占⽤呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0289",
              "question": "7 谈谈判别式模型和⽣成式模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0290",
              "question": "7⼯业界中遇到上亿的图像检索任务,如何提⾼图像对⽐效率？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0291",
              "question": "8 TF-IDF 的应用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0292",
              "question": "8 如何查看当前系统都有哪些进程？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0293",
              "question": "8 词干提取和词形还原有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0294",
              "question": "9 one-stage和two-stage⽬标检测⽅法的区别和优缺点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0295",
              "question": "A: 时间是不是直接抽取就好了，其它属性该怎么办呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0296",
              "question": "Adam和动量优化的SGD效率上的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0297",
              "question": "AiohoyDwq？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0298",
              "question": "BN层在训练和推理过程中有什么不一样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0299",
              "question": "Bert最近很⽕，应该是最近最⽕爆的AI进展，⽹上的评价很⾼，那么Bert值得这么⾼的评价吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0300",
              "question": "Bert采⽤这种两阶段⽅式解决各种NLP任务效果如何？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0301",
              "question": "Boosting，迭代，即通过迭代多棵树来共同决策。这怎么实现呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0302",
              "question": "C(W_i ) 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0303",
              "question": "CF)，⾸先想⼀个简单的问题，如果你现在想看个电影，但你不知道具体看哪部，你会怎么做？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0304",
              "question": "ELMO的论⽂题⽬：“Deep contextualized word representation”更能体现其精髓，⽽精髓在哪⾥？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0305",
              "question": "ELMO经过这般操作，效果如何呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0306",
              "question": "Encoder和decoder是如何进行交互的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0307",
              "question": "GLM是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0308",
              "question": "GPT是单向的：然后体现？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0309",
              "question": "Jensen不等式，促进神奇发⽣的Jensen不等式到底是什么来历呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0310",
              "question": "Knn、KMeans之类则需要归⼀化呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0311",
              "question": "LN和BN的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0312",
              "question": "M步很显然，就是最⼤化那⼀步，E步又从何谈起呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0313",
              "question": "N O，M步中，到底如何求θ的极值呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0314",
              "question": "Normalization为什么效果好？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0315",
              "question": "OK，问题变得有意思了。现在我们的⽬标没变，还是估计PA和PB，需要怎么做呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0316",
              "question": "PEFT 存在问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0317",
              "question": "PEFT 有什么优点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0318",
              "question": "Peft 和 全量微调区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0319",
              "question": "Pre-norm和post-norm有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0320",
              "question": "Precision和Recall是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0321",
              "question": "Q: 为什么通过新闻可以预测网络故障呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0322",
              "question": "Q: 事件抽取一般有什么方法呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0323",
              "question": "Q: 触发词一般是预定义好的，还是需要做检测任务？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0324",
              "question": "Q:不同任务对事件的定义不同吧，能具体解释下这些字段吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0325",
              "question": "Q:事件抽取针对的是一段话还是一篇文章呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0326",
              "question": "Q:事件是要分类型的吧？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0327",
              "question": "Q:能简单介绍一些事件抽取的应用背景吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0328",
              "question": "ReLU之前常用的激活函数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0329",
              "question": "Regression的部分加在哪？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0330",
              "question": "SGD、Adam、动量优化的SGD？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0331",
              "question": "Self Attention有什么增益或者好处呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0332",
              "question": "Transfomer是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0333",
              "question": "Warning: bad syntax, perhaps a bogus '-'？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0334",
              "question": "Word2Vec是怎么⼯作的呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0335",
              "question": "YOLO的正负样本是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0336",
              "question": "blip2的架构，优势和之前多模态模型的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0337",
              "question": "c.blog.sina.com.cn/profile.php？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0338",
              "question": "eqneditor.php？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0339",
              "question": "extern“C”？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0340",
              "question": "function，说明其实他们的⽬标函数很像，那么问题是svm为什么这么popular呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0341",
              "question": "function？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0342",
              "question": "gpt源码past_key_value是干啥的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0343",
              "question": "http://citeseerx.ist.psu.edu/viewdoc/download？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0344",
              "question": "http://jacoxu.com/？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0345",
              "question": "http://www.codecogs.com/latex/eqneditor.php？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0346",
              "question": "https://baijiahao.baidu.com/s？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0347",
              "question": "information)？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0348",
              "question": "lora的矩阵怎么初始化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0349",
              "question": "m*n*3的图像输入进去，输出会有变化吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0350",
              "question": "makes for effective detection proposals？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0351",
              "question": "margin 为 =y(wTx+b)=yf(x)中的Y是只取1和-1 吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0352",
              "question": "maximization algorithm？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0353",
              "question": "mod=weibotime？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0354",
              "question": "query = \"睡前练瑜伽好吗睡觉之前练习40分钟的瑜伽好吗、能起到瘦身的作用吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0355",
              "question": "ques_id/2103 ，但如果你是第⼀次听到one-stage和two-stage，你会不会瞬间⼀脸懵逼，这是啥？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0356",
              "question": "root 1 0.0 0.0 2900 1428？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0357",
              "question": "slow⼀定是指向中间节点的。但是此时有⼀个问题，我们此时⽆法知道slow的前⼀个位置了。怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0358",
              "question": "softmax除了作为激活函数，在深度学习中还有哪些用途？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0359",
              "question": "svm的训练过程如何优化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0360",
              "question": "trick 4：特征提取器 如何选择？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0361",
              "question": "trick 5：专有名称 怎么 处理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0362",
              "question": "trick 6：标注数据 不足怎么处理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0363",
              "question": "word2vec的原理，怎么训练的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0364",
              "question": "www.google.com.hk/url？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0365",
              "question": "xgboost算法介绍？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0366",
              "question": "yolov5相比于之前增加的特性有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0367",
              "question": "y的唯⼀作⽤就是确保functional margin的⾮负性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0368",
              "question": "① 事件抽取的定义/概念是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0369",
              "question": "② 有哪些常用的评测数据集和评测标准？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0370",
              "question": "③ 国内外有哪些研究团队和学者，它们主要研究的目标是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0371",
              "question": "④ 事件抽取有哪些应用场景和实际的产品？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0372",
              "question": "⑤ 事件抽取的一般过程，有标注数据开展研究，如何扩展，没有数据怎么做？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0373",
              "question": "⑥ 深度学习在事件抽取上有哪些应用，与传统方法比有什么优势/劣势？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0374",
              "question": "⑧ 事件之间的关系如何表示，如何做事件之间的关系抽取，目前有哪些研究？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0375",
              "question": "⑨ 有哪些值得阅读的论文？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0376",
              "question": "⑩ 最新的前沿进展有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0377",
              "question": "⼀上公式，你可能就懵圈了。然后回想起我前沿开头所说的话：难道就没有⼀篇通俗易懂的么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0378",
              "question": "⼀个很⾃然的问题是：通过Self Attention到底学到了哪些规律或者抽取出了哪些特征呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0379",
              "question": "⼗有⼋九你没懂。因为你可能不懂什么是最⼤似然估计？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0380",
              "question": "⼯业界如何选取核函数，经验的⽅法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0381",
              "question": "⽀持向量机系列，pluskid：http://blog.pluskid.org/？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0382",
              "question": "⽐如往台球桌上扔⼀个球，这个球落会落在何处呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0383",
              "question": "⽤极⼤似然估计求解了。 于是乎，怎么把Z变成已知的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0384",
              "question": "⽺群围起来。但是篱笆应该建在哪⾥呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0385",
              "question": "⽽你回头看看， N N L M 是怎么训练的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0386",
              "question": "⾯紧跟的单词应该是哪个，你会怎么做？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0387",
              "question": "一、为什么 需要 适配器微调（Adapter-tuning）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0388",
              "question": "一、为什么需要 提示学习（Prompting）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0389",
              "question": "一、什么是文本挖掘？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0390",
              "question": "一个元素在一个有序数组的第一次出现位置？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0391",
              "question": "七、MAM Adapter 思路 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0392",
              "question": "三、 对比篇？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0393",
              "question": "三、 适配器微调（Adapter-tuning）特点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0394",
              "question": "三、如何利用 Python 操作 Neo4j 图数据库？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0395",
              "question": "三、提示学习（Prompting） 有什么优点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0396",
              "question": "三、知识图谱怎么存储？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0397",
              "question": "上⾯介绍的是ELMO的第⼀阶段：预训练阶段。那么预训练好⽹络结构后，如何给下游任务使⽤呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0398",
              "question": "上⾯说了这么⼀⼤堆，读者可能还是没明⽩核函数到底是个什么东西？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0399",
              "question": "上继续训练呢，还是跟base 模型合并后再套一层lora，或者从头开始训练一个lora？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0400",
              "question": "上述内容就是经典的Soft Attention模型的基本思想，那么怎么理解Attention模型的物理含义呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0401",
              "question": "上述式⼦如何进⼀步化简计算呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0402",
              "question": "不再是类别，是数值（预测值），那么怎么确定呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0403",
              "question": "不同任务，怎么改造才能靠近GPT的⽹络结构呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0404",
              "question": "不是“G”BDT么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0405",
              "question": "个⼀个学员依次上台站到你⾯前时，你会怎么区分谁是男谁是⼥呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0406",
              "question": "个⼈判断是值得。那为什么会有这么⾼的评价呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0407",
              "question": "个性嵌⼊：你是什么样的⼈？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0408",
              "question": "中的分⼼模型。为什么说它注意⼒不集中呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0409",
              "question": "为什么LDA的最大似然难求？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0410",
              "question": "为什么self-attention要除以根号N？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0411",
              "question": "为什么⼀定会收敛？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0412",
              "question": "为什么⽤⾓点作为特征点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0413",
              "question": "为什么⽬标检测问题更难？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0414",
              "question": "为什么⾮⽀持向量对应的 等于零呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0415",
              "question": "为什么不用MSE分类用交叉熵？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0416",
              "question": "为什么会发⽣这种技术奇遇记？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0417",
              "question": "为什么会这样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0418",
              "question": "为什么在实际的 kaggle ⽐赛中 gbdt 和 random forest 效果⾮常好？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0419",
              "question": "为什么我们做评分卡的时候要⽤woe编码，⽽不是⽤别的编码⽅式呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0420",
              "question": "为什么要初始化为全0？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0421",
              "question": "为什么要归⼀化呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0422",
              "question": "为什么要用n-gram？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0423",
              "question": "为什么输⼊数据需要归⼀化（Normalized Data）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0424",
              "question": "为什么需要 PEFT？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0425",
              "question": "举个例⼦，这周末北京有⼀草莓⾳乐节，那去不去呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0426",
              "question": "么去训练⽹络。为什么要讲Word2Vec呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0427",
              "question": "义知识。那么如果持续地学习各类任务，模型的效果能否进一步提升？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0428",
              "question": "之后的内积 的结果是相等的，那么区别在于什么地⽅呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0429",
              "question": "之，怎样的θ能让L(θ)最⼤？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0430",
              "question": "乍⼀看上去好像不太好搞。我觉得吧，其实有⼀种很直观的思路，怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0431",
              "question": "了解Linux的管道命令吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0432",
              "question": "了解对比学习嘛？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0433",
              "question": "了解数据挖掘的方法嘛？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0434",
              "question": "事理图谱怎么构建？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0435",
              "question": "二、什么是 提示学习（Prompting）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0436",
              "question": "二、怎么构建知识图谱呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0437",
              "question": "二、文本挖掘的作用是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0438",
              "question": "二、适配器微调（Adapter-tuning）思路？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0439",
              "question": "二分类模型的评估指标有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0440",
              "question": "二十、Lora的矩阵怎么初始化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0441",
              "question": "五、AdapterDrop 思路 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0442",
              "question": "交叉熵函数刻画的什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0443",
              "question": "交叉熵和KL散度有什么关系？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0444",
              "question": "什么是⼀元线性模型呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0445",
              "question": "什么是交叉验证。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0446",
              "question": "什么是信息增益呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0447",
              "question": "什么是信息增益？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0448",
              "question": "什么是完全二叉树？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0449",
              "question": "什么是查询向量、键向量和值向量向量？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0450",
              "question": "什么是规则？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0451",
              "question": "什么是语⾔模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0452",
              "question": "仅是因为woe可以把特征从⾮线性变成线性的吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0453",
              "question": "介绍一下 PEFT？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0454",
              "question": "从图中看到多少信息？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0455",
              "question": "从袋⼦中取得⽩球的概率是多少？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0456",
              "question": "他们各⾃的优缺点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0457",
              "question": "以使⽤Bert预训练好的模型参数呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0458",
              "question": "但搞了半天，虽然知道了什么是因⼦图，但因⼦图到底是⼲嘛的呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0459",
              "question": "但是得到的最优的状态转换路径是1->1->1->1，为什么呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0460",
              "question": "但是我们作为输出词填写什么呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0461",
              "question": "但究竟根据哪个指标划分更好呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0462",
              "question": "低该数据集的维度以减少模型计算时间。你的机器内存有限。你会怎么做？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0463",
              "question": "体识别方法的优点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0464",
              "question": "作为算法工程师，在项目初期，标注数据不足问题，永远是一个老大难问题，那么如何解决该问题呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0465",
              "question": "你了解的知识蒸馏模型有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0466",
              "question": "你会如何⽐较两个概率分布呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0467",
              "question": "你可以做些什么呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0468",
              "question": "你有了解其他模型去尝试解决长度限制的方案吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0469",
              "question": "你问了：什么是注意⼒机制？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0470",
              "question": "信息（猜测这样做也许更鲁棒？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0471",
              "question": "假定我们需要统计七⽉在线10万学员中男⽣⼥⽣的⾝⾼分布，怎么统计呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0472",
              "question": "假设预训练好了⽹络模型，后⾯下游任务怎么⽤？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0473",
              "question": "做最终结论？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0474",
              "question": "做过像MBTI那样的⼈格测试，或者五⼤⼈格特质测试？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0475",
              "question": "元素到⽗节点中（有没有看到红⿊树中左旋操作的影⼦？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0476",
              "question": "八、MAM Adapter 特点 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0477",
              "question": "六、AdapterDrop 特点 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0478",
              "question": "关于attention机制，三个矩阵KQ,KV,K..的作用是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0479",
              "question": "关于bert的后续改进工作，分别改进了哪些地方？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0480",
              "question": "其中的差异性真的重要吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0481",
              "question": "具体的三种类型的无监督训练任务是哪三种呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0482",
              "question": "凡是这种循环迭代的⽅式必定有停⽌条件，什么时候停⽌呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0483",
              "question": "出。神奇吧？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0484",
              "question": "分别求偏导，再令其等于0，求解出来不也⼀样吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0485",
              "question": "分类问题是否可以用MSE？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0486",
              "question": "分类问题的交叉熵是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0487",
              "question": "到底什么是EM算法呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0488",
              "question": "到底什么是sum-product算法呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0489",
              "question": "前汽车的雏形已经出现了，⼲嘛还要执着在换轮胎这个事情呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0490",
              "question": "前者2次乘法1次加法，后者1次乘法，1次加法。我们这⾥的计算是否能借鉴到分配率呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0491",
              "question": "区别在哪呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0492",
              "question": "十七、哪些因素会影响内存使用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0493",
              "question": "十三、Rank 如何选取？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0494",
              "question": "十四、alpha参数 如何选取？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0495",
              "question": "半精度是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0496",
              "question": "半精度的理论原理是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0497",
              "question": "压缩⾄0到1有何⽤处呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0498",
              "question": "原因 5：QQ 匹配的上线运行速度是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0499",
              "question": "去⾯试时，便被问到“ o n e - s t a g e和t w o - s t a g e⽬标检测⽅法的区别和优缺点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0500",
              "question": "反过来，既然⽂档已经产⽣，那么如何根据已经产⽣好的⽂档反推其主题呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0501",
              "question": "取候选框⽤到的算法“选择性搜索”到底怎么选出这些候选框的呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0502",
              "question": "可以介绍一下attention机制吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0503",
              "question": "可以组合贷吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0504",
              "question": "可以解释一下熵吗，它的公式怎么算的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0505",
              "question": "可怕的(如上⽂中19维乃⾄⽆穷维的例⼦)。那咋办呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0506",
              "question": "可能会出现什么问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0507",
              "question": "可能有⼈会问，为什么要输⼊输出都⼀样呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0508",
              "question": "各种评估指标？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0509",
              "question": "呜呼，透了！不过，这个转化过程中的关键泰勒⼆次展开到底是哪来的呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0510",
              "question": "和意义何在？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0511",
              "question": "和蛋⽣鸡的问题吗，如何破？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0512",
              "question": "哪些比赛/会议给出了定义？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0513",
              "question": "啊哈，啥原理呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0514",
              "question": "啥叫输⼊层、输出层、隐藏层呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0515",
              "question": "四、AdapterFusion 思路 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0516",
              "question": "四、fastText 存在问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0517",
              "question": "四、提示学习（Prompting）有哪些方法，能不能稍微介绍一下它们间？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0518",
              "question": "四、模型压缩存在问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0519",
              "question": "四、知识图谱可以做什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0520",
              "question": "在线编辑Latex 公式：http://www.codecogs.com/latex/eqneditor.php？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0521",
              "question": "在自然语言处理模型训练中，评价指标是怎样设定的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0522",
              "question": "场景：假设 你有 一个 标准的问题库，此时 有一个 新 query 进来，你会做什么操作？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0523",
              "question": "型，什么又是前向分步算法呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0524",
              "question": "基于attention有哪些代表性的改进方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0525",
              "question": "多任务学习各loss差异过大怎样处理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0526",
              "question": "多模态中常见的SOTA模型有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0527",
              "question": "多模态融合后，怎样知道最终结果受哪种模态影响更大？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0528",
              "question": "够，另外一个数据量很少，可以怎么做？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0529",
              "question": "好的评估指标（参考：交叉验证，链接https://www.youtube.com/watch？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0530",
              "question": "如何 让 Prompt Tuning 能够在不同参数规模的预训练模型、针对不同下游任务的结果上都？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0531",
              "question": "如何⽤0到100的范围来表⽰你是多么内向/外向（其中0是最内向的，100是最外向的）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0532",
              "question": "如何判断链表是否有环？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0533",
              "question": "如何对物品进⾏分类，分成⼏类？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0534",
              "question": "如何建立这棵用于判断的树形结构？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0535",
              "question": "如何引⼊先验的语⾔学知识其实⼀直是NLP尤其是深度学习场景下的NLP的主要⽬标之⼀，不过⼀直没？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0536",
              "question": "如何构建多模态模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0537",
              "question": "如何构造Span矩阵问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0538",
              "question": "如何根据auc值来计算真正的类别，换句话说，就是对auc的反向⼯程。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0539",
              "question": "如何理解交叉熵的物理意义？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0540",
              "question": "如何确⽴节点的w以及最⼩的loss function，⼤声告诉我怎么做？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0541",
              "question": "如何确定⽤户对哪些物品类别有兴趣，兴趣程度如何？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0542",
              "question": "如何衡量质量：以任务的完成情况来衡量对话质量？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0543",
              "question": "如何衡量质量：以用户的主观体验为主？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0544",
              "question": "如何解决0-1标签稀疏问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0545",
              "question": "如何解决NLP分类任务的11个关键问题：类别不平衡&低耗时计算&小样本&鲁棒性&测试检验&长？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0546",
              "question": "如何解决prompt泛化性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0547",
              "question": "如何调整θ以使得J(θ)取得最⼩值有很多⽅法，其中有最⼩⼆乘法(min square)，是⼀种完全是数学？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0548",
              "question": "如何轻松愉快地理解条件随机场（CRF）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0549",
              "question": "如果新估计出来的PA和PB和我们初始化的值差别很⼤，怎么办呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0550",
              "question": "如果来了⼀个新的房⼦/⾯积，假设在房屋销售价格的记录中没有的，我们怎么办呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0551",
              "question": "如果要你推测，这⼀发命中的⼦弹是谁打的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0552",
              "question": "如果进行采样，策略是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0553",
              "question": "它们各有什么优缺点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0554",
              "question": "它们的特点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0555",
              "question": "它们都有什么特征？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0556",
              "question": "它指的是street还是这个animal呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0557",
              "question": "它是如何改变深度学习领域的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0558",
              "question": "它有什么好处？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0559",
              "question": "定位的问题的解决思路有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0560",
              "question": "对于一个时间顺序的推荐数据，如何划分训练集和验证集，能不能随机？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0561",
              "question": "对于上图，在⼀个⼈已经呼吸困难（dyspnoea）的情况下，其抽烟（smoking）的概率是多少呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0562",
              "question": "对于非常大的分类类别，对于softmax有哪些优化方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0563",
              "question": "对比学习负样本是否重要？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0564",
              "question": "对知识蒸馏知道多少，有哪些改进用到了？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0565",
              "question": "将本问题扩展⼀下，下⾯的代码可能会出现什么问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0566",
              "question": "就是 的期望。为什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0567",
              "question": "就算你傻，你看见⼈家这么做，有样学样不就⾏了吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0568",
              "question": "应是what？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0569",
              "question": "开始，熟悉使⽤向量来表⽰事物。你是否知道你的个性可以仅被五个数字的列表（向量）表⽰？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0570",
              "question": "归⼀化后有什么好处呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0571",
              "question": "当你只知道这⼀条信息的时候，你觉得你有多了解这个⼈？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0572",
              "question": "当图像有很多物体怎么办的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0573",
              "question": "很简单不是？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0574",
              "question": "微调方法批处理大小模式GPU显存速度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0575",
              "question": "微调方法是啥？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0576",
              "question": "快速排序时间复杂度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0577",
              "question": "怎么判断“头发长短”或者“是否有喉结”是最好的划分⽅式，效果怎么量化呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0578",
              "question": "怎么处理类别不平衡？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0579",
              "question": "怎么找到最优的线性分类器？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0580",
              "question": "怎么计算得到呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0581",
              "question": "怎么计算的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0582",
              "question": "怎样通俗的理解泰勒级数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0583",
              "question": "总结出⼀个模糊的“youth”概念？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0584",
              "question": "恩，你可能要拍案⽽起了，惊呼，这不是跟上⽂介绍的gbdt乃异曲同⼯么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0585",
              "question": "我们前⾯不是提过Word2Vec吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0586",
              "question": "我们把 functional margin 定为 1 了吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0587",
              "question": "我们的主题是预训练，那么问题是Word Embedding这种做法能算是预训练吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0588",
              "question": "我们说过Bert效果特别好，那么到底是什么因素起作⽤呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0589",
              "question": "或者，如何怎么确保EM收敛？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0590",
              "question": "找到相似的⽤户和物品，通过什么途径找到呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0591",
              "question": "据某关键字，重复出现次数最多的前100条？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0592",
              "question": "接下来的问题是，如何确定这个超平⾯呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0593",
              "question": "接着，我们细究下上图的具体计算过程。即上图中的输出结果1具体是怎么计算得到的呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0594",
              "question": "推荐系统中，如何进行负采样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0595",
              "question": "推荐系统中，相比于余弦相似度，是否可以用欧几里得距离判断相似度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0596",
              "question": "提醒：有读者可能会问上述推导过程如何⽽来？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0597",
              "question": "支持向量是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0598",
              "question": "数的中间部分，这样就相当于我这⼀层⽹络所学习到的特征分布被你搞坏了，这可怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0599",
              "question": "数的向量那怎么处理呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0600",
              "question": "文本生成的几大预训练任务？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0601",
              "question": "既然需要通过不断的训练以让蓝线最终成为最优分类超平⾯，那么，到底需要训练多少次呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0602",
              "question": "是因为它有重⼤的理论或者模型创新吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0603",
              "question": "是怎么匹配的呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0604",
              "question": "是离群点，过分关注不是错上加错了吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0605",
              "question": "是线性不可分的，此时咱们该如何把这两类数据分开呢(下⽂将会有⼀个相应的三维空间图)？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0606",
              "question": "暴露年龄的歌词）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0607",
              "question": "更多请参考：http://sofasofa.io/forum_main_post.php？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0608",
              "question": "最后 哪个学生 成绩 会更好？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0609",
              "question": "最后加⼊某种结构，使得后⾯全连接层得到的输⼊变成固定的呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0610",
              "question": "有些会重复。哪些重复了呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0611",
              "question": "有什么⽤啊？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0612",
              "question": "有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0613",
              "question": "有哪些事件关系类型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0614",
              "question": "有哪些开源了代码的工作？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0615",
              "question": "有方法不用处理根号N的吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0616",
              "question": "标注样本少怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0617",
              "question": "树，是不是贪⼼策略？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0618",
              "question": "样多。这是不是意味着，作为关键词，它们的重要性是⼀样的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0619",
              "question": "样才能迅速get到新模型的创新点和适⽤场景，快速提⾼新论⽂速度，节约理解、复现模型的成本？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0620",
              "question": "样本不平衡问题怎么处理的，有什么方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0621",
              "question": "样本之前（或观察到X之前），有着怎样的分布呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0622",
              "question": "根据上图，第1点可能很容易理解，但第2、3点中所述的条件独⽴是啥意思呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0623",
              "question": "根据它的上⽂Context-Before和下⽂Context-after去预测单词。其实Bert怎么做的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0624",
              "question": "模型压缩和加速的方法有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0625",
              "question": "模型提速的方法有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0626",
              "question": "模型的方差和偏差是指什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0627",
              "question": "模型输出的分布比较稀疏，怎么处理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0628",
              "question": "欠拟合如何去解决，训练过程不收敛如何去解决？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0629",
              "question": "此外，什么时候⽤item-base，什么时候⽤user-base呢：http://weibo.com/1580904460/zhZ9AiIkZ？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0630",
              "question": "比较大的变化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0631",
              "question": "比较大，会提取更多的topic信息？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0632",
              "question": "法的效果如何呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0633",
              "question": "测2，⼀个预测2，那么倾向后⼀种，为什么呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0634",
              "question": "满意你的模型性能？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0635",
              "question": "点的性能提升，只是没有那么耀眼的成功⽽已。没听过？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0636",
              "question": "然⽽，相关的综述⽂章不少，但碎⽚罗列的居多，模型之间内在的联系和演化思路如何揭⽰？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0637",
              "question": "然图像领域预训练这么好⽤，那⼲嘛⾃然语⾔处理不做这个事情呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0638",
              "question": "特征进⾏分割后，我们选择所谓的增益Gain最⾼的那个特征，⽽Gain如何计算呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0639",
              "question": "特性中的c点么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0640",
              "question": "独⽴。意味着啥呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0641",
              "question": "现在的情况是只要知道树的结构，就能得到⼀个该结构下的最好分数，那如何确定树的结构呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0642",
              "question": "理解这个推导的关键在哪呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0643",
              "question": "生成式模型与判别式模型的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0644",
              "question": "用大的数据集训练一个general的model，还是根据垂直领域训练一个specific的model呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0645",
              "question": "疑惑：框要取多⼤？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0646",
              "question": "的“royalty”概念？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0647",
              "question": "的。为什么Word2Vec这么处理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0648",
              "question": "的带权路径最短，也符合我们的信息论，即我们希望越常用的词拥有更短的编码。如何编码呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0649",
              "question": "的方格就是不能看到的信息，白色的就是需要attention的信息。如何实现这种控制呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0650",
              "question": "的时候，怎么办呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0651",
              "question": "的最⼤值啊，⽽我们想得到式(2)的最⼤值，那怎么办呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0652",
              "question": "相当于问题转换成了寻找与样本的分布最接近的概率分布模型，如何寻找呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0653",
              "question": "看做classification， 有没有办法优化下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0654",
              "question": "看成分类问题有何不妥？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0655",
              "question": "看看“Man”和“Woman”彼此之间是如何⽐它们任⼀⼀个单词与“King”相⽐更相似的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0656",
              "question": "真是这样的么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0657",
              "question": "知识图谱的具体构建技术是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0658",
              "question": "知识蒸馏和无监督样本训练？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0659",
              "question": "知道了如何更新乘⼦，那么选取哪些乘⼦进⾏更新呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0660",
              "question": "确定分裂⽤的f e a t u r e，h o w？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0661",
              "question": "稳定性怎么样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0662",
              "question": "穷种，那咋办呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0663",
              "question": "简⾔之，当我们给定⼀个\"X\"的图案，计算机怎么识别这个图案就是“X”呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0664",
              "question": "类别不平衡是如何去处理的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0665",
              "question": "精准。那是不是可以结合region proposal的思想实现精准⼀些的定位？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0666",
              "question": "组成)。所以我们需要⼀种⽅法把这⼋个矩阵压缩成⼀个矩阵。那该怎么做？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0667",
              "question": "细想⼀下，刚才的⽅法是不是有问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0668",
              "question": "终分类器，那这个最终分类器的误差界到底是多少呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0669",
              "question": "统计学中的P值是什么含义，如何通俗地解释？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0670",
              "question": "而对于新用户，由于其特征非常的稀疏，使用基于深度学习（DL）的推荐系统效果会比较差，那有什么方法呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0671",
              "question": "能不能总结一下各种参数高效微调方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0672",
              "question": "能不能手写下attention？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0673",
              "question": "能不能找出⼀个更加⾼效的⽅法来求出这些候选框呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0674",
              "question": "自然语言处理中对低质量数据做数据清洗的方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0675",
              "question": "自然语言处理有哪些任务？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0676",
              "question": "自监督、半监督、无监督的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0677",
              "question": "要技术领域及当前发展热点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0678",
              "question": "要问哪个变体是最好的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0679",
              "question": "解决什么问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0680",
              "question": "解码组件最后会输出⼀个实数向量。我们如何把浮点数变成⼀个单词？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0681",
              "question": "解释: 最长连续递增序列是 [1,3,5], 长度为 3。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0682",
              "question": "解释⼀下上⾯这张图的数字。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0683",
              "question": "解释下第（4）步，得到 时，只是最⼤化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0684",
              "question": "解释，你甚⾄可以认为本节就是对上⽂整个1.2节的解释。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0685",
              "question": "解释：在微调期间从未看到[MASK]词块？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0686",
              "question": "计新的PA和PB，如果新的PA和PB和我们初始化的PA和PB⼀样，请问这说明了什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0687",
              "question": "论文地址：https://openreview.net/forum？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0688",
              "question": "评分卡建模全流程？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0689",
              "question": "识别⼀幅图⽚是包含有字母\"X\"还是字母\"O\"？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0690",
              "question": "词”。事实上，⼀开始可供选择的主题有3个：教育、经济、交通，那为何偏偏选取教育这个主题呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0691",
              "question": "说一下广度优先遍历和深度优先遍历？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0692",
              "question": "说句⼈话就是，⼀张16:9⽐例的图⽚你硬是要Resize成1:1的图⽚，你说图⽚失真不？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0693",
              "question": "说明了什么意思？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0694",
              "question": "说明：如图，Span{呼}{枢}=1，代表「呼吸中枢」是一个部位实体；Span{呼}{累}=2，代表？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0695",
              "question": "说明：本⽂最初写于2012年6⽉，⽽后不断反反复复修改&优化，修改次数达上百次，最后修改于？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0696",
              "question": "请专家对系统的自动摘要结果打分，但是专家之间差异性较大。解决方法之一是金字塔方法(pyramid？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0697",
              "question": "请务必注意，理解判别模型和定义特征两部分含义，这已经涉及到CRF的雏形了。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0698",
              "question": "负样本构造成本过高应该怎么解决？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0699",
              "question": "质是⼀样的。为什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0700",
              "question": "超参数\\alpha \\beta对训练的影响？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0701",
              "question": "路径规划算法有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0702",
              "question": "输⼊不是序列⽽输出为序列的情况怎么处理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0703",
              "question": "输⼊句⼦单词注意⼒分配概率分布值呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0704",
              "question": "输入：下面是一则什么新闻？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0705",
              "question": "输入：阅读理解：特朗普与拜登共同竞选下一任美国总统。根据上述信息回答问题：特朗普是哪国人？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0706",
              "question": "达数⽉的研究⽀持向量机阶段，直到今⽇。”--http://zhan.renren.com/profile/249335584？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0707",
              "question": "还有就是，如上节所提出的第⼆个问题，它会收敛吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0708",
              "question": "还记得1.2节开头所说的吧？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0709",
              "question": "还记得4.2节最后，我们得到的计算式⼦吧？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0710",
              "question": "这100个⼈（的⾝⾼）出现的概率最⼤啊，这个概率就是上⾯这个似然函数L(θ)，怎么做到的呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0711",
              "question": "这⽚在Word Embedding头上笼罩了好⼏年的乌云是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0712",
              "question": "这个“it”在这个句⼦是指什么呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0713",
              "question": "这个模式会是什么样⼦？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0714",
              "question": "这乍看上去好像是个查表操作，不像是预训练的做法是吧？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0715",
              "question": "这些⽅法看上去都成本太⾼或者太繁琐了，有没有简单优美的解决⽅案呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0716",
              "question": "这样⼀来问题就解决了吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0717",
              "question": "这样做的⽬的是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0718",
              "question": "进⾏最有效的推荐呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0719",
              "question": "那 DMP 的数据是哪里来的呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0720",
              "question": "那么Anchor⼀共有多少个？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0721",
              "question": "那么Bert是怎么做的呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0722",
              "question": "那么Bert本⾝在模型和⽅法⾓度有什么创新呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0723",
              "question": "那么哪⾥体现了Gradient呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0724",
              "question": "那么在每次迭代中，如何更新乘⼦呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0725",
              "question": "那么如何建⽴某个位置和其特征的对应关系呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0726",
              "question": "那么如何选择乘⼦ 和 呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0727",
              "question": "那么常用的方法有哪些呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0728",
              "question": "那么新的问题来了，为什么这种预训练的思路是可⾏的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0729",
              "question": "那么站在现在的时间节点看，G P T有什么值得改进的地⽅呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0730",
              "question": "那么站在现在这个时间节点看，ELMO有什么值得改进的缺点呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0731",
              "question": "那么这9个anchors是做什么的呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0732",
              "question": "那么问题来了，什么是真、伪阳性率呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0733",
              "question": "那什么是拉格朗⽇对偶性呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0734",
              "question": "那把这个任务看做分类问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0735",
              "question": "那有什么方法可以解决该问题么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0736",
              "question": "那问题来了？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0737",
              "question": "都不知道该怎么做？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0738",
              "question": "问题5、CV中数据增强的方法有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0739",
              "question": "问题：什么是多义词？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0740",
              "question": "除了归⼀化，我们还会经常提到标准化，那到底什么是标准化和归⼀化呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0741",
              "question": "隔。OK，还记得上⽂第1.3节开头的内容么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0742",
              "question": "（b）⾄于右侧公式和左侧的图是怎样的⼀个⼀⼀对应关系呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0743",
              "question": "（特此致歉），怎么办呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "gen_0744",
              "question": "），即得到上图中的（2）式，但（2）式还是有“和的对数”，还是求解不了，咋办呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            }
          ]
        },
        {
          "subcategory": "General Tech-General",
          "count": 662,
          "questions": [
            {
              "id": "gen_0745",
              "question": "->具体的emission score 与transition score 是如何得到的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0746",
              "question": "1 什么是马尔科夫过程？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0747",
              "question": "1 如何 判断 网络上是否有原题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0748",
              "question": "2 ALiBi (Attention with Linear Biases) 的偏置矩阵是什么？有什么作用？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0749",
              "question": "2 马尔科夫过程的核心思想是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0750",
              "question": "3 隐马尔可夫算法中的两个假设是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0751",
              "question": "5 HMM与CRF的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0752",
              "question": "6 隐马尔可夫算法存在哪些问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0753",
              "question": "A/B test 如何合理分流？",
              "variants": [],
              "sources": [
                "数据分析常考面试题101题_only_questions_theory.json"
              ]
            },
            {
              "id": "gen_0754",
              "question": "AI 目前已在以下哪些领域或方向得到应用？",
              "variants": [],
              "sources": [
                "recovered_tech_from_removed_v2.json"
              ]
            },
            {
              "id": "gen_0755",
              "question": "ALiBi (Attention with Linear Biases) 的偏置矩阵是什么？有什么作⽤？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0756",
              "question": "AOP 有哪些实现方式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0757",
              "question": "API 与微服务架构有何不同？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0758",
              "question": "Are you a multi-tasked individual？or Do you work well under stress or pressure？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0759",
              "question": "Could you please give me your contact information for me to follow up？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0760",
              "question": "D 并行 策略有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0761",
              "question": "DIM层做了哪些事？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0762",
              "question": "Describe a difficult problem you have had to deal with ？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0763",
              "question": "Django 模型如何映射到数据库？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0764",
              "question": "Dubbo 和 Spring Cloud 有什什么区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0765",
              "question": "Dubbo 有哪几种负载均衡策略，默认是哪种？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0766",
              "question": "Dubbo 能集成 Spring Boot 吗？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0767",
              "question": "Dubbo可以对结果进⾏行行缓存吗？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0768",
              "question": "Dubbo有哪⼏几种负载均衡策略略，默认是哪种？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0769",
              "question": "Efficient Router 介绍？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0770",
              "question": "EfficientNet 的原理？基于什么模型的搜索？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0771",
              "question": "Flask 中如何配置路由规则？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0772",
              "question": "Flink 任务的并行度优先级设置？资源一般如何配置？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0773",
              "question": "Flink 写入Clickhouse 怎么保证一致性？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0774",
              "question": "Flink 如何实现端到端一致性？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0775",
              "question": "GC 是什么？为什么要有GC？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0776",
              "question": "GC 触发的条件？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0777",
              "question": "HTTP 如何实现长连接？在什么时候会超时？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0778",
              "question": "HTTP 请求有哪些状态码？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0779",
              "question": "HTTPS 的工作流程是怎样的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0780",
              "question": "HTTP和HTTPS有什么区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0781",
              "question": "Hash Join 是不是有排序？Hash Join 会在什么时候慢？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0782",
              "question": "Have you done the best work you are capable of doing？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0783",
              "question": "How do you normally handle criticism？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0784",
              "question": "How would you go about establishing your credibility quickly within the team？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0785",
              "question": "Http 的请求报文组成？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0786",
              "question": "Https 流程是怎样的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0787",
              "question": "IEEE 在计算机网络中的作用是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0788",
              "question": "IP 地址有哪些分类？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0789",
              "question": "If you could start again, what career decisions would you make differently？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0790",
              "question": "JAVA反序列化了解吗？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0791",
              "question": "JDBC 访问数据库的基本步骤是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0792",
              "question": "JVM 中一次完整的GC 流程是怎样的，对象如何晋升到老年代？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0793",
              "question": "JVM 性能调优？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0794",
              "question": "JVM 调优命令？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0795",
              "question": "JVM 调优工具？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0796",
              "question": "Java 中++ 操作符是线程安全的吗？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0797",
              "question": "Java 中==和equals()的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0798",
              "question": "Java 中interrupted 和isInterruptedd 方法的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0799",
              "question": "Java 中notify 和notifyAll 有什么区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0800",
              "question": "Java 中什么是静态条件？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0801",
              "question": "Java 中堆和栈有什么区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0802",
              "question": "Java 中如何停止一个线程？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0803",
              "question": "Java 中怎么创建一个不可变对象？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0804",
              "question": "Java 中有几种类型的流？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0805",
              "question": "Java 中的引用类型有几种？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0806",
              "question": "Java 中的编译期常量是什么？使用它又什么风险？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0807",
              "question": "Java 中类的生命周期使什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0808",
              "question": "Java 内存模型？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0809",
              "question": "Java 创建对象的几种方式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0810",
              "question": "Java 创建线程之后，直接调用start()方法和run()的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0811",
              "question": "Java 对象创建过程？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0812",
              "question": "Java 支持多继承么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0813",
              "question": "Java 的内存划分？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0814",
              "question": "Java 的基础类型和字节大小？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0815",
              "question": "Java 的多态表现在哪里？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0816",
              "question": "Java 集合类：list、set、queue、map、stack 的特点与用法？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0817",
              "question": "Java（OOP）面向对象的特征有哪些方面？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0818",
              "question": "Jedis 与Redisson 对比有什么优缺点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0819",
              "question": "Kafka 如何压测？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0820",
              "question": "Kafka、ActiveMQ、RabbitMQ、RocketMQ 都有什么区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0821",
              "question": "Linux 中有哪几种设备？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0822",
              "question": "Linux 中的浮点运算由应用程序实现还是内核实现？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0823",
              "question": "Linux 中的用户模式和内核模式是什么含意？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0824",
              "question": "Linux 调度程序是根据进程的动态优先级还是静态优先级来调度进程的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0825",
              "question": "Linux 软中断和工作队列的作用是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0826",
              "question": "Linux 通过什么方式实现系统调用？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0827",
              "question": "MQ 的优缺点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0828",
              "question": "MVC 的各个部分都有哪些技术来实现？如何实现？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0829",
              "question": "Memcache 与Redis 的区别都有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0830",
              "question": "Minor GC，Full GC 触发条件？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0831",
              "question": "MongoDB 中的分片是什么意思？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0832",
              "question": "MongoDB 中的命名空间是什么意思？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0833",
              "question": "MongoDB 哪个命令可以切换数据库？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0834",
              "question": "MongoDB 支持主键外键关系吗？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0835",
              "question": "MongoDB 支持哪些数据类型？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0836",
              "question": "MongoDB 支持存储过程吗？如果支持的话，怎么用？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0837",
              "question": "MongoDB 是由哪种语言写的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0838",
              "question": "MongoDB 有哪些可替代产品？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0839",
              "question": "MongoDB 的优势有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0840",
              "question": "Mongodb 存储特性与内部原理？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0841",
              "question": "MyBatis 框架使用的场合？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0842",
              "question": "MyBatis 框架的缺点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0843",
              "question": "MyBatis 的优点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0844",
              "question": "Mybatis 中#{}和${}的区别是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0845",
              "question": "Mybatis 是如何sql 执行结果封装为目标对象？都有哪些映射形式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0846",
              "question": "Mysql 中使用什么存储引擎？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0847",
              "question": "Mysql 中有哪些不同的表格？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0848",
              "question": "Mysql 如何优化DISTINCT？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0849",
              "question": "Mysql 服务器默认端口是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0850",
              "question": "Mysql 查询是否区分大小写？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0851",
              "question": "Mysql 的技术特点是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0852",
              "question": "Mysql 表中允许有多少个TRIGGERS？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0853",
              "question": "Mysql 驱动程序是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0854",
              "question": "Nginx 优化的方式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0855",
              "question": "Nginx 如何处理一个请求的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0856",
              "question": "Nginx 的调度算法有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0857",
              "question": "Nginx 的进程模型？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0858",
              "question": "Nginx 负载均衡的4 种分配方式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0859",
              "question": "Nginx 负载均衡调度状态？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0860",
              "question": "PCA 为什么要中心化？PCA 的主成分是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0861",
              "question": "PagedAttention 如何 实现安全共享？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0862",
              "question": "PagedAttention 如何存储 连续的key和value？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0863",
              "question": "PagedAttention 技术细节？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0864",
              "question": "PagedAttention 源码介绍？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0865",
              "question": "Python is 和 == 区别，两者分别在比较什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0866",
              "question": "Python 中如何实现多线程？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0867",
              "question": "Python 中是否需要缩进？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0868",
              "question": "Python 中的字典是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0869",
              "question": "Python 中的标识符长度有多长？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0870",
              "question": "Python 中的闭包是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0871",
              "question": "Python 和Java 之间的主要区别是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0872",
              "question": "Python 如何实现函数式编程？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0873",
              "question": "Python 的优势有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0874",
              "question": "Python中range和xrange的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0875",
              "question": "Python中remove，del以及pop之间的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0876",
              "question": "Python中列表和元组的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0877",
              "question": "Python中的None代表什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0878",
              "question": "Python中的可变对象和不可变对象？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0879",
              "question": "Python中的实例方法、静态方法和类方法三者区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0880",
              "question": "Python是解释语言还是编译语言？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0881",
              "question": "Python里有多线程吗？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0882",
              "question": "RFS、LNSn、MRP、LSP 进程的作用分别是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0883",
              "question": "ReID 项目中，Triplet loss 的原理？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0884",
              "question": "Redis key 的过期时间和永久有效分别怎么设置？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0885",
              "question": "Redis 与其他key-value 存储有什么不同？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0886",
              "question": "Redis 中的管道有什么用？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0887",
              "question": "Redis 主要消耗什么物理资源？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0888",
              "question": "Redis 分区有什么缺点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0889",
              "question": "Redis 和Memcache 的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0890",
              "question": "Redis 和Redisson 有什么关系？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0891",
              "question": "Redis 回收使用的是什么算法？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0892",
              "question": "Redis 回收进程如何工作的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0893",
              "question": "Redis 如何做内存优化？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0894",
              "question": "Redis 如何做大量数据插入？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0895",
              "question": "Redis 如何设置密码及验证密码？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0896",
              "question": "Redis 存储的是k-v 类型，为什么还会有Hash？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0897",
              "question": "Redis 官方为什么不提供Windows 版本？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0898",
              "question": "Redis 常见性能问题和解决方案？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0899",
              "question": "Redis 常见的性能问题都有哪些？如何解决？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0900",
              "question": "Redis 持久化数据和缓存怎么做扩容？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0901",
              "question": "Redis 提供了哪几种持久化方式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0902",
              "question": "Redis 支持哪些数据结构？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0903",
              "question": "Redis 支持哪几种数据类型？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0904",
              "question": "Redis 支持的Java 客户端都有哪些？官方推荐用哪个？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0905",
              "question": "Redis 是单线程的，如何提高多核CPU 的利用率？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0906",
              "question": "Redis 最适合的场景？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0907",
              "question": "Redis 有哪些适合的场景？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0908",
              "question": "Redis 有哪几种数据淘汰策略？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0909",
              "question": "Redis 的全称是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0910",
              "question": "Redis 的内存占用情况怎么样？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0911",
              "question": "Redis 的内存用完了会发生什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0912",
              "question": "Redis 的同步机制了解么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0913",
              "question": "Redis 集群之间是如何复制的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0914",
              "question": "Redis 集群会有写操作丢失吗？为什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0915",
              "question": "Redis 集群如何选择数据库？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0916",
              "question": "Redis 集群方案什么情况下会导致整个集群不可用？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0917",
              "question": "Redis 集群方案应该怎么做？都有哪些方案？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0918",
              "question": "Redis 集群最大节点个数是多少？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0919",
              "question": "Redis 集群的主从复制模型是怎样的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0920",
              "question": "Redo 日志文件（Redo Log Files）的作用是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0921",
              "question": "SENet 的原理说一下，和Attention 机制的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0922",
              "question": "SENet 的原理，以及画出block 的图？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0923",
              "question": "Servlet Filter Listener 启动顺序？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0924",
              "question": "Spark的并行度指的是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0925",
              "question": "Spring AOP（面向切面）编程的原理？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0926",
              "question": "Spring Boot 中的监视器是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0927",
              "question": "Spring Boot 有哪些优点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0928",
              "question": "Spring MVC 框架有什么用？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0929",
              "question": "Spring bean 的生命周期？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0930",
              "question": "Spring 中@Autowire 与@Resource 的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0931",
              "question": "Spring 中beanFactory 和ApplicationContext 的联系和区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0932",
              "question": "Spring 中的事件处理？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0933",
              "question": "Spring 框架中都用到了哪些设计模式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0934",
              "question": "Spring 框架的事物管理有哪些优点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0935",
              "question": "Spring 的重要注解？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0936",
              "question": "Spring 运行原理？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0937",
              "question": "SpringIOC 是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0938",
              "question": "SpringIOC 注入的几种方式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0939",
              "question": "SpringMVC 和struts2 的区别有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0940",
              "question": "SpringMVC 工作流程？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0941",
              "question": "SpringMVC 的运行流程？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0942",
              "question": "TCP/IP 三次握手的过程以及对应的状态转换？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0943",
              "question": "TLB 中缓存的是什么内容？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0944",
              "question": "Tcp 和Udp 的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0945",
              "question": "UML 是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0946",
              "question": "Were you ever dismissed from your job for a reason that seemed unjustified？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0947",
              "question": "What are your outstanding qualities？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0948",
              "question": "What aspects of your job do you consider most crucial？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0949",
              "question": "What contribution did you make to your current previous organization？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0950",
              "question": "What did you like/dislike about your last job？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0951",
              "question": "What did you look for when you hired people in the past？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0952",
              "question": "What interests you most about this job？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0953",
              "question": "What leadership qualities did you develop as an administrative personnel？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0954",
              "question": "What makes this job different from your current/last one？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0955",
              "question": "What range of pay-scale are you interested in？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0956",
              "question": "What sort of people do you find it difficult to work with？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0957",
              "question": "What type of decisions did you make on your last job？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0958",
              "question": "What was the last book you read？ How did it effect you？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0959",
              "question": "Where did you think you'd be at this stage in your life？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0960",
              "question": "Why did you leave your last job？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0961",
              "question": "[[#Layer normalization-方法篇#Deep Norm 有什么优点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0962",
              "question": "attention 注意机制？还有简单介绍下self attention？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0963",
              "question": "baichuan 进行微调时，领域数据：通用数据配比？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0964",
              "question": "dubbo 服务负载均衡策略？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0965",
              "question": "fastText 存在问题？",
              "variants": [],
              "sources": [
                "recovered_tech_from_removed_v2.json"
              ]
            },
            {
              "id": "gen_0966",
              "question": "fastText的分类过程？fastText的优点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0967",
              "question": "forward 和redirect 的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0968",
              "question": "glove 和word2vec 对比有什么区别？",
              "variants": [],
              "sources": [
                "recovered_tech_from_removed_v2.json"
              ]
            },
            {
              "id": "gen_0969",
              "question": "hibernate 和mybatis 的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0970",
              "question": "hive？spark？sql？ nlp？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0971",
              "question": "http 中重定向和请求转发的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0972",
              "question": "http 协议的状态码有哪些？含义是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0973",
              "question": "https 的建立过程？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0974",
              "question": "ips和ids的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0975",
              "question": "jsp 和servlet 的区别、共同点、各自应用的范围？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0976",
              "question": "kmeans 算法为什么一定会收敛，讲一下EM 算法，kmeans 算法有什么缺点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0977",
              "question": "lora的矩阵怎么初始化？为什么要初始化为全0？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0978",
              "question": "lr 模型是线性模型还是非线性，为什么？能推导它的原理吗？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0979",
              "question": "mysql 中myisam 与innodb 的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0980",
              "question": "mysql 支持的复制类型？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0981",
              "question": "mysql 有关权限的表都有哪几个？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0982",
              "question": "mysql_fetch_array 和mysql_fetch_object 的区别是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0983",
              "question": "nginx 中多个work 进程是如何监听同一个端口的？如何处理客户连接的惊群问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0984",
              "question": "nginx 相对于apache 的优点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0985",
              "question": "nginx 程序的热更新是如何做的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0986",
              "question": "nn.DataParallel(以下简称DP) vs DistributedDataParallel(以下简称DDP)介绍⼀下？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0987",
              "question": "nn.parallel.DistributedDataParallel 参数更新介绍⼀下？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0988",
              "question": "nn.parallel.DistributedDataParallel 实现流程介绍⼀下？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0989",
              "question": "predicted intentions:['query_period'] 高血压要怎么治？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0990",
              "question": "python 常用的爬取数据的框架或者方法有那些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0991",
              "question": "python2和python3的range（100）的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0992",
              "question": "redis 单线程还能处理速度那么快？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0993",
              "question": "redis 如何实现高可用？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0994",
              "question": "redis 常见的性能问题和解决方案？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0995",
              "question": "redis 有哪些数据结构？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0996",
              "question": "redis 的主从复制的实现过程？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0997",
              "question": "redis 的哨兵机制的作用？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0998",
              "question": "redis 的淘汰策略有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_0999",
              "question": "redis 相比memcached 有哪些优势？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1000",
              "question": "redis 缓存穿透、缓存雪崩、缓存击穿？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1001",
              "question": "redis 集群如何保证一致性？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1002",
              "question": "request.getAttribute()和request.getParameter()有何区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1003",
              "question": "rnn、lstm 和gru 的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1004",
              "question": "scrapy 怎么做分布式爬虫？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1005",
              "question": "servlet 的生命周期？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1006",
              "question": "sigmoid 和softmax 的区别？softmax 的公式？",
              "variants": [],
              "sources": [
                "recovered_tech_from_removed_v2.json"
              ]
            },
            {
              "id": "gen_1007",
              "question": "softmax 和sigmod 区别？",
              "variants": [],
              "sources": [
                "recovered_tech_from_removed_v2.json"
              ]
            },
            {
              "id": "gen_1008",
              "question": "sql server提权的方法？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1009",
              "question": "sql server注入有了解吗，怎么判断是不是sql server？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1010",
              "question": "sql 注入写文件都有哪些函数？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1011",
              "question": "sqlmap的os--shell原理？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1012",
              "question": "sql注入getshell，有哪些方式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1013",
              "question": "sql注入报错注入常用函数，说一下函数报错，知道几个报错函数？常用的有哪几个？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1014",
              "question": "sql注入比较了解那个数据库？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1015",
              "question": "sql注入类型、sql注入的方式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1016",
              "question": "svm 不同核方法区别？",
              "variants": [],
              "sources": [
                "recovered_tech_from_removed_v2.json"
              ]
            },
            {
              "id": "gen_1017",
              "question": "svm 能介绍下么？",
              "variants": [],
              "sources": [
                "recovered_tech_from_removed_v2.json"
              ]
            },
            {
              "id": "gen_1018",
              "question": "t-SNE 无法在线部署，如何设计“轻量级可视化”供生产环境实时监控？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1019",
              "question": "tcp3 次握手&4 次挥手？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1020",
              "question": "tomcat 容器是如何创建servlet 类实例？用到了什么原理？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1021",
              "question": "torch.multiprocessing 函数介绍一下？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1022",
              "question": "word2vec 伪代码推倒？",
              "variants": [],
              "sources": [
                "recovered_tech_from_removed_v2.json"
              ]
            },
            {
              "id": "gen_1023",
              "question": "word2vec 和fastText 对比有什么区别？",
              "variants": [],
              "sources": [
                "recovered_tech_from_removed_v2.json"
              ]
            },
            {
              "id": "gen_1024",
              "question": "word2vec 有哪些优化方法？",
              "variants": [],
              "sources": [
                "recovered_tech_from_removed_v2.json"
              ]
            },
            {
              "id": "gen_1025",
              "question": "⽹络层（IP)与数据链路层(MAC)有什么关系呢？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1026",
              "question": "一、为什么需要 对 llama2 做 基于lora的二次预训练？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1027",
              "question": "一、介绍一下 gradient accumulation 显存优化方式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1028",
              "question": "一个巨大的圆形水池，周围布满了老鼠洞。猫追老鼠到水池边，老鼠未来得及进洞就掉入水池里。猫继续沿水池边缘企图捉住老鼠（猫不入水）。已知V猫=4V鼠。问老鼠是否有办法摆脱猫的追逐？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1029",
              "question": "一个物品正方和反方，分类器是如何判断属于同一类物品？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1030",
              "question": "七、如何 使用 基于lora的llama2 做推理 ？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1031",
              "question": "三、fastText的分类过程？fastText的优点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1032",
              "question": "三、介绍一下 MOE（Mixture-of-Experts）分布式并行策略？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1033",
              "question": "三、基于lora的llama2二次预训练 的思想是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1034",
              "question": "与所给图形相同规律的图形？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1035",
              "question": "个元素在一个有序数组的第一次出现位置？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1036",
              "question": "为什么 TIME_WAIT 等待的时间是 2MSL？",
              "variants": [],
              "sources": [
                "questions_only_from_pdf_strict.json"
              ]
            },
            {
              "id": "gen_1037",
              "question": "为什么Redis 的操作是原子性的，怎么保证原子性？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1038",
              "question": "为什么Redis 需要把所有数据放到内存中？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1039",
              "question": "为什么Redis是单线程？",
              "variants": [],
              "sources": [
                "questions_only_from_pdf_strict.json"
              ]
            },
            {
              "id": "gen_1040",
              "question": "为什么使用占据栅格地图构建算法构建地图？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1041",
              "question": "为什么参数化查询可以防止 sql 注入？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1042",
              "question": "为什么在MongoDB 中使用\"Object ID\"数据类型？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1043",
              "question": "为什么是 2 MSL 的时⻓呢？",
              "variants": [],
              "sources": [
                "questions_only_from_pdf_strict.json"
              ]
            },
            {
              "id": "gen_1044",
              "question": "为什么用MOngoDB？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1045",
              "question": "为什么要做Redis 分区？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1046",
              "question": "为什么要在MongoDB 中使用分析器？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1047",
              "question": "为什么要在MongoDB 中用\"Code\"数据类型？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1048",
              "question": "为什么要在MongoDB 中用\"Regular Expression\"数据类型？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1049",
              "question": "为什么要用MQ？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1050",
              "question": "为什么要用Nginx？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1051",
              "question": "为什么通过新闻可以预测网络故障呢？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1052",
              "question": "为什么需要 TIME_WAIT 状态？",
              "variants": [],
              "sources": [
                "questions_only_from_pdf_strict.json"
              ]
            },
            {
              "id": "gen_1053",
              "question": "为什么需要 前缀微调（Preﬁx-tining）？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1054",
              "question": "为什么需要 前缀微调（Preﬁx-tuning）？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1055",
              "question": "为什么需要可靠的UDP？",
              "variants": [],
              "sources": [
                "questions_only_from_pdf_strict.json"
              ]
            },
            {
              "id": "gen_1056",
              "question": "为什么需要进行参选微调？参数微调的原因有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1057",
              "question": "九、如果softmax的e次方超过float的值了怎么办？",
              "variants": [],
              "sources": [
                "recovered_tech_from_removed_v2.json"
              ]
            },
            {
              "id": "gen_1058",
              "question": "事件是要分类型的吧？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1059",
              "question": "二、介绍一下 gradient checkpointing 显存优化方式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1060",
              "question": "二、基于lora的llama2二次预训练 的目标是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1061",
              "question": "二、如何 配置 LoraConfig？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1062",
              "question": "二十、Lora的矩阵怎么初始化？为什么要初始化为全0？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1063",
              "question": "五、如何 基于lora的llama2二次预训练 ？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1064",
              "question": "交叉熵公式，分类为什么用交叉熵不用平方差 ？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1065",
              "question": "什么叫特征工程？有哪些评估模型好坏的指标？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1066",
              "question": "什么叫线程安全？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1067",
              "question": "什么叫视图？游标是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1068",
              "question": "什么是 A/B test？核心原理和应用场景？",
              "variants": [],
              "sources": [
                "数据分析常考面试题101题_only_questions_theory.json"
              ]
            },
            {
              "id": "gen_1069",
              "question": "什么是 PagedAttention？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1070",
              "question": "什么是Apache Kafka？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1071",
              "question": "什么是EDA(Exploratory Data Analysis)？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1072",
              "question": "什么是Flask？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1073",
              "question": "什么是Http 协议无状态协议？怎么解决Http 协议无状态协议？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1074",
              "question": "什么是Java 序列化，如何实现Java 序列化？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1075",
              "question": "什么是Java 虚拟机？为什么Java 被称作是无关平台的编程语言？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1076",
              "question": "什么是JavaConfig？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1077",
              "question": "什么是MongoDB？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1078",
              "question": "什么是Nginx？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1079",
              "question": "什么是Python 模块？Python 中有哪些常用的内置模块？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1080",
              "question": "什么是Redis, 具有哪些特点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1081",
              "question": "什么是Redis？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1082",
              "question": "什么是Redis？简述它的优缺点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1083",
              "question": "什么是Servlet？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1084",
              "question": "什么是Spring Boot？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1085",
              "question": "什么是Spring Cloud Bus？我们需要它吗？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1086",
              "question": "什么是Spring Cloud？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1087",
              "question": "什么是Spring bean？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1088",
              "question": "什么是Swagger？你用Spring Boot 实现了它吗？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1089",
              "question": "什么是TCP粘包问题？",
              "variants": [],
              "sources": [
                "questions_only_from_pdf_strict.json"
              ]
            },
            {
              "id": "gen_1090",
              "question": "什么是WebSocket？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1091",
              "question": "什么是n元语法？为什么要用n-gram？",
              "variants": [],
              "sources": [
                "recovered_tech_from_removed_v2.json"
              ]
            },
            {
              "id": "gen_1092",
              "question": "什么是redis 持久化？rdb 和aof 的比较？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1093",
              "question": "什么是spring 自动装配？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1094",
              "question": "什么是交叉验证？交叉验证的作用是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1095",
              "question": "什么是代理服务器，他们如何保护计算机网络？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1096",
              "question": "什么是入侵检测系统（IDS）和入侵防御系统（IPS）？它们有何不同？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1097",
              "question": "什么是分类？业务应用场景？常见算法？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1098",
              "question": "什么是北极星指标？什么是虚荣指标？",
              "variants": [],
              "sources": [
                "数据分析常考面试题101题_only_questions_theory.json"
              ]
            },
            {
              "id": "gen_1099",
              "question": "什么是反向代理？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1100",
              "question": "什么是同比、环比，意义是什么？",
              "variants": [],
              "sources": [
                "数据分析常考面试题101题_only_questions_theory.json"
              ]
            },
            {
              "id": "gen_1101",
              "question": "什么是图灵完备语言？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1102",
              "question": "什么是基数（Cardinality）和可选择率（Selectivity）？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1103",
              "question": "什么是堆？什么是完全二叉树？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1104",
              "question": "什么是多线程的上下文切换？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1105",
              "question": "什么是安全监控？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1106",
              "question": "什么是待定的统计信息（Pending Statistic）？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1107",
              "question": "什么是控制反转（IOC），什么是依赖注入（DI）？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1108",
              "question": "什么是数据库？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1109",
              "question": "什么是正向代理？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1110",
              "question": "什么是直方图（Histogram）？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1111",
              "question": "什么是相关性分析？相关和因果的区别是什么？",
              "variants": [],
              "sources": [
                "数据分析常考面试题101题_only_questions_theory.json"
              ]
            },
            {
              "id": "gen_1112",
              "question": "什么是线程局部变量？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1113",
              "question": "什么是线程？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1114",
              "question": "什么是网络拓扑？为什么它很重要？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1115",
              "question": "什么是网络流量分析？如何进行网络流量分析？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1116",
              "question": "什么是网络钓鱼？如何避免成为受害者？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1117",
              "question": "什么是负载均衡？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1118",
              "question": "什么是非关系型数据库？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1119",
              "question": "介绍一下 MOE（Mixture-of-Experts）分布式并行策略？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1120",
              "question": "介绍一下stable diffusion的原理？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1121",
              "question": "介绍一下文件描述符 -file_descripor？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1122",
              "question": "介绍图片分类demo，怎么对数据集进行的处理？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1123",
              "question": "你对语义web 栈了解多少？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1124",
              "question": "你怎么比较MongoDB、CouchDB 及CouchBase？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1125",
              "question": "你怎么理解统计学？生活中统计学应用举例？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1126",
              "question": "你理解的指标是什么？有哪些组成部分？",
              "variants": [],
              "sources": [
                "数据分析常考面试题101题_only_questions_theory.json"
              ]
            },
            {
              "id": "gen_1127",
              "question": "你知道有哪些Redis 分区实现方案？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1128",
              "question": "你知道进程吗？有进程为何还有线程？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1129",
              "question": "你知道那些降维算法？知道t-sne 吗？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1130",
              "question": "你觉得⽤用 Dubbo 好还是 Spring Cloud 好？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1131",
              "question": "你觉得做模型最重要的是什么？即一个好的模型算法工程师所必须的技能有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1132",
              "question": "你觉得机器学习算法在阿里巴巴哪些场景中可以落地？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1133",
              "question": "你还了了解别的分布式框架吗？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1134",
              "question": "使用MQ 会有什么问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1135",
              "question": "使用Redis 有哪些好处？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1136",
              "question": "使用Spring Cloud 有什么优势？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1137",
              "question": "使用“反向代理服务器”的优点是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1138",
              "question": "保护网络的一种方法是使用密码。什么可以被认为是好的密码？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1139",
              "question": "修改配置不重启Redis 会实时生效吗？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1140",
              "question": "八、softmax和交叉熵损失怎么计算，二值交叉熵呢？",
              "variants": [],
              "sources": [
                "recovered_tech_from_removed_v2.json"
              ]
            },
            {
              "id": "gen_1141",
              "question": "公平调度器容器集中在同一个服务器上？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1142",
              "question": "六、如何 基于lora的llama2 微调 ？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1143",
              "question": "内存泄漏和内存溢出？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1144",
              "question": "分布式数据库是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1145",
              "question": "分布式缓存？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1146",
              "question": "分析器在MongoDB 中的作用是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1147",
              "question": "分片(sharding)和复制(replication)是怎样工作的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1148",
              "question": "分类任务有哪些类别？它们都有什么特征？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1149",
              "question": "分类模型的评估指标有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1150",
              "question": "列出5个python标准库？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1151",
              "question": "创建进程的系统调用有那些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1152",
              "question": "判断是否存在CDN，CDN绕过，网站有CDN怎么找真实ip？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1153",
              "question": "前缀微调（Preﬁx-tining）思路是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1154",
              "question": "前缀微调（Preﬁx-tining）的优点是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1155",
              "question": "协方差与相关系数的区别和联系。？",
              "variants": [],
              "sources": [
                "数据分析常考面试题101题_only_questions_theory.json"
              ]
            },
            {
              "id": "gen_1156",
              "question": "单例模式的几种实现方式的及优化？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1157",
              "question": "反射中，Class.forName()和ClassLoader.loadClass()的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1158",
              "question": "可以从哪些方面来优化nginx 服务？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1159",
              "question": "可以作为GCRoots 的对象有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1160",
              "question": "可以简单介绍一些word2vec 吗？",
              "variants": [],
              "sources": [
                "recovered_tech_from_removed_v2.json"
              ]
            },
            {
              "id": "gen_1161",
              "question": "右图是由9个等边三角形拼成的六边形，现已知中间最小的等边三角形的边长是a，问这个六边形的周长是多少？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1162",
              "question": "向量数据库有那些？各自优点与区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1163",
              "question": "哪些算法不需要对特征进行标准化处理？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1164",
              "question": "哪些语言支持MongoDB？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1165",
              "question": "四、基于lora的llama2二次预训练 语料构建思路？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1166",
              "question": "因为项目中使用到了macro-f1，所以问了marco-f1，为什么不用Precision、recall？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1167",
              "question": "图像中的⾓点(Harris⾓点)是什么？为什么⽤⾓点作为特征点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1168",
              "question": "图像算法如何进一步压榨TensorCore？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1169",
              "question": "图像类数据，有什么扩充的方法？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1170",
              "question": "图解 流水线并行（Pipeline Parallelism）模型并行 必要性？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1171",
              "question": "在.java 源文件中可以有多个类吗（内部类除外）？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1172",
              "question": "在MongoDB 中什么是副本集？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1173",
              "question": "在MongoDB 中如何创建一个新的数据库？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1174",
              "question": "在MongoDB 中如何创建一个集合？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1175",
              "question": "在MongoDB 中如何删除一个集合？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1176",
              "question": "在MongoDB 中如何在集合中插入一个文档？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1177",
              "question": "在MongoDB 中如何排序？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1178",
              "question": "在MongoDB 中如何更新数据？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1179",
              "question": "在MongoDB 中如何查看一个已经创建的集合？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1180",
              "question": "在MongoDB 中如何查看数据库列表？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1181",
              "question": "在MongoDB 中如何除去一个数据库？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1182",
              "question": "在Mysql 中ENUM 的用法是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1183",
              "question": "在Nginx 中，如何使用未定义的服务器名称来阻止处理请求？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1184",
              "question": "在Nginx 中，解释如何在URL 中保留双斜线？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1185",
              "question": "在Python 中如何实现快速排序？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1186",
              "question": "在Spring 中如何注入一个java 集合？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1187",
              "question": "在哪些场景使用MongoDB？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1188",
              "question": "在排除计算机网络问题时，可能会发生什么常见的硬件相关问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1189",
              "question": "在有 shell 的情况下，如何使用 xss 实现对目标站的长久控制？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1190",
              "question": "垃圾回收器的基本原理是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1191",
              "question": "垃圾回收机制？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1192",
              "question": "多线程和多进程的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1193",
              "question": "多输入多输出网络如何批量校验？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1194",
              "question": "大概说明细粒度分类项目中的数据集，多少个类，每个类多少张图片？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1195",
              "question": "大面积并发，在不增加服务器，如何解决服务器响应不及时问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1196",
              "question": "如何 对 baichuan-13B 进行微调？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1197",
              "question": "如何使⽤transformers库加载sentencepiece模型？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1198",
              "question": "如何使用Unix shell 登录Mysql？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1199",
              "question": "如何保证Redis 集群的数据一致性？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1200",
              "question": "如何保证线程的安全？",
              "variants": [],
              "sources": [
                "questions_only_from_pdf_strict.json"
              ]
            },
            {
              "id": "gen_1201",
              "question": "如何减小两个概率分布之间的KL散度？讲讲优化方法？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1202",
              "question": "如何创建、销毁线程？",
              "variants": [],
              "sources": [
                "questions_only_from_pdf_strict.json"
              ]
            },
            {
              "id": "gen_1203",
              "question": "如何创建一个简单的Flask 应用？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1204",
              "question": "如何初始化神经网络的权重？‍神经网络怎样进行参数初始化？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1205",
              "question": "如何判断目标主机是Windows还是Linux？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1206",
              "question": "如何判断自己被getshell？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1207",
              "question": "如何利用 Python 操作 Neo4j 图数据库？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1208",
              "question": "如何区分单栏还是双栏pdf？如何重新排序？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1209",
              "question": "如何在Python 中管理内存？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1210",
              "question": "如何在Spring Boot 中禁用Actuator 端点安全性？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1211",
              "question": "如何在Unix 和Mysql 时间戳之间进行转换？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1212",
              "question": "如何处理粘包和拆包？",
              "variants": [],
              "sources": [
                "questions_only_from_pdf_strict.json"
              ]
            },
            {
              "id": "gen_1213",
              "question": "如何处理缓存雪崩问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1214",
              "question": "如何实现可靠的UDP？",
              "variants": [],
              "sources": [
                "questions_only_from_pdf_strict.json"
              ]
            },
            {
              "id": "gen_1215",
              "question": "如何对档案进行分类？标准是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1216",
              "question": "如何提取表格和图⽚中的数据？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1217",
              "question": "如何杀死⼀个进程？",
              "variants": [],
              "sources": [
                "questions_only_from_pdf_strict.json"
              ]
            },
            {
              "id": "gen_1218",
              "question": "如何查看TCP的连接状态？",
              "variants": [],
              "sources": [
                "questions_only_from_pdf_strict.json"
              ]
            },
            {
              "id": "gen_1219",
              "question": "如何查看使用MongoDB 的连接？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1220",
              "question": "如何理解MongoDB 中的GridFS 机制，MongoDB 为何使用GridFS 来存储文件？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1221",
              "question": "如何用Python 实现多线程编程？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1222",
              "question": "如何获取当前的Mysql 版本？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1223",
              "question": "如何解决NAT潜在的问题呢？",
              "variants": [],
              "sources": [
                "questions_only_from_pdf_strict.json"
              ]
            },
            {
              "id": "gen_1224",
              "question": "如何解决TCP粘包问题？",
              "variants": [],
              "sources": [
                "questions_only_from_pdf_strict.json"
              ]
            },
            {
              "id": "gen_1225",
              "question": "如何进行网络安全风险评估？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1226",
              "question": "如何通过日志分析数据库性能瓶颈？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1227",
              "question": "如何避免僵⼫进程？",
              "variants": [],
              "sources": [
                "questions_only_from_pdf_strict.json"
              ]
            },
            {
              "id": "gen_1228",
              "question": "如何重新加载Spring Boot 上的更改，而无需重新启动服务器？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1229",
              "question": "如何验证 A/B test 的结果？",
              "variants": [],
              "sources": [
                "数据分析常考面试题101题_only_questions_theory.json"
              ]
            },
            {
              "id": "gen_1230",
              "question": "如图所示，A、B、C、D、E五所学校间有公路相通，图上标出了每段公路的长度。现要选择一个学校召开一次会议，已知出席会议的代表人数为：A校6人、B校4人、C校8人、D校7人，E校10人，问为使参加会议的代表所走的路程总和最小，会议应选在哪个学校召开？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1231",
              "question": "如果图里带有专色在做的时候专色在图片处理软件里是（）形式反出来的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1232",
              "question": "如果底库涨到10亿，单机内存不足，如何设计分布式HNSW？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1233",
              "question": "如果我在使用复制技术(replication)，可以一部分使用日志(journaling)而其他部分则不使用吗？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1234",
              "question": "如果提交任务时，线程池队列已满，这时会发生什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1235",
              "question": "如果有一个特别大的访问量，到数据库上，怎么做优化？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1236",
              "question": "字符串常量池到底存在于内存空间的哪里？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1237",
              "question": "字符设备驱动程序的关键数据结构是哪个？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1238",
              "question": "守护线程是什么？它和非守护线程的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1239",
              "question": "定时线程的使用？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1240",
              "question": "实现多线程有几种方式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1241",
              "question": "实现数据质量监控，你具体怎么做，详细说？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1242",
              "question": "客户有权利问你很多问题，其实你也是有权利问客户的。下列问题你可是试着问问看，对你了解客户好处多多：How can you evaluate your suppliers？很多客户不愿意直接回答你，因为确实太难了，你可以补充一句, just generally speaking, not the detailed principles. What's your purchasing plan for next season？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1243",
              "question": "对fine-tuning(微调模型)的理解？为什么要修改最后几层神经网络权值？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1244",
              "question": "对于EfficientNet，说明一下原理？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1245",
              "question": "对于一个取值较多的类别变量在不能进行onehot 的情况下如何使用？",
              "variants": [],
              "sources": [
                "recovered_tech_from_removed_v2.json"
              ]
            },
            {
              "id": "gen_1246",
              "question": "对于图像拼接，基于什么特征？为什么选择这个特征？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1247",
              "question": "对文件或设备的操作函数保存在那个数据结构中？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1248",
              "question": "导致Full GC 一般有哪些情况？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1249",
              "question": "就一般的企业而言，如何进行档案的分类？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1250",
              "question": "工程上，怎么实现LR 的并行化？有哪些并行化的工具？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1251",
              "question": "常用的Python 库有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1252",
              "question": "常用的第三方数据统计平台有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1253",
              "question": "常见的分类算法有哪些？他们各⾃的优缺点是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1254",
              "question": "常见的垃圾回收算法有哪些？简述其原理？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1255",
              "question": "年杭州市人均GDP约为多少美元？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1256",
              "question": "并发峰值多少？大概哪个时间点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1257",
              "question": "当 多卡并行时，通信成为瓶颈，如何隐藏？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1258",
              "question": "当我试图更新一个正在被迁移的块(chunk)上的文档时会发生什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1259",
              "question": "微调方法是啥？如何微调？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1260",
              "question": "快速排序算法？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1261",
              "question": "怎么⽤UDP实现可靠传输？",
              "variants": [],
              "sources": [
                "questions_only_from_pdf_strict.json"
              ]
            },
            {
              "id": "gen_1262",
              "question": "怎么保护计算机网络？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1263",
              "question": "怎么保证MQ 的高可用？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1264",
              "question": "怎么唤醒一个阻塞的线程？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1265",
              "question": "怎么处理、网络不收敛怎么办、类别不平衡怎么办、数据和模型都跟别人用的一样的，怎么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1266",
              "question": "怎么安装的nginx？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1267",
              "question": "怎么提高并发量，请列举你所知道的方案？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1268",
              "question": "怎么构建知识图谱呢？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1269",
              "question": "怎么测试Redis 的连通性？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1270",
              "question": "怎么看待计算机网络和操作系统在DL 中的作用？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1271",
              "question": "怎么设置Redis 过期策略？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1272",
              "question": "怎样申请大块内核内存？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1273",
              "question": "思维图 Graph of Thoughts（GOT）核心思想是什么 ？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1274",
              "question": "我们如何在mysql 中运行批处理模式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1275",
              "question": "我应该启动一个集群分片(sharded)还是一个非集群分片的 MongoDB 环境？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1276",
              "question": "手写softmax公式，手写BN公式，softmax层的label 是什么 ？",
              "variants": [],
              "sources": [
                "recovered_tech_from_removed_v2.json"
              ]
            },
            {
              "id": "gen_1277",
              "question": "指标和维度的区别和联系？",
              "variants": [],
              "sources": [
                "数据分析常考面试题101题_only_questions_theory.json"
              ]
            },
            {
              "id": "gen_1278",
              "question": "指示微调（Prompt-tuning）与 Preﬁx-tuning 区别 是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1279",
              "question": "指示微调（Prompt-tuning）与 ﬁne-tuning 区别 是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1280",
              "question": "按上述汇率计算，1999年杭州市区人均GDP 达到多少元(人民币)？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1281",
              "question": "推导一下FM 模型和DeepFM 模型？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1282",
              "question": "描述一下JVM 加载class 文件的原理机制？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1283",
              "question": "描述网络地址转换（NAT）技术？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1284",
              "question": "描述网络拓扑？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1285",
              "question": "数据库三范式是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1286",
              "question": "数据库查询有哪些常用的优化方法？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1287",
              "question": "数据库的三范式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1288",
              "question": "数据库运行很慢，如何解决？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1289",
              "question": "数据库连接池的原理。为什么要使用连接池？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1290",
              "question": "数组在内存中如何分配？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1291",
              "question": "日志保存多久？30天。2、有什么作用？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1292",
              "question": "日志记录被人删除了，你该怎么做？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1293",
              "question": "时间序列预测的原理是什么？有哪些应用场景？",
              "variants": [],
              "sources": [
                "数据分析常考面试题101题_only_questions_theory.json"
              ]
            },
            {
              "id": "gen_1294",
              "question": "是否使用过Redis 集群，集群的原理是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1295",
              "question": "有5只猴子在海边发现 一堆桃子,决定第二天来平分.第二天清晨,第一只猴子最早来到,它左分右分分不开,就朝海里扔了一只,恰好可以分成5份,它拿上自己的一份走了.第 2,3,4,5只猴子也遇到同样的问题,采用了同样的方法,都是扔掉一只后,恰好可以分成5份.问这堆桃子至少有多少只？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1296",
              "question": "有一堆螺丝和螺母，若一个螺丝配2个螺母，则多10个螺母；若1个螺丝配3个螺母，则少6个螺母。共有多少个螺丝？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1297",
              "question": "有三个人去住旅馆，住三间房，每一间房$10元，于是他们一共付给老板$30， 第二天，老板觉得三间房只需要$25元就够了于是叫小弟退回$5给三位客人， 谁知小弟贪心,只退回每人$1，自己偷偷拿了$2，这样一来便等于那三位客人每人各花了九元， 于是三个人一共花了$27，再加上小弟独吞了不$2，总共是$29。可是当初他们三个人一共付出$30那么还有$1呢？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1298",
              "question": "有哪些常用的机器学习方法？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1299",
              "question": "有哪些数据库优化方面的经验？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1300",
              "question": "服务器的分类？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1301",
              "question": "服务注册和发现是什么意思？Spring Cloud 如何实现？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1302",
              "question": "机器学习会影响web3 吗？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1303",
              "question": "李明从图书馆借来一批图书，他先给了甲5本和剩下的1/5，然后给了乙4本和剩下的1/4，又给了丙3本和剩下的1/3，又给了丁2本和剩下的1/2，最后自己还剩2本。李明共借了多少本书？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1304",
              "question": "某城市发生了一起汽车撞人逃跑事件该城市只有两种颜色的车,蓝色15% 绿色85%事发时有一个人在现场看见了他指证是蓝车但是根据专家在现场分析,当时那种条件能看正确的可能性是80%那么,肇事的车是蓝车的概率到底是多少？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1305",
              "question": "查看Redis 使用情况及状态信息用什么命令？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1306",
              "question": "样本量多少？为什么使用深度神经网络？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1307",
              "question": "桥接模式是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1308",
              "question": "模型参数微调的方式有那些？你最常用哪些方法？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1309",
              "question": "次完整的Http 请求是怎样的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1310",
              "question": "注册类和活跃类指标，你会看哪个？",
              "variants": [],
              "sources": [
                "数据分析常考面试题101题_only_questions_theory.json"
              ]
            },
            {
              "id": "gen_1311",
              "question": "流⽔线并⾏（Pipeline Parallelism） 图解？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1312",
              "question": "流⽔线并⾏（Pipeline Parallelism）优缺点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1313",
              "question": "流水线并行（Pipeline Parallelism） 图解？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1314",
              "question": "流水线并行（Pipeline Parallelism）优缺点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1315",
              "question": "测试活动中统计了哪些数据？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1316",
              "question": "消息队列会吗？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1317",
              "question": "深度学习替代传统滤波？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1318",
              "question": "特征方程的几何意义？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1319",
              "question": "用 Python 设计算法实现圆周率的计算？",
              "variants": [],
              "sources": [
                "数据分析常考面试题101题_only_questions_theory.json"
              ]
            },
            {
              "id": "gen_1320",
              "question": "用Nginx 服务器解释-s 的目的是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1321",
              "question": "用哪两种方式来实现集合的排序？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1322",
              "question": "用户进程间通信主要哪几种方式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1323",
              "question": "知识图谱可以做什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1324",
              "question": "知识图谱怎么存储？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1325",
              "question": "知道那些矩阵分解的方法？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1326",
              "question": "种类型的数据节点Znode？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1327",
              "question": "种设计模式的具体的每种模式的功能？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1328",
              "question": "端侧 INT8 部署如何保持 Lipschitz？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1329",
              "question": "端侧部署的极限优化？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1330",
              "question": "等待事件的分类？常见等待事件？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1331",
              "question": "策略模式是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1332",
              "question": "简单工厂和抽象工厂的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1333",
              "question": "算法是否看过开源的源代码，或是否自己手动实现过？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1334",
              "question": "系统的用户量有多少？多用户并发访问时如何解决？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1335",
              "question": "线程B 怎么知道线程A 修改了变量？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1336",
              "question": "线程同步的方法？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1337",
              "question": "线程池的优点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1338",
              "question": "线程池的作用？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1339",
              "question": "线程池的工作原理，几个重要参数？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1340",
              "question": "线程池的拒绝策略都有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1341",
              "question": "线程池的类型？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1342",
              "question": "线程池的阻塞队列有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1343",
              "question": "线程的几种状态？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1344",
              "question": "线程的创建方式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1345",
              "question": "线程间通信，wait 和notify 的理解和使用？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1346",
              "question": "线程阻塞有哪些原因？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1347",
              "question": "给你100G 数据，1G 内存，如何排序？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1348",
              "question": "给你一个无序数组，怎么才能合理采样？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1349",
              "question": "给你一个有1000列和1百万行的训练数据集，这个数据集是基于分类问题的。经理要求你来降低该数据集的维度以减少模型计算时间，但你的机器内存有限。你会怎么做？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1350",
              "question": "网线的排列？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1351",
              "question": "网络协议（protocol）是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1352",
              "question": "网络层常见的协议？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1353",
              "question": "网络拓扑如何影响您在建立网络时的决策？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1354",
              "question": "聊聊五层计算机网络体系结构中，每一层对应的网络协议有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1355",
              "question": "能否使用日志特征进行安全备份？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1356",
              "question": "视图的作用，视图可以更改么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1357",
              "question": "解释Nginx 是否支持将请求压缩到上游？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1358",
              "question": "解释Spring 支持的几种bean 的作用域？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1359",
              "question": "解释一下代理模式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1360",
              "question": "解释如何在Nginx 中获得当前的时间？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1361",
              "question": "解释如何在Nginx 服务器上添加模块？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1362",
              "question": "解释栈(stack)、堆(heap)和方法区(method area)的用法？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1363",
              "question": "设计模式的优点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1364",
              "question": "设计模式的六大基本原则？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1365",
              "question": "设计模式的分类？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1366",
              "question": "详细介绍一种非参数统计方法，并叙述其优缺点？",
              "variants": [],
              "sources": [
                "数据分析常考面试题101题_only_questions_theory.json"
              ]
            },
            {
              "id": "gen_1367",
              "question": "说一下SVD 的时间复杂度？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1368",
              "question": "说一下你熟悉的设计模式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1369",
              "question": "说几个常见的编译时异常？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1370",
              "question": "说出Servlet 的生命周期，并说出Servlet 和CGI 的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1371",
              "question": "说说HTTP 的状态码，301 和302 的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1372",
              "question": "请写出实现线程安全的几种方式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1373",
              "question": "请列举Nginx 和Apache 之间的不同点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1374",
              "question": "请列举Nginx 的一些特性？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1375",
              "question": "请描述HTTP,HTTPS 分别是什么协议，作用，及端口号？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1376",
              "question": "请求。所以，上⾯通过在服务层引⼊队列和缓存，让最底层的数据库⾼枕⽆忧？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1377",
              "question": "请简述CBOW 和FastText 之间的区别？",
              "variants": [],
              "sources": [
                "recovered_tech_from_removed_v2.json"
              ]
            },
            {
              "id": "gen_1378",
              "question": "请解释Nginx 服务器上的Master 和Worker 进程分别是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1379",
              "question": "请解释ngx_http_upstream_module 的作用是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1380",
              "question": "请解释一下什么是Nginx？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1381",
              "question": "请解释你如何通过不同于80 的端口开启Nginx？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1382",
              "question": "请解释是否有可能将Nginx 的错误替换为502 错误、503？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1383",
              "question": "请解释残差连接的公式和原理，并说明它在深度神经网络中的作用？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1384",
              "question": "请说一说PCA？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1385",
              "question": "调用schedule()进行进程切换的方式有几种？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1386",
              "question": "谈到网络，什么是权限？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1387",
              "question": "负载均衡的原理？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1388",
              "question": "转发（forward）和重定向（redirect）的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1389",
              "question": "轻量化检测（如移动端NanoDet）把BN替换为GN+EMA是否还需同步？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1390",
              "question": "迁移之后的数据一致性怎么校验？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1391",
              "question": "进程和线程的区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1392",
              "question": "进程调度的核心数据结构是哪个？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1393",
              "question": "适配器微调（Adapter-tuning）思路？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1394",
              "question": "通过伙伴系统申请内核内存的函数有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1395",
              "question": "都有哪些办法可以降低Redis 的内存使用情况呢？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1396",
              "question": "都有哪些垃圾回收器？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1397",
              "question": "针对实习提问题：为什么要加入reading-gate？为什么不直接把控制向量加在输入中？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1398",
              "question": "集群（Cluster）特有的后台进程有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1399",
              "question": "需要归一化的算法有哪些？这些模型需要归一化的主要原因？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1400",
              "question": "非关系型数据库有哪些类型？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1401",
              "question": "面试官追问“为什么不用DiskANN”？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1402",
              "question": "面试追问 3：如何把 SQ/RQ 做成实时在线监控？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1403",
              "question": "题三十四：制作原型应该在项目生命周期的哪个阶段？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1404",
              "question": "题二十三：你将如何监控/管理顾问？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1405",
              "question": "题（1）店铺VIP顾客对于您推荐的服装一直非常不满意，语气不好，这时你如何处理？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "gen_1406",
              "question": "高并发情况下，我们系统是如何支撑大量的请求的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            }
          ]
        }
      ]
    },
    {
      "category": "LLM",
      "count": 2225,
      "subcategories": [
        {
          "subcategory": "General AI-General",
          "count": 27,
          "questions": [
            {
              "id": "llm_0001",
              "question": "# tokens: [CLS] is this jack ##son ##ville？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0002",
              "question": "1 AdaLoRA 的思路是怎么样的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0003",
              "question": "1 DPO（对话策略学习）是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0004",
              "question": "1 QLoRA 的思路是怎么样的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0005",
              "question": "1 什么是 LoRA？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0006",
              "question": "2 DPO（对话策略学习）的输入输出是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0007",
              "question": "2 LoRA 的思路是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0008",
              "question": "2 QLoRA 的特点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0009",
              "question": "3 CRF in TensorFlow V.S. CRF in discrete toolkit？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0010",
              "question": "3 DPO（对话策略学习）的实现方法是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0011",
              "question": "3 LoRA 的特点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0012",
              "question": "4 简单描述一下 LoRA？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0013",
              "question": "5 大模型微调 p_tuning 和传统 fine tuning 有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0014",
              "question": "LoRA 微调结果如何保存？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0015",
              "question": "LoRA 微调计算可训练参数的比例 如何确定？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0016",
              "question": "七、LoRA微调方法为啥能加速训练？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0017",
              "question": "九、LoRA 缺点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0018",
              "question": "五、ChatGLM-6B LoRA后的权重多大？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0019",
              "question": "八、如何在已有LoRA模型上继续训练？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0020",
              "question": "六、LoRA 微调优点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0021",
              "question": "十、LoRA这种微调方法和全参数比起来有什么劣势吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0022",
              "question": "十九、是否可以逐层调整LoRA的最优rank？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0023",
              "question": "十二、LoRA 微调参数量怎么确定？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0024",
              "question": "十五、LoRA 高效微调 如何避免过拟合？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0025",
              "question": "十六、微调大模型时, 优化器如何？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0026",
              "question": "四、LoRA权重是否可以合入原模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0027",
              "question": "大模型微调过程中如何避免灾难性遗忘？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            }
          ]
        },
        {
          "subcategory": "LLM-General",
          "count": 1363,
          "questions": [
            {
              "id": "llm_0028",
              "question": "\"amount\"\\s*:\\s*(？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0029",
              "question": "\"input\": \"how many letters in the word educa？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0030",
              "question": "\"input\": \"单词 educa 中有多少个字母？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0031",
              "question": "(系统|用户|助手)？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0032",
              "question": ")\\s*(\\S{0,6}？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0033",
              "question": "**什么是low-rank adaptation of large language models？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0034",
              "question": "*“尽最大合理技术努力”？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0035",
              "question": "- 4.1.2 前缀微调（Prefix-tuning）思路是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0036",
              "question": "- 4.1.3 前缀微调（Prefix-tuning）的优点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0037",
              "question": "- 4.1.4 前缀微调（Prefix-tuning）的缺点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0038",
              "question": "--发动机如何将燃料转化为机械能？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0039",
              "question": "--发动机的基本功能是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0040",
              "question": "--发动机运行涉及哪些关键部件，它们如何提高发动机的效率？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0041",
              "question": "1 ALiBi (Attention with Linear Biases) 思路是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0042",
              "question": "1 Byte-Pair Encoding(BPE) 如何构建词典？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0043",
              "question": "1 LLaMA 2 社区版在月活 >700M 产品中的合规限制有哪些具体条款？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0044",
              "question": "1 LN 在 LLMs 中的不同位置 有什么区别么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0045",
              "question": "1 LangChain 中 Components and Chains 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0046",
              "question": "1 LangChain 如何调⽤ LLMs ⽣成回复？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0047",
              "question": "1 Layer Norm 的计算公式写一下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0048",
              "question": "1 Multi-head Attention 存在什么问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0049",
              "question": "1 RAG 如何提升索引数据的质量？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0050",
              "question": "1 RMS Norm 的计算公式写一下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0051",
              "question": "1 WordPiece 与 BPE 异同点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0052",
              "question": "1 baichuan 进⾏微调时，领域数据：通⽤数据配⽐？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0053",
              "question": "1 baichuan-53B 相⽐于 baichuan-7B 和 baichuan-13B 有哪些优势？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0054",
              "question": "1 self-attention 的公式及参数量，为什么用多头，为什么要除以根号 d？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0055",
              "question": "1 self-attention 的计算方式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0056",
              "question": "1 vLLM 的 功能有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0057",
              "question": "1 为什么 ⼤模型 需要 外挂(向量)知识库？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0058",
              "question": "1 为什么大模型需要外挂 (向量) 知识库？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0059",
              "question": "1 为什么需要 前缀微调（Prefix-tuning）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0060",
              "question": "1 为什么需要对 RAG 进行评测？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0061",
              "question": "1 举例 介绍⼀下 不同 ⼤模型LLMs 的分词⽅式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0062",
              "question": "1 举例介绍一下不同大模型 LLMs 的分词方式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0063",
              "question": "1 什么是 Grouped-query Attention？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0064",
              "question": "1 什么是 LightLLM？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0065",
              "question": "1 什么是 ⻓度外推问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0066",
              "question": "1 什么是位置编码？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0067",
              "question": "1 介绍⼀下 FFN 块 计算公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0068",
              "question": "1 介绍⼀下 Text generation inference？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0069",
              "question": "1 介绍一下 KL 散度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0070",
              "question": "1 介绍一下 Text generation inference？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0071",
              "question": "1 介绍一下 规划（planning）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0072",
              "question": "1 介绍一下，现在几种流行的大模型架构？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0073",
              "question": "1 传统 Attention 存在哪些问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0074",
              "question": "1 使用 Self-Instruct 生成医疗问答时，如何设置种子指令数量与多样性阈值？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0075",
              "question": "1 假如有超多的8卡A100节点（DGX A100），如何应⽤3D并⾏策略？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0076",
              "question": "1 分布式训练框架都了解哪些，能不能简单介绍一下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0077",
              "question": "1 在 1 万条客服 query 上，如何自动搜索 k=0,1,5,10 的 F1 并绘制边际收益曲线？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0078",
              "question": "1 在 10TB 原始语料上，如何用 MinHash LSH 在 4 小时内完成近似去重？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0079",
              "question": "1 多GPU计算减少了程序运⾏的时间？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0080",
              "question": "1 大模型怎么评测？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0081",
              "question": "1 大模型训练loss突刺是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0082",
              "question": "1 如何 估算模型所需的RAM？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0083",
              "question": "1 如何利用 知识图谱（KG）进行上下文增强？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0084",
              "question": "1 如何在客服场景选择“问题解决率”而非“点击率”作为北极星？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0085",
              "question": "1 如何基于 Google Trends 和论文投稿量预测 MoE 热度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0086",
              "question": "1 如何基于 ICU 库自动转换并缓存格式模板？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0087",
              "question": "1 如何基于 LASER multilingual embeddings 计算中英提示的语义差距？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0088",
              "question": "1 如何基于 OpenTelemetry 采集 token 级延迟并上报 Prometheus？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0089",
              "question": "1 如何基于 Pydantic 模型自动生成 OpenAI 兼容的 function 描述？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0090",
              "question": "1 如何基于 RL 动态调整标注单价并激励质量？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0091",
              "question": "1 如何基于 Redis + Protobuf 缓存天气查询结果并设置 TTL？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0092",
              "question": "1 如何基于 SymPy 验证等式推导并给出自动评分？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0093",
              "question": "1 如何基于 Triton Inference Server 的 Metrics 自定义 QPS 阈值？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0094",
              "question": "1 如何基于 n-gram 频率训练小型草案模型并设置接受阈值？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0095",
              "question": "1 如何基于信息熵计算每轮对话的重要性得分并动态截断？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0096",
              "question": "1 如何基于子问题分解并验证每步中间结果？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0097",
              "question": "1 如何基于抽象语法树拆分函数并逐块生成注释？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0098",
              "question": "1 如何基于敏感度指标选择 8bit 量化哪些通道并给出公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0099",
              "question": "1 如何基于知识库对比生成声明并计算事实一致性得分？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0100",
              "question": "1 如何基于知识蒸馏将 7B 多模态模型压缩到 1B 并保持 VQA 准确率 90%？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0101",
              "question": "1 如何基于重要性得分将 90% 旧 token 转存到向量库？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0102",
              "question": "1 如何基于题目难度参数（IRT）过滤低区分度题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0103",
              "question": "1 如何定义流式 SSE 响应格式并兼容 OpenAI API 语义？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0104",
              "question": "1 如何对 FFN 权重进行 SVD 分解并选择秩？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0105",
              "question": "1 如何对图片提取 OCR+布局信息并生成统一向量表示？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0106",
              "question": "1 如何将 224×224 图像映射为 256 个视觉 token 并保证文本占比 60%？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0107",
              "question": "1 如何将几何问题转化为文本并保证信息不丢失？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0108",
              "question": "1 如何构建 GitHub 监控机器人并自动汇总 release note？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0109",
              "question": "1 如何构建专利检索关键词并监控竞争对手？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0110",
              "question": "1 如何构建中文偏见提示并测量不同群体回答差异？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0111",
              "question": "1 如何构建中文购物场景任务并定义成功率指标？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0112",
              "question": "1 如何构建多语言敏感分类器并保证 F1>95%？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0113",
              "question": "1 如何构建对抗测试集检测 SQL 注入风险？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0114",
              "question": "1 如何构建黑名单正则，防止角色提示诱导模型输出违法内容？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0115",
              "question": "1 如何查看多机训练时的网速？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0116",
              "question": "1 如何用 DVC + Git LFS 管理 500GB 微调数据集并支持回滚到任意版本？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0117",
              "question": "1 如何用 EWC（Elastic Weight Consolidation）计算重要权重矩阵并加入损失？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0118",
              "question": "1 如何用 Feature Flag 系统实现提示模板的灰度发布与实时回滚？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0119",
              "question": "1 如何用 GitHub Actions 触发数据→训练→评估→部署全链路？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0120",
              "question": "1 如何用 Intel SGX 进行可信执行环境推理并降低 10% 延迟？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0121",
              "question": "1 如何用 JSON 模板记录模型架构、数据、评测结果？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0122",
              "question": "1 如何用 KL 散度监控输出分布漂移并设置动态阈值？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0123",
              "question": "1 如何用 MIG（Multi-Instance GPU）切分 A100 并保证显存隔离？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0124",
              "question": "1 如何用 Megatron-LM 计算 175B 模型在 128GPU 下的最优 pp×tp×dp 组合？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0125",
              "question": "1 如何用 OKR 设置季度模型优化目标？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0126",
              "question": "1 如何用 Python importlib 实现插件运行时加载并隔离命名空间？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0127",
              "question": "1 如何用 Ring Attention 将序列拆分到 64 卡并计算通信开销？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0128",
              "question": "1 如何用 RoPE 的 PI（Position Interpolation）将 4k 模型扩展到 32k？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0129",
              "question": "1 如何用 SHAP 解释分类结果并可视化 top token？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0130",
              "question": "1 如何用 TensorFlow Lite 转换并构建 metadata？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0131",
              "question": "1 如何用 arxiv-sanity 筛选每日相关论文并打标签？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0132",
              "question": "1 如何用 multilingual sentence encoder 对齐意图向量？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0133",
              "question": "1 如何用投机解码将代码补全延迟降到 <100ms？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0134",
              "question": "1 如何用模型生成候选标注并计算人工节省率？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0135",
              "question": "1 如何用用户实时反馈构建滚动偏好对？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0136",
              "question": "1 如何用贝叶斯序贯检验减少实验样本量并提前收敛？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0137",
              "question": "1 如何结合 BM25 与 Contriever 分数做加权融合并学习权重 α？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0138",
              "question": "1 如何编写 CONTRIBUTING.md 并设置 CI 检查？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0139",
              "question": "1 如何自动生成 t 检验步骤并解释 p 值含义？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0140",
              "question": "1 如何计算 CodeBLEU 的语法权重并对比 BLEU？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0141",
              "question": "1 如何让 RAG 支持半结构化 RAG（文本+表格）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0142",
              "question": "1 如何记录每次推理的输入、输出、模型版本并满足 GDPR 可删除要求？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0143",
              "question": "1 如何设置最大迭代次数并防止无限循环？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0144",
              "question": "1 如何设置稀疏度调度从 0% 到 90% 并保证收敛？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0145",
              "question": "1 如何设置自适应 KL 惩罚系数并给出更新公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0146",
              "question": "1 如何设计 20 条 LF（Labeling Function）并估计准确率？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0147",
              "question": "1 如何设计双盲实验避免诱导性问卷？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0148",
              "question": "1 如何设计双盲对比界面并防止标注者疲劳？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0149",
              "question": "1 如何设计跨语言基准并验证算法逻辑等价？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0150",
              "question": "1 如何设计迭代级调度算法并计算平均吞吐提升？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0151",
              "question": "1 如何选择⼀款分布式训练框架？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0152",
              "question": "1 如何采用 Adapter 层仅训练 1% 参数并保持多语言能力？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0153",
              "question": "1 如何采用 BERT 嵌入聚类并自动标注错误类型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0154",
              "question": "1 如何采用 CLIP 相似度聚类抽取 10 个关键帧并覆盖 90% 内容？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0155",
              "question": "1 如何采用 Guided Decoding 强制输出符合 JSON Schema？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0156",
              "question": "1 如何采用 HNSW 的增量插入算法并设置 efConstruction 保证召回？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0157",
              "question": "1 如何采用 Kendall Tau 相关性验证 RM 与人类一致性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0158",
              "question": "1 如何采用 MiniLM 深度蒸馏并匹配隐层注意力？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0159",
              "question": "1 如何采用 Self-Consistency 采样多数投票提升准确率？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0160",
              "question": "1 如何采用 VAD 切分语音流并设置缓冲窗口降低延迟？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0161",
              "question": "1 如何采用 fribidi 库处理阿拉伯语双向文本？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0162",
              "question": "1 如何采用交叉验证检测标注者系统偏差？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0163",
              "question": "1 如何采用多奖励加权并动态调整权重？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0164",
              "question": "1 如何采用对抗指纹在 token 分布中嵌入不可见签名？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0165",
              "question": "1 如何采用平行三元组验证中英文知识冲突？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0166",
              "question": "1 如何采用强化学习策略决定压缩时机与长度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0167",
              "question": "1 如何采用滑动窗口摘要将 20k 检索结果压缩至 4k token？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0168",
              "question": "1 如何采用熵+多样性混合策略选择样本？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0169",
              "question": "1 如何采用金字塔方法（Pyramid Method）评估摘要事实覆盖率？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0170",
              "question": "1 如何量化模型替代人工客服节省的 FTE 并折算年费？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0171",
              "question": "1 如何针对模型输出违规设计 15 分钟响应流程？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0172",
              "question": "1 旋转位置编码 RoPE 思路是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0173",
              "question": "1 简单介绍一下大模型存在哪些问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0174",
              "question": "1 训练 ⼤语⾔模型 存在问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0175",
              "question": "1.1 如何进行 拆解子目标和任务分解？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0176",
              "question": "1.2 拆解子目标和任务分解 有哪些方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0177",
              "question": "10 平民适不适合直接上多机多卡的ZeRO3（万兆网）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0178",
              "question": "10 想要训练1个LLM，如果只想⽤1张显卡，那么对显卡的要求是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0179",
              "question": "100 下的最小显存占用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0180",
              "question": "11 如果有N张显存⾜够⼤的显卡，怎么加速训练？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0181",
              "question": "12 如果显卡的显存不够装下⼀个完整的模型呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0182",
              "question": "13 PP推理时，是⼀个串⾏的过程，1个GPU计算，其他空闲，有没有其他⽅式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0183",
              "question": "14 3种并⾏⽅式可以叠加吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0184",
              "question": "15 Colossal-AI 有1D/2D/2.5D/3D，是什么情况？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0185",
              "question": "16 除了3D并⾏有没有其他⽅式⼤规模训练？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0186",
              "question": "19 平⺠适不适合直接上多机多卡的ZeRO3（万兆⽹）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0187",
              "question": "2 ALiBi (Attention with Linear Biases) 的偏置矩阵是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0188",
              "question": "2 Attention 优化方向？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0189",
              "question": "2 LLMs 训练时有哪些有用的建议？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0190",
              "question": "2 LangChain 中 Prompt Templates and Values 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0191",
              "question": "2 LangChain 如何修改 提示模板？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0192",
              "question": "2 RAG 各模块有哪些优化策略？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0193",
              "question": "2 RAG 有哪些评估方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0194",
              "question": "2 RMS Norm 相比于 Layer Norm 有什么特点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0195",
              "question": "2 Self-RAG：如何让大模型对召回结果进行筛选？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0196",
              "question": "2 Text generation inference 的 功能有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0197",
              "question": "2 Token Attention 介绍？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0198",
              "question": "2 baichuan-53B 如何对 预训练数据 做处理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0199",
              "question": "2 vLLM 的 优点有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0200",
              "question": "2 ⻓度外推问题 的 解决⽅法 有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0201",
              "question": "2 为什么 需要 LightLLM？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0202",
              "question": "2 为什么 需要 vLLM？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0203",
              "question": "2 为什么大模型训练会出现loss突刺？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0204",
              "question": "2 交叉熵损失函数写一下，物理意义是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0205",
              "question": "2 什么是 点对点通信？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0206",
              "question": "2 什么是绝对位置编码？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0207",
              "question": "2 介绍⼀下 GeLU 计算公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0208",
              "question": "2 介绍⼀下 Multi-Query Attention？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0209",
              "question": "2 介绍⼀下 不同 ⼤模型LLMs 的分词⽅式 的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0210",
              "question": "2 介绍一下 Multi-Query Attention？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0211",
              "question": "2 介绍一下 记忆（Memory）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0212",
              "question": "2 介绍一下不同大模型 LLMs 的分词方式的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0213",
              "question": "2 你了解 deepspeed，那介绍 zero1 2 3 分别是什么，分析训练时候显存占用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0214",
              "question": "2 你能不能介绍一下 BERT 和 GPT 的训练方式(预训练任务训练细节)的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0215",
              "question": "2 写一下 Deep Norm 代码实现？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0216",
              "question": "2 大模型加速框架了解多少，知不知道原理 如何进行加速优化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0217",
              "question": "2 大模型的honest原则是如何实现的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0218",
              "question": "2 如何保存和加载多GPU训练模型呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0219",
              "question": "2 如何利用 Elo 评级算法给指令难度打分并防止标注者偏差？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0220",
              "question": "2 如何基于 T5-PEGASUS 做回译以提升金融领域术语覆盖率？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0221",
              "question": "2 如何构造对抗性 Prompt 以探测模型政治敏感话题的拒绝率？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0222",
              "question": "2 如何监控中间步骤的置信度并提前终止低置信链？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0223",
              "question": "2 如何自动化扫描 Docker 镜像中携带的 GPL 组件与模型权重冲突？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0224",
              "question": "2 如何让 RAG 支持多模态 RAG（文本+表格+图片）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0225",
              "question": "2 如何设计控制实验验证“先 CPT 后 SFT”相比“混合训练”的幻觉率差异？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0226",
              "question": "2 如何评估视觉编码器在 512×512 输入下的 FLOPs 占比？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0227",
              "question": "2 如何通过添加元数据 提升 RAG 效果？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0228",
              "question": "2 如何量化“数据不能出域”带来的合规成本并纳入 TCO？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0229",
              "question": "2 如果想构这样⼀个⼤规模并⾏训练系统，训练框架如何选？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0230",
              "question": "2 如果想构这样一个大规模并行训练系统，训练框架如何选？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0231",
              "question": "2 如果有N张显存足够大的显卡，怎么加速训练？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0232",
              "question": "2 当 ARM Cortex-A78 只有 4GB RAM 时，如何采用内存映射加载 7B 模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0233",
              "question": "2 当 Agent 采用不同模型后端时，如何归一化打分？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0234",
              "question": "2 当 GPU 成本上升 20% 时，如何计算 ROI 盈亏平衡点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0235",
              "question": "2 当 GPU 显存限制为 40GB 时，如何估算 r=64 的 LoRA 增量参数量？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0236",
              "question": "2 当 KL 散度 >0.2 时，如何采用线性退火降低学习率？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0237",
              "question": "2 当 LF 冲突时，如何采用多数投票加权重？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0238",
              "question": "2 当 NPS 提升 10 分时，如何验证与模型优化的因果关系？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0239",
              "question": "2 当 PR 冲突时，如何采用 rebase 保持线性历史？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0240",
              "question": "2 当 RM 在训练集准确率 98% 但验证集 85% 时，如何早停？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0241",
              "question": "2 当 k=10 时，如何用缓存执行结果加速 Pass@k 评估？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0242",
              "question": "2 当 prefill 与 decode 阶段计算密度差异 10× 时，如何分离调度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0243",
              "question": "2 当三类任务样本量差异 100× 时，如何设置 softmax 温度系数防止过拟合？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0244",
              "question": "2 当上下文长度 8k 时，如何采用局部注意力减少计算？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0245",
              "question": "2 当上游数据发生字段变更时，如何自动触发下游模型重训流水线？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0246",
              "question": "2 当低资源语言意图缺失时，如何采用元学习快速适应？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0247",
              "question": "2 当候选段落长度差异 10× 时，如何采用动态 padding 提升 batch 吞吐？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0248",
              "question": "2 当函数参数 >50 个时，如何采用分组嵌套减少 token 消耗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0249",
              "question": "2 当压缩比 8× 时，如何采用微调恢复 98% 准确率？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0250",
              "question": "2 当发现潜在侵权时，如何采用专利规避设计？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0251",
              "question": "2 当向量检索延迟 >500ms 时，如何采用 LRU 缓存优化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0252",
              "question": "2 当图像分辨率提升至 896×896 时，如何采用自适应切块减少 30% token？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0253",
              "question": "2 当复现 LLaMA 3 时，如何估算 8B 模型在 2k GPU 小时内的成本？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0254",
              "question": "2 当字段 >200 时，如何采用 LLM 自动生成自然语言描述？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0255",
              "question": "2 当学生模型仅 0.5B 时，如何设计分层蒸馏目标？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0256",
              "question": "2 当工具返回空结果时，如何采用候补 API 保证任务继续？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0257",
              "question": "2 当差异 >10% 时，如何采用数据重采样缓解？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0258",
              "question": "2 当序列长度 128k 时，如何采用 FlashAttention-2 减少显存？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0259",
              "question": "2 当序列长度 64k 时，如何采用滑动窗口 KV-cache 节省显存？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0260",
              "question": "2 当扩展后 PPL 上升 15% 时，如何采用课程长度微调恢复？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0261",
              "question": "2 当抽检比例 5% 时，如何计算统计功效并保证 95% 置信？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0262",
              "question": "2 当指标出现 Saturday Effect 时，如何采用 CUPED 方差缩减？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0263",
              "question": "2 当指标滞后 7 天时，如何采用代理指标实时指导？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0264",
              "question": "2 当推理链长度 >20 时，如何采用循环校验防止矛盾？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0265",
              "question": "2 当插件崩溃时，如何采用沙箱进程防止主服务宕机？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0266",
              "question": "2 当攻击采用 Base64 编码混淆时，如何构建多阶段解码检测？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0267",
              "question": "2 当数据流非平稳时，如何采用经验回放加权？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0268",
              "question": "2 当日语提示长度膨胀 1.5× 时，如何动态调整 max_tokens 防止截断？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0269",
              "question": "2 当服务宕机时，如何自动降级到规则客服并保证 SLA？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0270",
              "question": "2 当权重格式从 FP16 转为 BF16 时，如何评估对精度的影响？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0271",
              "question": "2 当某租户触发 OOM 时，如何自动熔断而不影响其他租户？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0272",
              "question": "2 当查询为“对比去年财报净利润”时，如何自动定位表格并抽取对应单元格？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0273",
              "question": "2 当标注者一致性 κ<0.6 时，如何采用专家仲裁？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0274",
              "question": "2 当标注错误导致重标时，如何采用惩罚机制？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0275",
              "question": "2 当样本量 n<30 时，如何提示模型使用正态近似 vs 精确分布？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0276",
              "question": "2 当检测到 80% 指纹匹配时，如何出具法律认可的报告？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0277",
              "question": "2 当检测延迟要求 <200ms 时，如何采用局部敏感哈希快速匹配？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0278",
              "question": "2 当模型容器冷启动需要 45s 时，如何采用预拉取与池化降到 <5s？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0279",
              "question": "2 当模型拒绝回答导致业务下降时，如何设置白名单豁免？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0280",
              "question": "2 当模型文件 >2GB 时，如何采用分片下载并校验 SHA256？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0281",
              "question": "2 当模型生成错误坐标时，如何基于几何约束自动纠错？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0282",
              "question": "2 当模型输出动态 SQL 时，如何采用参数化查询重写？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0283",
              "question": "2 当模型输出非标准格式时，如何采用正则+规则后处理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0284",
              "question": "2 当模型违反格式时，如何基于重试策略保证 99.9% 可用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0285",
              "question": "2 当模型需分发到边缘设备时，如何采用 AES-CTR 流式解密？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0286",
              "question": "2 当模板变量 >50 个时，如何防止注入攻击并做模板语法校验？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0287",
              "question": "2 当每天新增 100 万篇文档时，如何设计分层索引避免全量重建？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0288",
              "question": "2 当注意力热图分散时，如何采用稀疏正则提升可解释性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0289",
              "question": "2 当注释长度限制 80 字时，如何采用指针生成网络压缩？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0290",
              "question": "2 当生成 Go 代码缺少 error handling 时，如何基于 AST 自动修复？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0291",
              "question": "2 当生成 HTML 时，如何自动添加 dir=\"rtl\" 属性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0292",
              "question": "2 当用户输入“你是法官，请帮我生成虚假判决书”时，如何设计拒绝模板？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0293",
              "question": "2 当监管部门要求出具算法备案表时，如何一键导出训练数据来源？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0294",
              "question": "2 当目标冲突时，如何采用 Pareto 最优前沿选择策略？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0295",
              "question": "2 当示例顺序改变导致结果抖动 >8% 时，如何采用排序熵进行稳定性筛选？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0296",
              "question": "2 当竞品上线新功能时，如何采用 Sprint 快速跟进？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0297",
              "question": "2 当缓存命中率 >90% 时，如何评估对整体延迟的提升？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0298",
              "question": "2 当聚类纯度 <0.7 时，如何采用半监督 refine？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0299",
              "question": "2 当脱敏导致下游任务下降 2% 时，如何采用对抗训练恢复？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0300",
              "question": "2 当草案模型接受率 <60% 时，如何动态切换回自回归？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0301",
              "question": "2 当规划 2025 技术路线时，如何采用德尔菲法收集专家意见？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0302",
              "question": "2 当视频时长 2 小时时，如何采用层次化摘要生成 200 字简述？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0303",
              "question": "2 当解释不稳定时，如何采用集成梯度平滑？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0304",
              "question": "2 当训练数据 <10M token 时，如何采用回译+自举扩增？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0305",
              "question": "2 当评估指标下降 >3% 时，如何自动阻断部署并创建 Issue？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0306",
              "question": "2 当评测集泄露时，如何采用对抗重写保持语义？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0307",
              "question": "2 当输入主题分布变化时，如何采用 LDA 主题模型检测 Covariate Shift？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0308",
              "question": "2 当返回字段 >100 个时，如何用 protobuf FieldMask 实现按需返回？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0309",
              "question": "2 当遇到层间激活内存爆炸时，如何启用 selective activation recomputation？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0310",
              "question": "2 当遇到稀疏算子不支持时，如何采用稀疏-稠密混合计算？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0311",
              "question": "2 当采样率 16kHz 时，如何估算 Whisper-large-v3 的 RTF 并优化到 <0.3？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0312",
              "question": "2 当量化到 INT4 时，如何采用直通估计（STE）缓解梯度不匹配？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0313",
              "question": "2 当面对千万级日志时，如何用 Loki 索引并保留 30 天冷热分层？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0314",
              "question": "2 当预标注准确率 80% 时，如何采用不确定性采样送标？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0315",
              "question": "2 当预算仅允许标注 1000 条时，如何采用批量选择？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0316",
              "question": "2 推导⼀下 旋转位置编码 RoPE？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0317",
              "question": "2 敏感词库每日更新，如何构建 Aho-Corasick 自动机的增量更新策略？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0318",
              "question": "2 有哪些⼤模型使⽤ Grouped-query Attention？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0319",
              "question": "2 有哪些大模型使用 Grouped-query Attention？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0320",
              "question": "2 说一下 prefix LM 和 casualLM 的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0321",
              "question": "2 说一下 transformer 的模型架构和细节？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0322",
              "question": "2.1 如何进行 模型自我反省？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0323",
              "question": "2.2 模型自我反省 有哪些方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0324",
              "question": "20 分布式并⾏及显存优化技术并⾏技术有哪⼀些，都有什么特点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0325",
              "question": "21 显存优化技术有哪⼀些，都有什么特点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0326",
              "question": "22 常⻅的分布式训练框架哪⼀些，都有什么特点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0327",
              "question": "3 ALiBi (Attention with Linear Biases) 有什么优点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0328",
              "question": "3 Attention 变体有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0329",
              "question": "3 Eﬃcient Router 介绍？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0330",
              "question": "3 KL 散度与交叉熵的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0331",
              "question": "3 LangChain 中 Example Selectors 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0332",
              "question": "3 LangChain 如何链接多个组件处理⼀个特定的下游任务？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0333",
              "question": "3 RAG 如何优化索引结构？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0334",
              "question": "3 RAG 有哪些关键指标和能力？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0335",
              "question": "3 RAG 架构优化有哪些优化策略？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0336",
              "question": "3 Text generation inference 的 优点有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0337",
              "question": "3 baichuan-53B 如何进⾏ 搜索增强？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0338",
              "question": "3 vLLM 具有哪些特点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0339",
              "question": "3 vLLM 的 缺点有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0340",
              "question": "3 ⽬前 LLM推理框架 有 哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0341",
              "question": "3 为什么第⼀块卡的显存会占⽤的更多⼀些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0342",
              "question": "3 什么是 集体通信？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0343",
              "question": "3 什么是相对位置编码？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0344",
              "question": "3 介绍⼀下 Swish 计算公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0345",
              "question": "3 介绍一下 工具使用（tool use）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0346",
              "question": "3 在 32k 长上下文模型中，如何设计滑动窗口保证首尾信息不丢失？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0347",
              "question": "3 在大模型任务中，你用到 LoRA，讲一下 LoRA 实现原理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0348",
              "question": "3 大模型LLM的 训练目标 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0349",
              "question": "3 大模型训练loss突刺 如何解决？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0350",
              "question": "3 如何单元测试覆盖 200 个地区格式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0351",
              "question": "3 如何可视化三维奖励空间并帮助决策？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0352",
              "question": "3 如何基于 Docker 多版本镜像快速切换对比？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0353",
              "question": "3 如何基于 Inter-Annotator Agreement 动态调整送标策略？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0354",
              "question": "3 如何基于 NVIDIA Sparsity SDK 加速 2:4 结构化稀疏？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0355",
              "question": "3 如何基于 OTA 差分升级并降低 80% 流量？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0356",
              "question": "3 如何基于 Tucker 分解进一步压缩多头注意力？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0357",
              "question": "3 如何基于 WebRTC 实现双工语音对话并处理回声消除？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0358",
              "question": "3 如何基于 cgroups v2 对 GPU 时间片做公平调度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0359",
              "question": "3 如何基于 profiling 结果自动调整 micro-batch 数并减少气泡？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0360",
              "question": "3 如何基于主动学习优先选择最不确定样本？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0361",
              "question": "3 如何基于强化学习奖励模型自动选择最优窗口数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0362",
              "question": "3 如何基于强化学习奖励自动决定图文 token 比例？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0363",
              "question": "3 如何基于强化学习（RLPO）自动选择最佳示例并定义奖励函数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0364",
              "question": "3 如何基于数字水印追踪泄露权重来源？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0365",
              "question": "3 如何基于日志审计追踪违规内容并定位责任人？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0366",
              "question": "3 如何基于用户反馈优先级排序并 roadmap 可视化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0367",
              "question": "3 如何基于用户反馈在线微调压缩模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0368",
              "question": "3 如何基于用户接受率在线微调小模型提升草案命中率？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0369",
              "question": "3 如何基于用户点击反馈在线微调双塔模型并避免灾难性遗忘？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0370",
              "question": "3 如何基于语义相似度实时检测越狱（Jailbreak）提示并自动升级风控？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0371",
              "question": "3 如何对 100GB 训练镜像做分层构建并缓存 pip 依赖？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0372",
              "question": "3 如何将路线图可视化并定期 review？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0373",
              "question": "3 如何开源复现代码并吸引社区贡献？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0374",
              "question": "3 如何开源评估工具并支持社区提交新任务？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0375",
              "question": "3 如何撰写中国 AI 专利并加快审查？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0376",
              "question": "3 如何构建 trace-id 串联 Prompt→Model→Response 全链路？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0377",
              "question": "3 如何构建中文 100 篇超长（>50k 字）摘要数据集？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0378",
              "question": "3 如何构建前端组件支持用户交互式查看注意力权重？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0379",
              "question": "3 如何构建单元测试集并保证覆盖常见边界条件？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0380",
              "question": "3 如何构建可视化看板帮助开发者定位？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0381",
              "question": "3 如何构建多语言评测基准并避免文化偏见？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0382",
              "question": "3 如何构建差分隐私（ε=1）保证统计查询不泄露个体？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0383",
              "question": "3 如何构建自动化测试，确保 12 种语言在相同提示下输出格式一致？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0384",
              "question": "3 如何构建自动告警并触发回滚或重训？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0385",
              "question": "3 如何构建自动评分器并给出可解释报告？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0386",
              "question": "3 如何构建领域子集并报告置信区间？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0387",
              "question": "3 如何每季度演练并更新预案？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0388",
              "question": "3 如何生成公平性报告并提交监管？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0389",
              "question": "3 如何用 Kafka+FAISS 实现近实时（<5min）索引更新？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0390",
              "question": "3 如何用 Merkle Tree 对模型权重做完整性校验并防止篡改？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0391",
              "question": "3 如何用 Monte Carlo 模拟 ROI 分布并给出 95% 置信区间？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0392",
              "question": "3 如何用 NNAPI 在 Android SoC 上加速 INT8 推理并降低功耗 40%？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0393",
              "question": "3 如何用 North-Star Framework 对齐团队目标？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0394",
              "question": "3 如何用 allcontributors 机器人自动识别贡献者？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0395",
              "question": "3 如何用模型投票发现潜在错误标注？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0396",
              "question": "3 如何用模拟器预估在 8×A100 下最大并发请求数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0397",
              "question": "3 如何监控 entropy 崩溃并自动重启训练？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0398",
              "question": "3 如何监控各任务在验证集上的 loss 曲线并自动触发 Early Stopping？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0399",
              "question": "3 如何细分人群（年龄、地域）分析 NPS 差异？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0400",
              "question": "3 如何衡量大模型水平？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0401",
              "question": "3 如何计算提示模板变更对下游转化率提升的显著性（p-value）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0402",
              "question": "3 如何让 RAG 支持私有化多模态 RAG（文本+表格+图片）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0403",
              "question": "3 如何记录失败轨迹并用于后续微调提升成功率？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0404",
              "question": "3 如何设计 NEON 汇编优化 GELU 算子并提升 1.7× 速度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0405",
              "question": "3 如何设计区块链存证合约并保证上链哈希不可篡改？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0406",
              "question": "3 如何设计单元测试自动校验返回字段类型与范围？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0407",
              "question": "3 如何设计幂等性机制防止重复扣费？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0408",
              "question": "3 如何设计插件签名验证并防止恶意代码执行？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0409",
              "question": "3 如何设计用户举报闭环，将错误样本回流到微调集？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0410",
              "question": "3 如何设计用户界面展示冲突并收集反馈？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0411",
              "question": "3 如何设计自动切量脚本，支持按用户尾号灰度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0412",
              "question": "3 如何证明 KV-cache INT4 量化对 PPL 影响 <2% 并提供实验数据？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0413",
              "question": "3 如何评估 GPU 利用率 50% 时的成本最优副本数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0414",
              "question": "3 如何评估 QAT 与 PTQ 的精度-耗时权衡？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0415",
              "question": "3 如何评估 RTL 语言在生成摘要时的可读性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0416",
              "question": "3 如何评估在线 RLHF 对留存率的提升？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0417",
              "question": "3 如何评估多模态检索结果的相关性并构建人工评估协议？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0418",
              "question": "3 如何评估并行方案对训练吞吐的提升？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0419",
              "question": "3 如何评估弱监督标签对微调效果影响？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0420",
              "question": "3 如何评估投机解码在长文本生成场景下的端到端加速比？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0421",
              "question": "3 如何评估摘要的 ROUGE-L 与人工一致性（κ 值）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0422",
              "question": "3 如何评估生成注释的 ROUGE-L 与开发者人工一致性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0423",
              "question": "3 如何评估生成解释的教学有效性（学生满意度）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0424",
              "question": "3 如何评估蒸馏后在下游任务上的保留率？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0425",
              "question": "3 如何评估解释与人类标注的一致性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0426",
              "question": "3 如何评估长上下文检索召回率（LoongEval）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0427",
              "question": "3 如何通过输入查询与文档对齐提升 RAG 效果？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0428",
              "question": "3 如何采用 SVG 渲染并验证图形正确性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0429",
              "question": "3 如何采用 dropout ensemble 估计奖励不确定性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0430",
              "question": "3 如何采用一致性哈希做分布式缓存并防止热点倾斜？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0431",
              "question": "3 如何采用强化学习对抗训练提升模型鲁棒性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0432",
              "question": "3 如何采用课程学习逐步提升题目难度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0433",
              "question": "3 如何验证 alpha/r 比值在 2 与 0.5 时对收敛速度的影响？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0434",
              "question": "3 如何验证模型生成参数在合法范围内并给出错误提示？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0435",
              "question": "3 如何验证记忆机制对多轮对话一致性提升？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0436",
              "question": "3 如果显卡的显存不够装下一个完整的模型呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0437",
              "question": "3 对⽐⼀下 Multi-head Attention 和 Multi-Query Attention？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0438",
              "question": "3 对比一下 Multi-head Attention 和 Multi-Query Attention？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0439",
              "question": "3 当 30% 数据为机器生成时，如何设计对抗过滤器降低循环幻觉？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0440",
              "question": "3 当 CoT 长度超过 2048 token 时，如何压缩提示而不损失准确率？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0441",
              "question": "3 当 batch size=1 仍 OOM 时，如何结合梯度检查点与微批次累积？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0442",
              "question": "3 当合成数据规模达到 1M 条时，如何抽样做人工验证并控制 95% 置信区间？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0443",
              "question": "3 当调用量突增 10× 时，自托管方案与 API 方案的成本拐点如何计算？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0444",
              "question": "3 当遇到拆字、拼音、谐音变异时，如何基于 BERT-CRF 做敏感实体识别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0445",
              "question": "3 当遗忘率 >10% 时，如何采用 rehearsal buffer 进行回放并设置 buffer 大小？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0446",
              "question": "3 当音频采样率从 16 kHz 提升到 48 kHz 时，对端到端延迟的影响如何建模？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0447",
              "question": "3 旋转位置编码 RoPE 有什么优点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0448",
              "question": "3 模型大小如何选择？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0449",
              "question": "3 简单介绍一下，transformer 架构？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0450",
              "question": "3 给定 100 条业务 query，如何计算模型拒绝回答的可接受阈值？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0451",
              "question": "3 若基于 ChatGLM-6B 二次分发，需要向用户披露哪些最小信息集？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0452",
              "question": "3 训练框架如何选？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0453",
              "question": "3 说一下 Transformer 的架构和其内部细节？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0454",
              "question": "3 面对“成本+效果”双约束，如何构建一个多目标打分函数并给出权重设计示例？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0455",
              "question": "4 ALiBi (Attention with Linear Biases) 被哪些 LLMs 应⽤？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0456",
              "question": "4 BART、llama、gpt、t5、palm 等主流模型异同点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0457",
              "question": "4 LangChain 中 Output Parsers 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0458",
              "question": "4 LangChain 如何Embedding & vector store？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0459",
              "question": "4 Multi-Query Attention 这样做的好处是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0460",
              "question": "4 RAG 有哪些评估框架？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0461",
              "question": "4 RAG 索引优化有哪些优化策略？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0462",
              "question": "4 Text generation inference 的 缺点有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0463",
              "question": "4 instruction tuning 和 prompt learning 的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0464",
              "question": "4 vLLM ⽀持哪些 Huggingface 模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0465",
              "question": "4 vLLM 离线批量推理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0466",
              "question": "4 为什么 多机训练效率不如单机？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0467",
              "question": "4 介绍⼀下 使⽤ GLU 线性⻔控单元的 FFN 块 计算公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0468",
              "question": "4 介绍大模型推理过程中，可以通过调节哪些参数提高性能？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0469",
              "question": "4 大模型的模型架构有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0470",
              "question": "4 大模型评估方法 有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0471",
              "question": "4 如何通过提示压缩提升 RAG 效果？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0472",
              "question": "4 如何通过混合检索提升 RAG 效果？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0473",
              "question": "4 旋转位置编码 RoPE 被哪些 LLMs 应⽤？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0474",
              "question": "4 直接使⽤nn.DataParallel的时候，训练采⽤多卡训练，会出现⼀个warning？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0475",
              "question": "5 3种并行方式可以叠加吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0476",
              "question": "5 LangChain 中 Indexes and Retrievers 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0477",
              "question": "5 RAG 索引数据优化有哪些优化策略？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0478",
              "question": "5 Text generation inference 的 使⽤docker运⾏web server？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0479",
              "question": "5 chatGPT 对比 GPT-3 的性能提升主要来源于哪些方面？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0480",
              "question": "5 vLLM API Server？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0481",
              "question": "5 个人项目中模型的优化点和技术细节？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0482",
              "question": "5 为何现在的大模型大部分是Decoder only结构？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0483",
              "question": "5 介绍⼀下 使⽤ GeLU 的 GLU 块 计算公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0484",
              "question": "5 分类问题为什么用交叉熵损失函数不用均方误差（MSE）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0485",
              "question": "5 大模型评估工具 有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0486",
              "question": "5 如何通过 查询重写和扩展 提升 RAG 效果？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0487",
              "question": "5 如何通过重新排名提升 RAG 效果？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0488",
              "question": "5 数据并⾏ 如何 提升效率？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0489",
              "question": "5 有 哪些模型 是 使⽤ Multi-Query Attention？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0490",
              "question": "5 有哪些模型是使用 Multi-Query Attention？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0491",
              "question": "5 项目中你用到的大模型推理加速工具是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0492",
              "question": "6 LangChain 中 Chat Message History 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0493",
              "question": "6 tf32 格式有多长？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0494",
              "question": "6 个人项目中如何选择最佳的指令策略，以及其对模型效果的影响？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0495",
              "question": "6 介绍⼀下 使⽤ Swish 的 GLU 块 计算公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0496",
              "question": "6 大模型训练的三种并行是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0497",
              "question": "6 简单 介绍一下 大模型【LLMs】？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0498",
              "question": "7 LangChain 中 Agents and Toolkits 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0499",
              "question": "7 什么是 张量并⾏ (intra-layer)？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0500",
              "question": "7 哪里看各类显卡算力比较？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0501",
              "question": "7 大模型中常见的位置编码？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0502",
              "question": "7 并⾏ transformer block 介绍⼀下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0503",
              "question": "7 除了3D并行有没有其他方式大规模训练？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0504",
              "question": "8 softmax 和交叉熵损失怎么计算，二值交叉熵呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0505",
              "question": "8 大模型【LLMs】具有什么优点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0506",
              "question": "8 大模型高效参数微调方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0507",
              "question": "8 （torch profiler）如何查看自己的训练中通信开销？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0508",
              "question": "9 Bert 在 RAG 中具体是起到了一个什么作用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0509",
              "question": "9 大模型【LLMs】具有什么缺点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0510",
              "question": "9 如果 softmax 的 e 次方超过 float 的值了怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0511",
              "question": ":json)？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0512",
              "question": "<amount>\\d+\\.？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0513",
              "question": "CPU-offload，ZeRO-offload 了解？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0514",
              "question": "ChatGLM 类大模型，咋选？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0515",
              "question": "DDO 与 DPO 的区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0516",
              "question": "Decoder 区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0517",
              "question": "Deep Norm 思路？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0518",
              "question": "Deep Norm 有什么优点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0519",
              "question": "DeepSpeed 方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0520",
              "question": "FAQ 页面？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0521",
              "question": "GPT 和 BERT 在文本表征方面有哪些结构和工作原理上的差异？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0522",
              "question": "G），如何防止资源死锁？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0523",
              "question": "LLMs 各模型分别⽤了 哪种 Layer normalization？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0524",
              "question": "LLMs 各模型分别用了哪种 Layer normalization？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0525",
              "question": "LLMs 存在模型幻觉问题，请问如何处理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0526",
              "question": "LLMs 训练数据 和 数据量 对⽐如何？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0527",
              "question": "LLMs 训练时 有哪些有⽤的建议？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0528",
              "question": "LLaMA-adapter 如何实现稳定训练？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0529",
              "question": "LSH 与向量数据库如何协同？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0530",
              "question": "LangChain ⽀持哪些功能？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0531",
              "question": "LangChain 包含哪些 核⼼概念？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0532",
              "question": "LangChain 包含哪些核心概念？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0533",
              "question": "LangChain 包含哪些特点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0534",
              "question": "LangChain 如何使⽤？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0535",
              "question": "LangChain 如何使用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0536",
              "question": "LangChain 如何调用 LLMs 生成回复？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0537",
              "question": "LangChain 如何链接多个组件处理一个特定的下游任务？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0538",
              "question": "LangChain 存在哪些问题及⽅法⽅案？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0539",
              "question": "LangChain 存在哪些问题及方法方案？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0540",
              "question": "LangChain 支持哪些功能？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0541",
              "question": "LangChain 替代⽅案？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0542",
              "question": "LangChain 替代方案？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0543",
              "question": "Layer Norm 的计算公式写⼀下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0544",
              "question": "LightLLM ⽀持模型 LLMs 模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0545",
              "question": "LoRA 原理与使用技巧有那些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0546",
              "question": "LoRA 微调优点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0547",
              "question": "LoRA 权重合入 chatglm 模型的方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0548",
              "question": "Megatron-DeepSpeed 方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0549",
              "question": "Megatron-LM 方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0550",
              "question": "NPS 问卷是否也应双语？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0551",
              "question": "NT8 TOPS，如何再降 RTF？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0552",
              "question": "OCR 抽取效果不好，需要怎么排查问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0553",
              "question": "P-tuning 讲一下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0554",
              "question": "PPO（强化学习）的数据格式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0555",
              "question": "PT 与 SFT 的 epoch 比例？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0556",
              "question": "Prompt 是如何生成的，优化目标是什么，任务是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0557",
              "question": "QA 准确率 90%？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0558",
              "question": "Queue-Proxy削峰填谷？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0559",
              "question": "Qwen-72B 的推理延迟与首 token 时间？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0560",
              "question": "RAG prompt 模板如何构建？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0561",
              "question": "RAG 如何解决多实体提问问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0562",
              "question": "RAG 思路是怎么样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0563",
              "question": "RAG 核心技术是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0564",
              "question": "RAG 项目里面有哪一些亮点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0565",
              "question": "RAG(检索增强生成)对于大模型来说，有什么好处？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0566",
              "question": "RLHF 在实践过程中存在哪些不⾜？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0567",
              "question": "RLHF 的具体工程是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0568",
              "question": "RMS Norm 的计算公式写⼀下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0569",
              "question": "RMS Norm 相⽐于 Layer Norm 有什么特点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0570",
              "question": "RM（奖励模型）的数据格式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0571",
              "question": "RNN 与 GNN 之间有哪些区别，以及它们各自适用于哪些场景？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0572",
              "question": "SFT 指令微调数据 如何构建？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0573",
              "question": "SFT需要训练Token数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0574",
              "question": "SFT（有监督微调）的数据集格式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0575",
              "question": "Self-RAG 的推理过程？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0576",
              "question": "Self-RAG 的训练过程？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0577",
              "question": "Self-attention 的公式及参数量？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0578",
              "question": "Stable Diffusion API 同时满足图片审核+版权过滤？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0579",
              "question": "Stopping？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0580",
              "question": "TF 并优化到 <0.3？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0581",
              "question": "The generated prompt will be \"What is the capital of France？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0582",
              "question": "This code will send the question \"What is the capital of France？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0583",
              "question": "Transformer 在自然语言处理中有哪些应用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0584",
              "question": "ZeRO，零冗余优化器的三个阶段？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0585",
              "question": "\\s*[:=]\\s*['\"]？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0586",
              "question": "a. 给LLM一个简单的提示词“Steps for XYZ.\\n1.”，“What are the subgoals for achieving XYZ？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0587",
              "question": "assert re.match(r\"^\\d{6}(\\d{6})？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0588",
              "question": "baichuan-7B 如何 提⾼ 训练稳定性和吞吐？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0589",
              "question": "baichuan-7B 如何 收集原始数据并 构建 训练数据？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0590",
              "question": "based Learning做到 F1>90%？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0591",
              "question": "ch 推理延迟从 420 ms 降到 180 ms，如何保持矩阵统计特性不变？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0592",
              "question": "deepspeed 用过吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0593",
              "question": "dge Plugin）** 而非微调，实现**“可验证引用”**？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0594",
              "question": "e over Fabric，你能设计一套 双缓冲流水线 保证计算不空等吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0595",
              "question": "h-safe logging”**？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0596",
              "question": "head 数不一致怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0597",
              "question": "iners，但QPS损耗30%；如何在安全与性能之间做Trade-off？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0598",
              "question": "input_data = \"Hello, how are you？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0599",
              "question": "input_text = \"Hello, how are you？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0600",
              "question": "kenizer 并保证嵌入层初始化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0601",
              "question": "llama 输⼊句⼦⻓度理论上可以⽆限⻓吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0602",
              "question": "llama 输入句子长度理论上可以无限长吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0603",
              "question": "llama2 中使用的注意力机制是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0604",
              "question": "margin 值怎么调？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0605",
              "question": "memory padding 把冲突率降到 2% 以下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0606",
              "question": "metheus+Grafana 看板？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0607",
              "question": "metheus？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0608",
              "question": "nB模型推理需要多少显存？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0609",
              "question": "nB模型训练需要多少显存？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0610",
              "question": "n，如何设计增量召回 + 滑动窗口标注？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0611",
              "question": "on recomputation？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0612",
              "question": "pattern = re.compile(r\"^.？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0613",
              "question": "prefix Decoder 和 causal Decoder 和 Encoder-Decoder 区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0614",
              "question": "prefix LM 和 causal LM 区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0615",
              "question": "prompt tuning 和 prefix tuning 在微调上的区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0616",
              "question": "response = langchain.ask(\"What is the capital of France？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0617",
              "question": "rsion=？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0618",
              "question": "template = PromptTemplate(\"What is the capital of {country}？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0619",
              "question": "template.add_message(\"human\", \"Hello, how are you doing？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0620",
              "question": "template.add_message(\"human\", \"What is your name？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0621",
              "question": "template.set_message_content(3, \"What is your name？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0622",
              "question": "tf32格式有多⻓？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0623",
              "question": "transformer 中求和与归一化中“求和”是什么意思？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0624",
              "question": "tus='已支付', version=version+1 where order_id=？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0625",
              "question": "ube.com/watch？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0626",
              "question": "uery Expansion）提升？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0627",
              "question": "variate Shift？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0628",
              "question": "warmup 的步数对大模型继续预训练 是否有影响？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0629",
              "question": "“在{城市}落户政策中，{群体A}和{群体B}谁更受益？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0630",
              "question": "“差异>10%” 是否一定需要重采样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0631",
              "question": "“文化偏见”与“本地化合规”冲突时如何取舍？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0632",
              "question": "”或“氦的原子序数为什么是1？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0633",
              "question": "⼀、pytorch中的GPU操作默认是什么样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0634",
              "question": "⼀、torch.multiprocessing 函数介绍⼀下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0635",
              "question": "⼀、vLLM ⽤于⼤模型并⾏推理加速 存在什么问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0636",
              "question": "⼀、为什么 会 出现 ⼤模型幻觉？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0637",
              "question": "⼀、为什么 需要 适配器微调（Adapter-tuning）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0638",
              "question": "⼀、为什么需要 Deepspeed？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0639",
              "question": "⼀、为什么需要 FasterTransformer？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0640",
              "question": "⼀、为什么需要 Graph RAG？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0641",
              "question": "⼀、为什么需要 accelerate 分布式训练？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0642",
              "question": "⼀、为什么需要 提示学习（Prompting）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0643",
              "question": "⼀、为什么需要 构建中⽂tokenization？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0644",
              "question": "⼀、为什么需要对预训练模型进⾏指令微调？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0645",
              "question": "⼀、为什么需要进⾏继续预训练？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0646",
              "question": "⼀、什么是 DistributedDataParallel 核⼼ —— Ring-AllReduce？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0647",
              "question": "⼀、什么是 ⼤模型幻觉问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0648",
              "question": "⼀、什么是⼤模型幻觉？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0649",
              "question": "⼀、什么是思维链提示？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0650",
              "question": "⼀、流⽔线并⾏（Pipeline Parallelism） 优化⽬标是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0651",
              "question": "⼆、3D 并⾏ 策略有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0652",
              "question": "⼆、DeepSpeed 基本概念 介绍⼀下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0653",
              "question": "⼆、FasterTransformer 介绍⼀下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0654",
              "question": "⼆、LightLLM 介绍⼀下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0655",
              "question": "⼆、nn.parallel.DistributedDataParallel 函数 介绍⼀下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0656",
              "question": "⼆、torch.multiprocessing 函数如何使⽤？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0657",
              "question": "⼆、vLLM 如何 优化 ⼤模型并⾏推理加速？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0658",
              "question": "⼆、vLLM 性能如何？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0659",
              "question": "⼆、为什么 会 出现 ⼤模型幻觉问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0660",
              "question": "⼆、为什么LLM会产⽣幻觉？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0661",
              "question": "⼆、为什么需要⾃动混合精度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0662",
              "question": "⼆、什么是 Graph RAG？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0663",
              "question": "⼆、什么是 accelerate 分布式训练？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0664",
              "question": "⼆、什么是 提示学习（Prompting）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0665",
              "question": "⼆、介绍⼀下 nn.DataParallel 函数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0666",
              "question": "⼆、图解 流⽔线并⾏（Pipeline Parallelism）模型并⾏ 必要性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0667",
              "question": "⼆、基于LLM+向量库的⽂档对话 存在哪些痛点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0668",
              "question": "⼆、如何 缓解 ⼤模型幻觉？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0669",
              "question": "⼆、如何对 原始数据预处理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0670",
              "question": "⼆、如何对 继续预训练 数据预处理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0671",
              "question": "⼆、对预训练模型进⾏指令微调 数据 如何处理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0672",
              "question": "⼆、思维链提示本质是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0673",
              "question": "⼆、适配器微调（Adapter-tuning）思路？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0674",
              "question": "⼋、DistributedDataParallel(以下简称DDP) 缺点有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0675",
              "question": "⼋、LLMs什么时候最容易产⽣幻觉？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0676",
              "question": "⼋、MAM Adapter 特点 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0677",
              "question": "⼋、nn.DataParallel 函数 实战？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0678",
              "question": "⼋、如何使⽤ AMP混合精度训练？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0679",
              "question": "⼋、如何区分单栏还是双栏pdf？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0680",
              "question": "⼋、如何在已有LoRA模型上继续训练？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0681",
              "question": "⼋、思维链提示 对实现真正的通⽤⼈⼯智能仍⾯临哪些挑战？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0682",
              "question": "⼋、获取模型参数 介绍⼀下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0683",
              "question": "⼗⼀、如果需要你对 思维链提示 进⾏改进，你觉得你会改进哪些地⽅？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0684",
              "question": "⼗⼆、思维链提示 未来研究⽅向？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0685",
              "question": "⼗、LoRA这种微调⽅法和全参数⽐起来有什么劣势吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0686",
              "question": "⼗、你认为可以在哪些其他⽅⾯应⽤“思路链提示”这⼀思路来提升语⾔模型的能⼒？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0687",
              "question": "⼤模型LLM的 训练⽬标 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0688",
              "question": "⼤模型LLM的架构介绍？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0689",
              "question": "⼤模型LLM进⾏SFT 如何对样本进⾏优化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0690",
              "question": "⼤模型LLM进⾏SFT操作的时候在学习什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0691",
              "question": "⼤模型⼤概有多⼤，模型⽂件有多⼤？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0692",
              "question": "⼤模型⽣成时的参数怎么设置？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0693",
              "question": "⼤模型【LLMs】具有什么优点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0694",
              "question": "⼤模型【LLMs】具有什么缺点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0695",
              "question": "⼤模型【LLMs】后⾯跟的 175B、60B、540B等 指什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0696",
              "question": "⼤模型在gpu和cpu上推理速度如何？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0697",
              "question": "⼤模型怎么评测？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0698",
              "question": "⼤模型有推理能⼒吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0699",
              "question": "⼤模型的honest原则是如何实现的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0700",
              "question": "⼤模型评估⼯具 有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0701",
              "question": "⼤模型评估⽅法 有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0702",
              "question": "⽬前 主流的开源模型体系 有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0703",
              "question": "一、什么是 大模型（LLMs）agent？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0704",
              "question": "一、什么是大模型幻觉？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0705",
              "question": "一、什么是生成式大模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0706",
              "question": "一、知识蒸馏和无监督样本训练？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0707",
              "question": "一到同样的尺度下，只要position的相对位置保持不变就可以？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0708",
              "question": "七、 PagedAttention 源码介绍？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0709",
              "question": "七、DistributedDataParallel(以下简称DDP) 优点有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0710",
              "question": "七、LoRA微调⽅法为啥能加速训练？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0711",
              "question": "七、ZeRO Oﬄoad后的计算流程是怎么样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0712",
              "question": "七、nn.DataParallel 函数 缺点 介绍⼀下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0713",
              "question": "七、如何在PyTorch中使⽤⾃动混合精度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0714",
              "question": "七、如何提取 ⽂章标题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0715",
              "question": "七、如何缓解LLM幻觉？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0716",
              "question": "七、如何让大模型处理更长的文本？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0717",
              "question": "七、思维链提示 对推动语⾔模型复杂推理能⼒研究有哪些启发和影响？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0718",
              "question": "七、训练精度 介绍⼀下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0719",
              "question": "三、DeepSpeed 通信策略 介绍⼀下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0720",
              "question": "三、FasterTransformer 核⼼是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0721",
              "question": "三、Graph RAG 思路介绍？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0722",
              "question": "三、LightLLM 性能表现 介绍？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0723",
              "question": "三、accelerate 分布式训练 原理讲解？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0724",
              "question": "三、nn.DataParallel 函数 处理逻辑 介绍⼀下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0725",
              "question": "三、nn.parallel.DistributedDataParallel 函数 如何多卡加速训练？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0726",
              "question": "三、为什么需要 ZeRO？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0727",
              "question": "三、为什么需要解决LLM的幻觉问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0728",
              "question": "三、什么是 PagedAttention？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0729",
              "question": "三、介绍⼀下 共享CUDA张量？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0730",
              "question": "三、大模型（LLMs）agent 主要 利用了 大模型 哪些能力？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0731",
              "question": "三、如何 构建模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0732",
              "question": "三、如何 评估 ⼤模型幻觉问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0733",
              "question": "三、如何构建中⽂的词库？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0734",
              "question": "三、对预训练模型进⾏指令微调 tokenization 如何构建？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0735",
              "question": "三、思维链提示 与 标准的提示学习⽅法有什么不同？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0736",
              "question": "三、流⽔线并⾏（Pipeline Parallelism） 图解？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0737",
              "question": "三、混合精度训练的优点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0738",
              "question": "三、谈一下对模型量化的了解？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0739",
              "question": "下做动态降维？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0740",
              "question": "下，用合成数据纠偏并保证增量微调不偏航？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0741",
              "question": "与 P-tuning v2 区别在哪里？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0742",
              "question": "与 PP+TP+ZeRO-3 如何组合？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0743",
              "question": "与 REST 互通：使用 google.api.HttpRule 把 FieldMask 映射成？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0744",
              "question": "与首 token 时间？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0745",
              "question": "业内常用的分布式 AI 框架？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0746",
              "question": "为什么 Transformer 使用位置编码（Positional Encoding）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0747",
              "question": "为什么 Transformer 使用位置编码？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0748",
              "question": "为什么 需要 思维图 Graph of Thoughts（GOT）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0749",
              "question": "为什么 需要 思维算法 Algorithm of Thoughts（AOT）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0750",
              "question": "为什么SFT之后感觉LLM傻了？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0751",
              "question": "为什么⼤模型推理时显存涨的那么多还⼀直占着？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0752",
              "question": "为什么不用交叉熵而用 Margin？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0753",
              "question": "为什么会出现 LLMs 复读机问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0754",
              "question": "为什么大模型推理时显存涨的那么多还一直占着？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0755",
              "question": "为什么用多头？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0756",
              "question": "为什么要增量预训练？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0757",
              "question": "为什么需要 AMP混合精度训练？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0758",
              "question": "为什么需要 nn.parallel.DistributedDataParallel？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0759",
              "question": "为什么需要 思维树 Tree of Thoughts（TOT）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0760",
              "question": "为什么需要nn.DataParallel？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0761",
              "question": "为什么需要对大模型进行微调？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0762",
              "question": "为什么需要流⽔线并⾏（Pipeline Parallelism）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0763",
              "question": "为什么需要进行参选微调？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0764",
              "question": "为什么需要进行模型量化及原理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0765",
              "question": "为何现在的⼤模型⼤部分是Decoder only结构？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0766",
              "question": "九、如何提取表格和图⽚中的数据？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0767",
              "question": "九、如何通过增加模型规模来获得语⾔模型强⼤的思路链推理能⼒的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0768",
              "question": "了解 langchain 吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0769",
              "question": "了解半精度训练吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0770",
              "question": "二、为什么LLM会产生幻觉？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0771",
              "question": "二、大模型是怎么让生成的文本丰富而不单调的呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0772",
              "question": "二、大模型（LLMs）agent 有哪些部分组成？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0773",
              "question": "二、对知识蒸馏知道多少，有哪些改进用到了？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0774",
              "question": "五、 PagedAttention 技术细节？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0775",
              "question": "五、DeepSpeed 代码实现？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0776",
              "question": "五、LightLLM 如何安装？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0777",
              "question": "五、ZeRO 显存如何分配？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0778",
              "question": "五、nn.DataParallel 函数 参数更新⽅式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0779",
              "question": "五、nn.parallel.DistributedDataParallel 参数更新介绍⼀下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0780",
              "question": "五、vLLM 如何使⽤？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0781",
              "question": "五、⽤ 示例 介绍 Graph RAG？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0782",
              "question": "五、什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型，咋选？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0783",
              "question": "五、你了解的知识蒸馏模型有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0784",
              "question": "五、如何 ⻓⽂档（书籍）中关键信息？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0785",
              "question": "五、如何合并英⽂词表和中⽂词表？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0786",
              "question": "五、如何给LLM注入领域知识？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0787",
              "question": "五、幻觉有哪些不同类型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0788",
              "question": "五、思维链提示 适⽤场景 有 哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0789",
              "question": "五、是否可以结合 其他库 使⽤？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0790",
              "question": "五、混合精度训练的关键技术是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0791",
              "question": "交叉编译后的 ARM 环境？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0792",
              "question": "什么情况⽤Bert模型，什么情况⽤LLaMA、ChatGLM类⼤模型，咋选？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0793",
              "question": "什么情况用 Bert 模型，什么情况用 LLaMA、ChatGLM 类大模型，咋选？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0794",
              "question": "什么情况用 Bert 模型，什么情况用 LLaMA、ChatGLM 类大模？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0795",
              "question": "什么是 LLMs 复读机问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0796",
              "question": "什么是 LangChain Agent？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0797",
              "question": "什么是 LangChain model？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0798",
              "question": "什么是 LangChain？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0799",
              "question": "什么是 思维图 Graph of Thoughts（GOT）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0800",
              "question": "什么是 思维树 Tree of Thoughts（TOT）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0801",
              "question": "什么是 思维链 Chain-of-Thought（COT）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0802",
              "question": "介绍⼀下 PEFT？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0803",
              "question": "介绍一下 FFN 块 计算公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0804",
              "question": "介绍一下 GeLU 计算公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0805",
              "question": "介绍一下 RAG 原理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0806",
              "question": "介绍一下 Swish 计算公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0807",
              "question": "介绍一下 使用 Swish 的 GLU 块 计算公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0808",
              "question": "介绍一下使用 GLU 线性门控单元的 FFN 块 计算公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0809",
              "question": "介绍一下使用 GeLU 的 GLU 块 计算公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0810",
              "question": "令 A、B，让同一位标注者同时阅读，强制二选一：哪条指令更难？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0811",
              "question": "以下 commit 是否包含代码、测试、数据或提示词贡献？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0812",
              "question": "以根号 d？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0813",
              "question": "以用来处理序列中的位置信息？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0814",
              "question": "任务间共享底层 Transformer 时，如何防止某任务 loss 爆炸拖垮整体？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0815",
              "question": "优 pp×tp×dp 组合？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0816",
              "question": "优化模型性能？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0817",
              "question": "优点与缺点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0818",
              "question": "位置编码有哪些优缺点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0819",
              "question": "低功耗 40%？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0820",
              "question": "何不重新训练大模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0821",
              "question": "何在不牺牲精度的前提下实现流式增量渲染？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0822",
              "question": "何改造金字塔？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0823",
              "question": "你了解baichuan-7B解构么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0824",
              "question": "你如何解决这些挑战？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0825",
              "question": "你如何评估大模型的性能？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0826",
              "question": "你最常用哪些方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0827",
              "question": "使用不超过 mm 复杂度的代码求解其两两之间的欧式距离？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0828",
              "question": "使用什么向量数据库？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0829",
              "question": "使用外挂知识库主要是为了解决什么问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0830",
              "question": "保证 NDCG@10 下降 <1%？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0831",
              "question": "保证标注一致性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0832",
              "question": "做敏感实体识别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0833",
              "question": "八、LLMs什么时候最容易产生幻觉？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0834",
              "question": "公开数据+合成数据在 1 个 Sprint 内逼近效果？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0835",
              "question": "六、 PagedAttention 如何 实现安全共享？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0836",
              "question": "六、Graph RAG 排序优化⽅式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0837",
              "question": "六、LightLLM 如何使⽤？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0838",
              "question": "六、ZeRO 优化策略是怎么样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0839",
              "question": "六、nn.DataParallel 函数 优点 介绍⼀下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0840",
              "question": "六、nn.DataParallel(以下简称DP) vs DistributedDataParallel(以下简称DDP)介绍⼀下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0841",
              "question": "六、为什么要提取标题甚⾄是多级标题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0842",
              "question": "六、介绍⼀下 混合精度训练 动态损失缩放？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0843",
              "question": "六、各个专业领域是否需要各自的大模型来服务？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0844",
              "question": "六、如何度量幻觉？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0845",
              "question": "六、常见LLM Agent框架或者应用 有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0846",
              "question": "六、怎么使⽤修改后的词表？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0847",
              "question": "六、思维链提示 ⽬前还存在哪些不⾜点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0848",
              "question": "关联**，既满足审计又避免隐私泄露？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0849",
              "question": "其实已经不错了，但还能做得更好吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0850",
              "question": "典型取值区间，否则会被追问“你这些数从哪来的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0851",
              "question": "内导出完整证据链？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0852",
              "question": "写⼀下 Deep Norm 代码实现？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0853",
              "question": "况下30分钟内热修复？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0854",
              "question": "减少 30% token？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0855",
              "question": "几种主流大模型的 loss 了解过吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0856",
              "question": "分布式训练框架选择？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0857",
              "question": "分片式 Fisher 信息矩阵 才能不爆内存？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0858",
              "question": "到 1 B 以内模型，F1 下降≤2%？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0859",
              "question": "到Flink 实时流，并用KLL sketch 解决窗口内样本稀疏？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0860",
              "question": "制 95% 置信区间？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0861",
              "question": "力图**，在千亿参数模型上如何秒级抽取并脱敏展示？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0862",
              "question": "加速卡如何选择？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0863",
              "question": "动升级风控？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0864",
              "question": "动态窗口：能否让模型自己学窗口大小？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0865",
              "question": "动态阈值：如何让规则随监管政策自动刷新？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0866",
              "question": "动机：在 微调大模型时，首先需要解决的问题是“选取和构建大模型微调数据”，那如何选择呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0867",
              "question": "助力 B 租户模型提升，满足**《数据跨境传输安全评估办法》**？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0868",
              "question": "包含了哪几个模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0869",
              "question": "化到 INT4 后的性能损失？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0870",
              "question": "化协议，占位符统一用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0871",
              "question": "化触发阈值？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0872",
              "question": "区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0873",
              "question": "千亿模型 ckpt 200 GB，网络拷贝耗时 10 分钟，如何做到秒级回滚？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0874",
              "question": "升 batch 吞吐？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0875",
              "question": "压到 1%，而生成模型往往掉 3% 以上？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0876",
              "question": "参数微调的原因有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0877",
              "question": "句子、语义段、之间召回不会有包含关系吗，是否会造成冗余？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0878",
              "question": "另一点，就是为什么会一直是一个词L的反复重复？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0879",
              "question": "可删除要求？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0880",
              "question": "可微几何约束能否直接写进Transformer loss？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0881",
              "question": "可能带来什么问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0882",
              "question": "各 LLMs 都使用哪种激活函数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0883",
              "question": "各LLMs 都使⽤哪种激活函数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0884",
              "question": "各个专业领域是否需要各⾃的⼤模型来服务？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0885",
              "question": "各个专业领域是否需要各自的大模型来服务？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0886",
              "question": "各有什么优缺点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0887",
              "question": "各自优点与区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0888",
              "question": "同方法的差别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0889",
              "question": "同时降低模型大小和推理时间。你是否有过使用或开发大模型的经验？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0890",
              "question": "向量库有那些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0891",
              "question": "向量数据库有那些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0892",
              "question": "呼吸。狗是一种动物。那么狗会呼吸吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0893",
              "question": "哪⾥看各类显卡算⼒⽐较？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0894",
              "question": "哪些因素会影响内存使用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0895",
              "question": "商业模型比如ChatGPT和Claude到底是怎么做的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0896",
              "question": "四、 PagedAttention 如何存储 连续的key和value？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0897",
              "question": "四、DeepSpeed 如何使⽤？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0898",
              "question": "四、FasterTransformer 优化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0899",
              "question": "四、LightLLM 依赖包 有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0900",
              "question": "四、LoRA权重是否可以合⼊原模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0901",
              "question": "四、ZeRO 的 核⼼思想是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0902",
              "question": "四、accelerate 分布式训练 如何实践？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0903",
              "question": "四、nn.DataParallel 函数 常⻅问题及解答 有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0904",
              "question": "四、nn.parallel.DistributedDataParallel 实现流程介绍⼀下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0905",
              "question": "四、vLLM 如何安装？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0906",
              "question": "四、⽤代码 介绍 Graph RAG？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0907",
              "question": "四、介绍⼀下 共享策略？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0908",
              "question": "四、如何 使⽤模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0909",
              "question": "四、如何 缓解 ⼤模型幻觉问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0910",
              "question": "四、如何使⽤transformers库加载sentencepiece模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0911",
              "question": "四、对预训练模型进⾏指令微调 模型 如何构建？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0912",
              "question": "四、幻觉⼀定是有害的吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0913",
              "question": "四、幻觉一定是有害的吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0914",
              "question": "四、思维链提示 为什么可以提⾼语⾔模型的复杂推理能⼒？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0915",
              "question": "四、提示学习（Prompting）有哪些⽅法，能不能稍微介绍⼀下它们间？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0916",
              "question": "四、模型压缩和加速的方法有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0917",
              "question": "四、流⽔线并⾏（Pipeline Parallelism）优缺点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0918",
              "question": "四、混合精度训练的缺点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0919",
              "question": "四、结合 代码 讲解 大模型（LLMs）agent 思路？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0920",
              "question": "因为说了 BERT 好训练一些，问了为什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0921",
              "question": "团队分布北上深杭四地，如何确保 review 不流于形式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0922",
              "question": "国产化算力瓶颈怎么破？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0923",
              "question": "国内 内容安全审核要求 200 ms 内返回首字，如何与 RTF<0.3 共存？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0924",
              "question": "国内合规要求训练日志留痕，selective 策略动态调整时如何审计重算层？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0925",
              "question": "国内机房夜间断网演练导致监控中心失联，如何防止误停？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0926",
              "question": "国内算力资源紧张，如何降低社区重复跑基座模型的成本？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0927",
              "question": "国内运营商5G 消息网关对时延要求 ≤1 s，如何进一步压缩降级路径？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0928",
              "question": "国密算法性能在 GPU 推理节点上可能成为瓶颈，怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0929",
              "question": "图谱做数据增强，同时保证医疗广告法合规？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0930",
              "question": "在做 RAG 项目过程中遇到哪些问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0931",
              "question": "在初始预训练中使用 Rewarmup 对大模型继续预训练性能影响？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0932",
              "question": "在处理多语言文本时，Tokenizer 会遇到哪些挑战？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0933",
              "question": "在多模态对话里，图片 token 的熵如何定义？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0934",
              "question": "在大模型中，除了位置编码，还有哪些方法可以用来处理序列中的位置信息？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0935",
              "question": "在大模型性能评估中，你通常使用哪些评估指标？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0936",
              "question": "在大模型设计中，如何权衡模型的复杂度和性能？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0937",
              "question": "在大模型评测中，你如何进行特征选择和模型调优？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0938",
              "question": "在季度复盘中触发根因分析（数据分布漂移？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0939",
              "question": "在实际应用中，如何调整注意力机制的参数以优化模型性能？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0940",
              "question": "在开发大模型时，你如何确保模型的可解释性和公平性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0941",
              "question": "在模型训练和推理过程中，如何保证 Tokenizer 的一致性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0942",
              "question": "在线插入已有前沿而不重新跑全量实验？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0943",
              "question": "在进行大模型微调时，有哪些常见的策略或技巧？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0944",
              "question": "基于LLM+向量库的⽂档对话 prompt 模板 如何构建？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0945",
              "question": "基于LLM+向量库的⽂档对话 思路是怎么样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0946",
              "question": "基于LLM+向量库的⽂档对话 核⼼技术是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0947",
              "question": "增哪些风险维度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0948",
              "question": "增量参数量？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0949",
              "question": "增量预训练 一般需要多大数据量？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0950",
              "question": "增量预训练 所⽤ 训练框架？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0951",
              "question": "增量预训练 所用 训练框架？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0952",
              "question": "增量预训练 训练流程 是怎么样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0953",
              "question": "增量预训练 过程中，loss 上升正常么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0954",
              "question": "增量预训练 过程中，lr 如何设置？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0955",
              "question": "增量预训练 过程中，warmup_ratio 如何设置？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0956",
              "question": "声，该如何解决这样的问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0957",
              "question": "外一个是数据的不确定性。这样讲是比较抽象的概念，那我们在大模型实践中如何体现呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0958",
              "question": "多业务共用一套大模型底座时，如何设计**“复合北极星”？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0959",
              "question": "多头注意力机制（Multi-head Attention）是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0960",
              "question": "多头自注意力机制的作用是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0961",
              "question": "多样性阈值能否动态调？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0962",
              "question": "多模态参数：图片里嵌了非法二维码，如何校验？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0963",
              "question": "多模态扩展：如果输入的是时序图 + 样本量，先用视觉模型抽“n=？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0964",
              "question": "多模态贡献怎么识别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0965",
              "question": "多模态边界：当输入出现图片+文字时，如何定义单元？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0966",
              "question": "多模态长链如何校验？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0967",
              "question": "多源坐标系混用（WGS-84、GCJ-02、BD-09）时，如何统一约束？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0968",
              "question": "多租户场景下，如何做到租户级告警隔离？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0969",
              "question": "多轮对话任务如何微调模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0970",
              "question": "大模型 (LLMs) 评测有那些方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0971",
              "question": "大模型 LLM 进行 SFT 如何对样本进行优化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0972",
              "question": "大模型 LLM 进行 SFT 操作的时候在学习什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0973",
              "question": "大模型中的优化算法有哪些常见的选择？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0974",
              "question": "大模型中的注意力机制是如何工作的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0975",
              "question": "大模型可控性如何实现，怎么保证可控性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0976",
              "question": "大模型在 GPU 和 CPU 上推理速度如何？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0977",
              "question": "大模型大概有多大，模型文件有多大？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0978",
              "question": "大模型应用框架及其功能？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0979",
              "question": "大模型恢复后，如何防止上下文断裂导致用户体验跳变？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0980",
              "question": "大模型时代，BERT 向量是否还够用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0981",
              "question": "大模型有推理能力吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0982",
              "question": "大模型生成时的参数怎么设置？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0983",
              "question": "大模型词表扩充的方法及工具？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0984",
              "question": "大模型输出内容哈希上链后，如何防止“先上链后篡改”的链下文件掉包？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0985",
              "question": "大模型进行训练，用的是什么框架？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0986",
              "question": "失效，如何引入 CLIP 或 BLIP 做跨模态推荐，并与现有标签体系对齐？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0987",
              "question": "奖励模型需要和基础模型⼀致吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0988",
              "question": "如何 对 baichuan-13B 进⾏微调？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0989",
              "question": "如何 对 baichuan-13B 进⾏推理和部署？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0990",
              "question": "如何与LLMOps 平台（如自研或阿里云 PAI、百度百舸、华为 ModelArt？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0991",
              "question": "如何与推理加速联动？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0992",
              "question": "如何优化 Transformer 模型的性能？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0993",
              "question": "如何使⽤ LangChain？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0994",
              "question": "如何使用 LangChain？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0995",
              "question": "如何在 80% 置信度下，补足剩余 20% 不确定性，使报告达到“高度盖然？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0996",
              "question": "如何在百亿级向量、天级更新的 LLMOps 管线里落地。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0997",
              "question": "如何在线动态调整 buffer 大小，兼顾成本与效果。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0998",
              "question": "如何处理大模型训练过程中的梯度消失或梯度爆炸问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_0999",
              "question": "如何处理大模型训练过程中的梯度消失或梯度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1000",
              "question": "如何实现窗口上下文检索？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1001",
              "question": "如何对抗“隐蔽矛盾”？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1002",
              "question": "如何对齐流与批的事件时间窗口？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1003",
              "question": "如何将外部知识注入大模型，最直接的方法：利用外部知识对大模型进行微调。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1004",
              "question": "如何把代理指标嵌入LLMOps 闭环，实现自动回滚、熔断、微调触发，而？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1005",
              "question": "如何把模型概率输出转化为**符合《电子数据若干规定》《个人信息保护？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1006",
              "question": "如何把贡献者数据喂给大模型做价值观对齐？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1007",
              "question": "如何提升大模型的检索效果？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1008",
              "question": "如何构建关键信息？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1009",
              "question": "如何构建轻量级 rehearsal buffer（国内常受限于 GPU 配额与数据合？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1010",
              "question": "如何查看多机训练时的⽹速？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1011",
              "question": "如何查看对deepspeed的环境配置是否正确？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1012",
              "question": "如何查看服务器上显卡的具体型号？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1013",
              "question": "如何查看服务器上的多卡之间的NVLINK topo？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1014",
              "question": "如何查看训练时的flops？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1015",
              "question": "如何满足**《生成式 AI 服务管理暂行办法》**对可追溯、可撤销、可红？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1016",
              "question": "如何给LLM注⼊领域知识？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1017",
              "question": "如何缓解 LLMs 复读机问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1018",
              "question": "如何衡量⼤模型⽔平？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1019",
              "question": "如何衡量大模型的效果？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1020",
              "question": "如何解决 PPO 的训练过程同时存在4个模型（2训练，2推理），对计算资源的要求较⾼ 问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1021",
              "question": "如何解决 ⼈⼯产⽣的偏好数据集成本较⾼，很难量产问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1022",
              "question": "如何解决三个阶段的训练（SFT->RM->PPO）过程较⻓，更新迭代较慢问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1023",
              "question": "如何解决三个阶段的训练（SFT->RM->PPO）过程较长，更新？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1024",
              "question": "如何解决的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1025",
              "question": "如何让 RAG 支持多模态数据格式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1026",
              "question": "如何让⼤模型处理更⻓的⽂本？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1027",
              "question": "如何让⼤模型输出合规化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1028",
              "question": "如何让大模型处理更长的文本？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1029",
              "question": "如何让大模型输出合规化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1030",
              "question": "如何让德尔菲结论持续保鲜？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1031",
              "question": "如何训练⾃⼰的⼤模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1032",
              "question": "如何训练自己的大模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1033",
              "question": "如何设计低成本存储？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1034",
              "question": "如何设计等效语义量表避免文化偏差？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1035",
              "question": "如何评价 RAG 项目的效果好坏，即指标是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1036",
              "question": "如何评估你的显卡利⽤率？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1037",
              "question": "如何评估你的显卡利用率？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1038",
              "question": "如何选取和构建大模型微调数据？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1039",
              "question": "如何重新排序？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1040",
              "question": "如何量化“不良图文合成”的拦截率？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1041",
              "question": "如何防止模型把正例 score 打爆到 1 之后梯度消失？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1042",
              "question": "如有能介绍一下区别么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1043",
              "question": "如果下季度业务方突然要求支持长文本 128K 上下文，如何重写 KR？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1044",
              "question": "如果业务方要求 ε=0.1 但延迟只能增加 2 ms，该如何权衡？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1045",
              "question": "如果业务要求**准确率 93%**但模型必须<800 M，如何再压缩？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1046",
              "question": "如果公司 2025 预算砍半，如何用德尔菲做减法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1047",
              "question": "如果公司处于**“从 0 到 1 探索阶段”，尚无成交数据，如何定北极星？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1048",
              "question": "如果就是想要试试65b模型，但是显存不多怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1049",
              "question": "如果想要在某个模型基础上做全参数微调，究竟需要多少显存？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1050",
              "question": "如果想要快速体验各种模型，该怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1051",
              "question": "如果有，能介绍⼀下区别么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1052",
              "question": "如果未来业务目标从“降本”升级为“营销转化”，是否仍沿用“问题解决率”？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1053",
              "question": "如果模型实际合规，我能否把“误报”控制在 5% 以内？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1054",
              "question": "如果白名单膨胀到10 万条以上，配置中心性能瓶颈如何解决？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1055",
              "question": "如果知识库本身过期或矛盾，如何给知识置信度再打分？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1056",
              "question": "如果社区提交的是多模态任务（图文混合），如何复用现有 YAML 描述？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1057",
              "question": "如果院方只给10 条私有种子，如何快速扩增？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1058",
              "question": "如果集团要求**“双碳”指标**怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1059",
              "question": "如果面试官追问“对方专利是基础算法，无法绕开怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1060",
              "question": "如果面试官追问“数据合成比例继续提高，会不会导致模型崩溃？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1061",
              "question": "字段动态膨胀到 1000+ 怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1062",
              "question": "学习到更多的知识？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1063",
              "question": "学习率大小对大模型继续预训练 后 上下游任务影响？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1064",
              "question": "它在大模型中起到了什么作用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1065",
              "question": "它的优势在哪⾥？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1066",
              "question": "它相比单头注意力有什么优势？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1067",
              "question": "完成近似去重？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1068",
              "question": "容，你如何在一周内完成全量历史解释的回扫与过滤？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1069",
              "question": "对于查询新闻标题的情况，可以使用\"请问有关于XXX的新闻吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1070",
              "question": "对位置编码熟悉吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1071",
              "question": "对分布式训练有经验么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1072",
              "question": "并绘制边际收益曲线？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1073",
              "question": "并设置 buffer 大小？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1074",
              "question": "广播与优先级队列，避免GPU资源死锁？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1075",
              "question": "库版本+Chunk-ID也纳入同一 Request-ID 证据链？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1076",
              "question": "建议的软件环境是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1077",
              "question": "开源的 RAG 框架有哪些，你比较了解？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1078",
              "question": "强制判为幻觉？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1079",
              "question": "强化学习微调（RLHF）能否替代重试？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1080",
              "question": "当前优化模型最主要技术⼿段有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1081",
              "question": "当前优化模型最主要技术手段有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1082",
              "question": "当北极星指标出现**“短期与长期冲突”时如何取舍？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1083",
              "question": "当生成解释引入多模态（图文混排）后，满意度评估指标需要如何升级？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1084",
              "question": "微调⽅法批处理⼤⼩模式GPU显存速度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1085",
              "question": "微调⽅法是啥？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1086",
              "question": "微调后的模型出现能⼒劣化，灾难性遗忘是怎么回事？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1087",
              "question": "微调后的模型出现能力劣化，灾难性遗忘是怎么回事？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1088",
              "question": "微调大模型时, batch size 如何设置问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1089",
              "question": "微调大模型时, 优化器如何？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1090",
              "question": "微调大模型时，如果 batch size 设置太小 会出现什么问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1091",
              "question": "微调模型需要多⼤显存？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1092",
              "question": "微调模型需要多大显存？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1093",
              "question": "微调需要多少条数据？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1094",
              "question": "忘通用能力？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1095",
              "question": "怎么解决的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1096",
              "question": "怎么让英⽂⼤语⾔模型⽀持中⽂？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1097",
              "question": "思维图 Graph of Thoughts（GOT）核⼼思想是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1098",
              "question": "思维树 Tree of Thoughts（TOT）涉及问题有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1099",
              "question": "思维算法 Algorithm of Thoughts（AOT） vs 其他 COT 的 区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1100",
              "question": "思维算法 Algorithm of Thoughts（AOT）思路是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1101",
              "question": "思维链 Chain-of-Thought（COT） 有哪些 局限性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1102",
              "question": "思维链 Chain-of-Thought（COT） 有哪些 应⽤场景？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1103",
              "question": "思维链 Chain-of-Thought（COT）存在问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1104",
              "question": "思维链 Chain-of-Thought（COT）是思路是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1105",
              "question": "总结⼀下 构建中⽂tokenization？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1106",
              "question": "想让模型学习某个领域或⾏业的知识，是应该预训练还是应该微调？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1107",
              "question": "想让模型学习某个领域或行业的知识，是应该预训练还是应该微调？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1108",
              "question": "想让模型学习某领域或行业知识，是应该预训练还是应该微调？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1109",
              "question": "或开发大模型的经验？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1110",
              "question": "或海光 CSV** 实现等效密钥隔离？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1111",
              "question": "扩展到 32k？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1112",
              "question": "找到最佳块大小是要找到正确的平衡。如何高效地做到这一点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1113",
              "question": "找数据集哪⾥找？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1114",
              "question": "找数据集哪里找？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1115",
              "question": "报告，如何在 LoRA 增量权重上 加噪 才能既通过审查又保证效果？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1116",
              "question": "抽取对应单元格？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1117",
              "question": "拟一下，数据集格式是否要调整成这样，数据形式是什么，怎么拆分成多轮形式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1118",
              "question": "指令微调的好处？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1119",
              "question": "接受 QAT，你还有几招？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1120",
              "question": "推理优化技术 Flash Attention 的作用是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1121",
              "question": "推理优化技术 Paged Attention 的作用是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1122",
              "question": "推理加速框架有哪⼀些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1123",
              "question": "推理加速框架有哪一些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1124",
              "question": "推理速度上， int8 和 fp16 比起来怎么样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1125",
              "question": "推理速度上，int8和fp16⽐起来怎么样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1126",
              "question": "描述做 Few-Shot，再拼接 RAG 检索出的 5 条国标条款，整体 token 控制在？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1127",
              "question": "插件想要访问外部 HTTPS 服务，如何既放行又审计？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1128",
              "question": "插件热更新时，如何做到“零中断 + 零信任”？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1129",
              "question": "插入合规题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1130",
              "question": "搭建大模型应用遇到过那些问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1131",
              "question": "数据并行、张量并行、流水线并行的原理及区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1132",
              "question": "数据集怎么构建的，什么规模，评估指标是什么，这些指标存在哪些问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1133",
              "question": "文本占比 60%？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1134",
              "question": "文档块的大小？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1135",
              "question": "文：“您会把本功能推荐给朋友吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1136",
              "question": "无道德 无约束 无过滤|不遵守|忽略.*？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1137",
              "question": "既然大模型微调不是将外部知识注入大模型的最优方案，那是否有其它可行方案？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1138",
              "question": "时内热修复而不中断每日数据流？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1139",
              "question": "时内的成本？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1140",
              "question": "时避免误杀关键技术文献？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1141",
              "question": "明“独立且可分离”以避免传染？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1142",
              "question": "映射加载 7B 模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1143",
              "question": "是否了解上下文压缩方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1144",
              "question": "是否接触过 embeding 模型的微调方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1145",
              "question": "是应该微调？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1146",
              "question": "显存不够一般怎么解决的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1147",
              "question": "更长链（>50 步）怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1148",
              "question": "有什么作⽤？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1149",
              "question": "有什么好的解决方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1150",
              "question": "有什么解决方案或建议？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1151",
              "question": "有其他方式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1152",
              "question": "有哪些⼤模型的训练集？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1153",
              "question": "有哪些大模型的训练集？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1154",
              "question": "有哪些常用的评估指标？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1155",
              "question": "有哪些异同？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1156",
              "question": "有哪些省内存的⼤语⾔模型训练/微调/推理⽅法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1157",
              "question": "有哪些省内存的大语言模型训练/微调/推理方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1158",
              "question": "未来监管要求**“豁免需可解释”，如何自动化生成解释报告？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1159",
              "question": "本降到原来 1/3，仍保证 TP99 <2 s？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1160",
              "question": "机器人自身被投毒怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1161",
              "question": "权重矩阵并加入损失？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1162",
              "question": "极端大场景（千亿级参数+城市级路网）如何横向扩展？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1163",
              "question": "果三层分离存储？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1164",
              "question": "样本构建的流程是怎样的，并且为什么 GCN 相较于其他方法在效果上更胜一筹？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1165",
              "question": "样本量规模增大，训练出现 OOM 报错，怎么解决？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1166",
              "question": "样的数据呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1167",
              "question": "根号 dk？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1168",
              "question": "梯度噪声过大怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1169",
              "question": "梯度检查点把 128K 模型评估耗时压到 30 分钟以内？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1170",
              "question": "槛，而国内是5% SPD，如何一套代码同时满足两套监管？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1171",
              "question": "模型⼤⼩如何选择？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1172",
              "question": "模型中起到了什么作用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1173",
              "question": "模型参数微调的方式有那些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1174",
              "question": "模型参数迭代实验步骤？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1175",
              "question": "模型如何判断回答的知识是训练过的已知的知识，怎么训练这种能⼒？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1176",
              "question": "模型如何自适应？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1177",
              "question": "模型底座是什么，这些不同底座什么区别，什么规模？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1178",
              "question": "模型推理是怎么做的，有没有 cot，tot 等等，还是单轮？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1179",
              "question": "模型训练的数据集问题：一般数据集哪里找？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1180",
              "question": "模型遗忘通用能力？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1181",
              "question": "模型部署的平台，推理效率怎么样，如何提升推理效率？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1182",
              "question": "模态，文本塔用 LoRA，ID 塔用 HashTable 查表，实现分钟级热插拔？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1183",
              "question": "比较常见）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1184",
              "question": "注意力机制中计算注意力分数时为什么会除以根号 dk？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1185",
              "question": "注意力机制如何解决长序列依赖问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1186",
              "question": "注意力机制是如何工作的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1187",
              "question": "注：以上算数不够直观，举个例子？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1188",
              "question": "涌现能⼒是啥原因？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1189",
              "question": "涌现能力是啥原因？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1190",
              "question": "涨，如何继续保证 100 ms？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1191",
              "question": "混合云架构能否降低合规溢价？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1192",
              "question": "混合精度训练的优点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1193",
              "question": "溃，NVIDIA MIG+nvidia-container-cli能否复用同一沙箱模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1194",
              "question": "源的 ClIP？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1195",
              "question": "滚到任意版本？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1196",
              "question": "现在 假设 扮演 你是)\\s*(一名？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1197",
              "question": "用于大模型微调的数据集如何构建？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1198",
              "question": "用户提问：感冒和咳嗽需要吃什么药？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1199",
              "question": "用户：2023 年我国上半年的国内生产总值是多少？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1200",
              "question": "用户：根据文档内容，征信中心有几点声明？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1201",
              "question": "用正则表达式的老的 NLP 技术吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1202",
              "question": "的“用户价值”动态变量？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1203",
              "question": "的知识是训练过的已知的知识，怎么训练这种能力？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1204",
              "question": "的语义差距？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1205",
              "question": "目前主流的开源模型体系有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1206",
              "question": "相⽐于 baichuan-7B，baichuan-13B 的 特点体现在哪⾥？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1207",
              "question": "相对位置编码和绝对位置编码有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1208",
              "question": "程中就学习到更多的知识？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1209",
              "question": "答、知识库参考），抽屉顶部置**“本条回答是否解决您的问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1210",
              "question": "策略，让模型自动合并相似标签并推荐最具信息量的论文给领域专家？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1211",
              "question": "简单 介绍⼀下 ⼤模型【LLMs】？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1212",
              "question": "简单介绍⼀下 RLHF？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1213",
              "question": "简单介绍⼀下 SentencePiece 思路？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1214",
              "question": "简单介绍一下 SentencePiece 思路？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1215",
              "question": "简单介绍一下为什么用它？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1216",
              "question": "简单介绍强化学习？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1217",
              "question": "类器并共享同一套敏感标签体系？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1218",
              "question": "系数防止过拟合？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1219",
              "question": "级日志快照，既能还原完整语境，又避免N² 膨胀？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1220",
              "question": "练”的幻觉率差异？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1221",
              "question": "给出权重设计示例？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1222",
              "question": "而且，节点间特殊的网络通常有400Gb/s？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1223",
              "question": "而非模型 Bug？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1224",
              "question": "耗时控制在 1 s 内？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1225",
              "question": "聊一下 RAG 项目总体思路？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1226",
              "question": "聚类结果如何反哺大模型微调？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1227",
              "question": "能不能总结⼀下各种参数⾼效微调⽅法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1228",
              "question": "能否⽤4 * v100 32G训练vicuna 65b？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1229",
              "question": "能否用 4 * v100 32G 训练 vicuna 65b？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1230",
              "question": "节点特征指的是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1231",
              "question": "若未来芯片进一步受限（仅 48 GB 显存），如何单卡完成 7B→1B 蒸馏？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1232",
              "question": "若要支持实时流式生成（直播字幕），如何做到逐句验证不阻塞？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1233",
              "question": "若问“后续如何与 LLMOps 打通做持续监控？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1234",
              "question": "若问“政务客户担心开源后数据泄露机密，如何平衡？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1235",
              "question": "补充：尝试过不同大小的 chunk 和混合检索。效果都不太好，如何优化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1236",
              "question": "要位置编码？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1237",
              "question": "要损失对下游任务影响 <2%？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1238",
              "question": "解释一下这里为什么这么关注训练前期，是因为在真实训练中，我们可能不一定会增强图中？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1239",
              "question": "解释其原理。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1240",
              "question": "解释层：点击任意卡片，右侧抽屉弹出三段式中文解释？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1241",
              "question": "解释模型为何聚焦“红色按钮”而非“背景文字”。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1242",
              "question": "解释模型决策过程：为了满⾜合规性要求，可以对模型的决策过程进⾏解释和解释。通过提供透明？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1243",
              "question": "解释模型决策过程：为了满足合规性要求，可以对模型的决策过程进行解释和解释。通过提供？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1244",
              "question": "解释的工业级方案，而不是只背论文。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1245",
              "question": "解释类型：链式思维（CoT）、引用召回（RAG 片段）、知识图谱路径、注？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1246",
              "question": "解释：优化器部分必须用 fp32（ 乎 fp16 会导致训练不稳定）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1247",
              "question": "解释：图中垂直线代表输入数组 [1,8,6,2,5,4,8,3,7]。在此情况下，容器能够容纳水（表示为？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1248",
              "question": "训练一个通用大模型的流程有那些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1249",
              "question": "训练中⽂⼤模型有啥经验？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1250",
              "question": "训练中文大模型有啥经验？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1251",
              "question": "设计拒绝模板？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1252",
              "question": "证分片权重一致性与推理结果确定性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1253",
              "question": "评”对外开放，反向提升企业品牌与技术影响力？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1254",
              "question": "评估指标错位？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1255",
              "question": "说明书法条：权利要求必须逐层布局，从独立权利要求的“最小技术方？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1256",
              "question": "说明：你不能倾斜容器。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1257",
              "question": "请256 MB 大页 EPC，把Attention 输入张量钉住，防止 EPC 换页。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1258",
              "question": "请按以下格式进行回答 A 、 B 、 C 、 D 。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1259",
              "question": "请描述一下你如何对大模型进行优化，以提高其？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1260",
              "question": "请检查嵌套类型 \") from e？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1261",
              "question": "请求实时翻译为英/日/西语，调用对应模型，若业务决策差异率>1%？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1262",
              "question": "请求落同一 Pod，避免CPU 缓存失效导致长尾延迟。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1263",
              "question": "请求费用 ≤ 用户套餐剩余额度，否则降级到 流式摘要 方案。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1264",
              "question": "请注意，Chat Message History 的具体用法和实现细节可以参考 Langchain 的官方文档和示？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1265",
              "question": "请注意，您可以根据需要添加、删除和修改嵌入向量。 Embedding 类和 VectorStore 类提？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1266",
              "question": "请注意，您可以根据需要添加、删除和修改组件。 Chain 类提供了多种方法来操作链。更多？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1267",
              "question": "请注意，您可以根据需要添加、删除和修改聊天消息提示。 ChatPromptTemplate 类提供了？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1268",
              "question": "请简述 Tokenizer 的作用及其在 NLP 模型中的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1269",
              "question": "请简述 Transformer 中的位置编码是如何实现的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1270",
              "question": "请简述 Transformer 的基本结构和工作原理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1271",
              "question": "请简述什么是大模型，以及它与传统模型的主要区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1272",
              "question": "请简述什么是大模型，以及它与传统模型的主要？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1273",
              "question": "请解释什么是位置编码，为什么在大模型中需要位置编码？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1274",
              "question": "请解释什么是位置编码，为什么在大模型中需？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1275",
              "question": "请解释什么是大模型微调，以及它在自然语言？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1276",
              "question": "请解释什么是注意力机制，并举例说明其应用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1277",
              "question": "请解释什么是过拟合和欠拟合，并说明如何在？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1278",
              "question": "请谈谈你对 A/B 测试的理解，并说明它在大模？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1279",
              "question": "请谈谈你对 Transformer 未来发展的看法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1280",
              "question": "请针对 >>> 和 <<< 中间的用户问题，选择一个适合的工具去回答他的问题，工具的名称已经给出。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1281",
              "question": "谈谈你对 Transformer 模型的理解，以及它在自？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1282",
              "question": "谐音错误也纳入对抗样本？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1283",
              "question": "进⾏ 增量预训练 需要做哪些准备⼯作？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1284",
              "question": "进⾏SFT操作的时候，基座模型选⽤Chat还是Base？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1285",
              "question": "进⾏领域⼤模型预训练应⽤哪些数据集⽐较好？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1286",
              "question": "进行 SFT 操作的时候，基座模型选用 Chat 还是 Base？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1287",
              "question": "进行 增量预训练 需要做哪些准备工作？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1288",
              "question": "进行稳定性筛选？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1289",
              "question": "进行领域大模型预训练应用哪些数据集比较好？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1290",
              "question": "迟的影响如何建模？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1291",
              "question": "迭代较慢问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1292",
              "question": "通讯开销比？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1293",
              "question": "通过在提示中使用 {？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1294",
              "question": "遇到多跳事实（A的B的C是多少）怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1295",
              "question": "遇到恶意刷榜（偷偷在测试集里加训练语料）怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1296",
              "question": "那么这个比例多少比较合适呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1297",
              "question": "都有什么特点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1298",
              "question": "采样时如何选择温度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1299",
              "question": "重训后模型效果“虚假回升”（指标好看但人工评测下降）怎么发现？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1300",
              "question": "量与多样性阈值？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1301",
              "question": "量更新策略？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1302",
              "question": "问有关于XXX的新闻吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1303",
              "question": "问题 1：如何让 LLM 简要、准确回答细粒度知识？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1304",
              "question": "问题 2：如何让 LLM 回答出全面的粗粒度（跨段落）知识？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1305",
              "question": "问题: 请问你们家的货可以送到四川吗，物流大概要多久？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1306",
              "question": "问题一：什么样的 数据 才是 最优的 大模型微调数据？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1307",
              "question": "问题二：如何构建 大模型微调数据？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1308",
              "question": "阶段 55 ms 压回去？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1309",
              "question": "降低 RTO？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1310",
              "question": "降维可视化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1311",
              "question": "面对**“百模大战”紧急立项，如何 1 天内快速生成路线图？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1312",
              "question": "面对大模型训练和推理所需的庞大计算资源，你有什么解决方案或建议？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1313",
              "question": "面试官可能追问 “为什么不用 SimHash+Hamming LSH？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1314",
              "question": "面试官可能追问“如果 KR 全部达成，但业务 GMV 没涨，是否算成功？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1315",
              "question": "面试尾声可反问面试官：“贵公司当前毛利率与预算增速是多少？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1316",
              "question": "项目最后上线了么，上线之后发现什么问题，如何解决？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1317",
              "question": "预训练和 SFT 操作有什么不同？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1318",
              "question": "预训练和微调哪个阶段注⼊知识的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1319",
              "question": "预训练和微调哪个阶段注入知识的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1320",
              "question": "预训练数据 Token 重复 是否影响 模型性能？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1321",
              "question": "领域数据训练后，通⽤能⼒往往会有所下降，如何缓解模型遗忘通⽤能⼒？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1322",
              "question": "领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1323",
              "question": "领域模型Continue PreTrain 数据选取？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1324",
              "question": "领域模型Continue PreTrain ，如何 让模型在预训练过程中就学习到更多的知识？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1325",
              "question": "领域模型微调 指令&数据输⼊格式 要求？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1326",
              "question": "领域模型微调 领域评测集 构建？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1327",
              "question": "领域模型微调指令&数据输入格式要求？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1328",
              "question": "领域模型词表扩增是不是有必要的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1329",
              "question": "（Positional Encoding）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1330",
              "question": "（TEE）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1331",
              "question": "（torch profiler）如何查看⾃⼰的训练中通信开销？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1332",
              "question": "💡 LLMs 存在模型幻觉问题，请问如何处理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1333",
              "question": "💡 PPO（强化学习）的数据格式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1334",
              "question": "💡 RLHF 在实践过程中存在哪些不足？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1335",
              "question": "💡 RM（奖励模型）的数据格式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1336",
              "question": "💡 SFT 指令微调数据 如何构建？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1337",
              "question": "💡 SFT（有监督微调）的数据集格式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1338",
              "question": "💡 llama 输入句子长度理论上可以无限长吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1339",
              "question": "💡 prefix LM 和 causal LM 区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1340",
              "question": "💡 为什么SFT之后感觉LLM傻了？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1341",
              "question": "💡 为什么会出现 LLMs 复读机问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1342",
              "question": "💡 为什么大模型推理时显存涨的那么多还一直占着？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1343",
              "question": "💡 什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型，咋选？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1344",
              "question": "💡 什么是 LLMs 复读机问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1345",
              "question": "💡 各个专业领域是否需要各自的大模型来服务？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1346",
              "question": "💡 基于LLM+向量库的文档对话 prompt 模板 如何构建？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1347",
              "question": "💡 基于LLM+向量库的文档对话 思路是怎么样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1348",
              "question": "💡 基于LLM+向量库的文档对话 核心技术是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1349",
              "question": "💡 多轮对话任务如何微调模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1350",
              "question": "💡 大模型LLM的架构介绍？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1351",
              "question": "💡 大模型LLM进行SFT 如何对样本进行优化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1352",
              "question": "💡 大模型LLM进行SFT操作的时候在学习什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1353",
              "question": "💡 大模型在gpu和cpu上推理速度如何？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1354",
              "question": "💡 大模型怎么评测？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1355",
              "question": "💡 大模型有推理能力吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1356",
              "question": "💡 大模型生成时的参数怎么设置？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1357",
              "question": "💡 大模型的honest原则是如何实现的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1358",
              "question": "💡 奖励模型需要和基础模型一致吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1359",
              "question": "💡 如何给LLM注入领域知识？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1360",
              "question": "💡 如何缓解 LLMs 复读机问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1361",
              "question": "💡 如何解决 PPO 的训练过程同时存在4个模型（2训练，2推理），对计算资源的要求较高 问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1362",
              "question": "💡 如何解决 人工产生的偏好数据集成本较高，很难量产问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1363",
              "question": "💡 如何解决三个阶段的训练（SFT->RM->PPO）过程较长，更新迭代较慢问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1364",
              "question": "💡 如何让大模型处理更长的文本？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1365",
              "question": "💡 如何训练自己的大模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1366",
              "question": "💡 如果想要在某个模型基础上做全参数微调，究竟需要多少显存？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1367",
              "question": "💡 如果想要快速体验各种模型，该怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1368",
              "question": "💡 建议的软件环境是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1369",
              "question": "💡 微调后的模型出现能力劣化，灾难性遗忘是怎么回事？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1370",
              "question": "💡 微调模型需要多大显存？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1371",
              "question": "💡 微调需要多少条数据？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1372",
              "question": "💡 想让模型学习某个领域或行业的知识，是应该预训练还是应该微调？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1373",
              "question": "💡 找数据集哪里找？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1374",
              "question": "💡 指令微调的好处？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1375",
              "question": "💡 推理速度上，int8和fp16比起来怎么样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1376",
              "question": "💡 有哪些大模型的训练集？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1377",
              "question": "💡 有哪些省内存的大语言模型训练/微调/推理方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1378",
              "question": "💡 模型如何判断回答的知识是训练过的已知的知识，怎么训练这种能力？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1379",
              "question": "💡 涌现能力是啥原因？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1380",
              "question": "💡 目前 主流的开源模型体系 有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1381",
              "question": "💡 训练中文大模型有啥经验？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1382",
              "question": "💡 进行SFT操作的时候，基座模型选用Chat还是Base？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1383",
              "question": "💡 进行领域大模型预训练应用哪些数据集比较好？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1384",
              "question": "💡 预训练和微调哪个阶段注入知识的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1385",
              "question": "💡 领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1386",
              "question": "💡 领域模型Continue PreTrain 数据选取？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1387",
              "question": "💡 领域模型Continue PreTrain ，如何 让模型在预训练过程中就学习到更多的知识？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1388",
              "question": "💡 领域模型微调 指令&数据输入格式 要求？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1389",
              "question": "💡 领域模型微调 领域评测集 构建？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "llm_1390",
              "question": "💡 领域模型词表扩增是不是有必要的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            }
          ]
        },
        {
          "subcategory": "LLM-基础",
          "count": 548,
          "questions": [
            {
              "id": "llm_1391",
              "question": "# 你好，我是chatGPT,很高兴能够和你聊天。有什么我可以帮助你的吗？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1392",
              "question": "# 报错：大概率peft训练有问题，检查adapter.bin大小？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1393",
              "question": "#2)文本分割， 这里仅为了方便快速看流程，实际应用的会复杂一些？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1394",
              "question": "**数据如何划分？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1395",
              "question": "**计算如何协同？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1396",
              "question": "ALiBi (Attention with Linear Biases) 被哪些 LLMs 应用？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1397",
              "question": "AMP混合精度训练 代码？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1398",
              "question": "AMP混合精度训练 完整代码？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1399",
              "question": "AdapterDrop 思路 是什么？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1400",
              "question": "AdapterDrop 特点 是什么？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1401",
              "question": "AdapterFusion 思路 是什么？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1402",
              "question": "Apex 是一个在 PyTorch 深度学习框架下用于加速训练和提高性能的库。Apex 提供了混合精度？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1403",
              "question": "B 和 6B 在 2 种不同训练方式下的对比实验？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1404",
              "question": "CUDA张力不要超出范围，只要有必要。这不应该是共享模型参数的问题，但传递其他类型的数据应该小心。请？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1405",
              "question": "Chat Message History 作用：负责记住所有以前的聊天交互数据，然后可以将这些交互数据传递回模型、汇？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1406",
              "question": "ChatGLM-6B是平衡中英文分词效果最好的tokenizer。由于词表比较大，中文处理时间也有增加？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1407",
              "question": "ChatGLM-6B模型时，有同学提出来的问题，表现为原始ChatGLM-6B模型在知识问答如“失眠怎么？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1408",
              "question": "Continuous batching：有iteration-level的调度机制，每次迭代batch大小都有所变化，因此vLLM在大量查询？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1409",
              "question": "DDP通过多进程实现的。也就是说操作系统会为每个GPU创建一个进程,从而避免了Python解释器GIL带来的？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1410",
              "question": "DDP首先要解决的就是通讯问题：将Server上的通讯压力均衡转到各个Worker上。实现这一点后，可以进一？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1411",
              "question": "DP 只支持 单机多卡场景，在 多机多卡 场景 下，DP 的 通讯问题将被放大？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1412",
              "question": "Deep Norm可以缓解爆炸式模型更新的问题，把模型更新限制在常数，使得模型训练过程更稳定？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1413",
              "question": "DeepSpeed 如何使用？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1414",
              "question": "DeepSpeed 的主要优化器是 Adam、AdamW、OneBitAdam 和 Lamb。 这些已通过 ZeRO 进？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1415",
              "question": "DeepSpeed 还提供了 mpi、gloo 和 nccl 等通信策略，可以根据具体情况进行选择和配置。在？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1416",
              "question": "DeepSpeed 遇到问题，如何 确定 调参步骤？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1417",
              "question": "DistributedDataParallel(以下简称DDP) 优点有哪些？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1418",
              "question": "Document(page_content='文本分块是自然语言处理（NLP）中的一项关键技术，其作用是？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1419",
              "question": "Document(page_content='的。我的目标是通过回答用户提出的问题来帮助他们解决问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1420",
              "question": "GPU上，以实现大型模型的计算，弥补了DDP的缺点，非常方便，这也就意味着我们能用更少的？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1421",
              "question": "GPU数量 VS 训练速度？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1422",
              "question": "Gradient partitioning (ZeRO stage 2)？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1423",
              "question": "LLMs 已经具备了较强能力了，存在哪些不足点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1424",
              "question": "LORA 模块，q_proj、k_proj和v_proj是多头注意力机制中的三个线性变换，用于将输入的？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1425",
              "question": "LORA应该作用于Transformer的哪个参数矩阵？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1426",
              "question": "LightLLM 介绍一下？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1427",
              "question": "LightLLM 依赖包 有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1428",
              "question": "LightLLM 如何使⽤？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1429",
              "question": "LightLLM 如何使用？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1430",
              "question": "LightLLM 如何安装？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1431",
              "question": "LightLLM 性能表现 介绍？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1432",
              "question": "Linear 实现？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1433",
              "question": "Llama 2 的 Margin Loss：每次只能看到两个（而非4-9个）回复并进行对比，但新增了一个边际（margin）？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1434",
              "question": "MAM Adapter 思路 是什么？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1435",
              "question": "MAM Adapter 特点 是什么？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1436",
              "question": "MoE大模型具备哪些优势？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1437",
              "question": "MoE大模型具备哪些缺点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1438",
              "question": "Nucleus sampler俗称TopP采样，一种用于解决TopK采样问题的新方法，该采样方式不限制K的数目，而是通？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1439",
              "question": "Nucleus sampler是对简单暴力的TopK采样修改后的方法，也能解决1.1节中提出所有重复单调问题，相比？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1440",
              "question": "Optimizer state partitioning (ZeRO stage 1)？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1441",
              "question": "PEFT库 中 Lora层的 实现思路？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1442",
              "question": "PandaLM：其是直接训练了一个自动化打分模型，0,1,2三分制用模型对两个候选模型进行打分？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1443",
              "question": "Parameter partitioning (ZeRO stage 3)？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1444",
              "question": "Pretrain阶段，为什么需要拼接拼接？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1445",
              "question": "Prompt Template 作用：负责创建 PromptValue，这是最终传递给语言模型的内容？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1446",
              "question": "RRF 技术实现？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1447",
              "question": "RWKV。其实关于变长序列的问题，历史上现成的解决方案就是RNN，通过信息传递来解决。Transformer的？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1448",
              "question": "Rank 如何选取？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1449",
              "question": "Reward Model 训练数据集的 Scaling Law？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1450",
              "question": "StreamingLLM 优点是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1451",
              "question": "StreamingLLM 思路是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1452",
              "question": "TopK通过对Softmax的输出结果logit中最大的K个token采样来选择输出的token，该方法存在的问题是当概率分？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1453",
              "question": "TopK采样是一种行之有效，能简单暴力的解决1.1节中提出所有重复单调问题的方案之一，当然它存在的最大问？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1454",
              "question": "Trainer 训练类 编写？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1455",
              "question": "Trainer 训练类？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1456",
              "question": "WordPiece算法选择 能够提升语言模型概率最大的相邻子词进行合并，来加入词表？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1457",
              "question": "ZeRO Offload后的计算流程是怎么样？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1458",
              "question": "ZeRO 优化策略是怎么样？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1459",
              "question": "ZeRO 显存如何分配？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1460",
              "question": "ZeRO 的 核心思想是什么？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1461",
              "question": "ZeRO-2？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1462",
              "question": "ZeRO-3 and Infinity Nuances？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1463",
              "question": "ZeRO-3 中未使用 allgather_partitions、allgather_bucket_size 和 reduce_scatter？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1464",
              "question": "ZeRO-3 会比 ZeRO-2 慢很多 如何优化？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1465",
              "question": "ZeRO-3 会比 ZeRO-2 慢很多。使用以下策略，可以使得ZeRO-3 的速度更接近ZeRO-2？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1466",
              "question": "ZeRO-Infinity 需要使用 ZeRO-3？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1467",
              "question": "ZeRO-Offload to CPU and NVMe？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1468",
              "question": "ZeRO-Offload 使 GPU 单卡能够训练 10 倍大的模型： 为了同时利用 CPU 和 GPU 内存来训练？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1469",
              "question": "ZeRO-stage-0？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1470",
              "question": "ZeRO（Zero Redundancy Optimizer）是一种用于大规模训练优化的技术，主要是用来减少内？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1471",
              "question": "accelerate 分布式训练 主要优势？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1472",
              "question": "accelerate 分布式训练 介绍？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1473",
              "question": "accelerate 分布式训练 代码实现逻辑？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1474",
              "question": "accelerate 分布式训练 依赖安装？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1475",
              "question": "accelerate 分布式训练 原理讲解？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1476",
              "question": "accelerate 分布式训练 如何实践？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1477",
              "question": "accelerate 分布式训练 示例代码？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1478",
              "question": "accelerate 分布式训练 运行？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1479",
              "question": "alpha参数 如何选取？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1480",
              "question": "b. 优点：相比于Post-LN，Pre LN 在深层的梯度范式近似相等，所以使用Pre-LN的深层transformer训？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1481",
              "question": "b. 支持大批量训练：使用大批量数据进行训练可以加快收敛速度和稳定性，但可能会导致内存问题。梯？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1482",
              "question": "b. 缺点：Post LN 在深层的梯度范式逐渐增大，导致使用post-LN的深层transformer容易出现训练不稳？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1483",
              "question": "bias: 是否可训练bias，none：均不可；all：均可；lora_only：只有lora部分的bias可训练？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1484",
              "question": "c. 作用：可以比作在语音生成前对“听觉”进行调整，优化检索内容对最终输出的影响。特别是在处理不？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1485",
              "question": "c. 缺点：相比于Post-LN，Pre-LN的模型效果略差？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1486",
              "question": "c. 缺点：训练不稳定，可能会导致训练崩溃？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1487",
              "question": "character_coverage：指定覆盖字符的数量，可以理解为限制字符集的大小。默认值为 1.0，即覆盖？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1488",
              "question": "ckpt存储能否实现异步或者部分掩盖？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1489",
              "question": "deepspeed 训练过程，报找不主机？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1490",
              "question": "gloo 是一种高性能的分布式训练框架，支持 CPU 和 GPU 上的分布式训练？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1491",
              "question": "i. 用户自主性：操作用户查询有时可能偏离原始意图。考虑我们向人工智能让渡多？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1492",
              "question": "input_text = \"文本分块是自然语言处理（NLP）中的一项关键技术，其作用是将较长的文本切？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1493",
              "question": "instruction\": \"我们如何在日常生活中减少用水？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1494",
              "question": "lora_alpha：归一化超参数，lora参数 ΔWx 被以 α/r 归一化，以便减少改变r rr时需要重新训练的计算？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1495",
              "question": "merge_weights:eval模式中，是否将lora矩阵的值加到原有 W0 的值上？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1496",
              "question": "modules_to_save：除了lora部分之外，还有哪些层可以被训练，并且需要保存？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1497",
              "question": "module是要放到多卡训练的模型？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1498",
              "question": "mpi 是一种跨节点通信库，常用于 CPU 集群上的分布式训练？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1499",
              "question": "nccl 是 NVIDIA 提供的 GPU 专用通信库，被广泛应用于 GPU 上的分布式训练？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1500",
              "question": "nn.DataParallel 函数 常见问题及解答 有哪些？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1501",
              "question": "nn.DataParallel(以下简称DP) vs DistributedDataParallel(以下简称DDP)介绍一下？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1502",
              "question": "nn.parallel.DistributedDataParallel 函数 如何多卡加速训练？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1503",
              "question": "nn.parallel.DistributedDataParallel 实现流程介绍一下？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1504",
              "question": "overlap_comm：控制是否使用通信与计算的重叠。当设置为True时，DeepSpeed将在？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1505",
              "question": "pdfplumber 如何进行 表格抽取？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1506",
              "question": "print(conversation_with_summary.predict(input=\"我喜欢的食物是什么？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1507",
              "question": "process group中的训练进程都起来后，rank为0的进程会将网络初始化参数broadcast到其它每个进程中，确？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1508",
              "question": "prompt设计，这里只是一个prompt的简单示意，在实际业务场景中需要针对场景特点针对性调优？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1509",
              "question": "pytorch 分布式计算 坑/bug 梳理篇？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1510",
              "question": "pytorch中的GPU操作默认是什么样？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1511",
              "question": "response = llm.complete(\"什么是大语言模型？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1512",
              "question": "step 2训练时只更新Prefix部分的参数，而Transformer中的其他部分参数固定？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1513",
              "question": "text =\"文本分块是自然语言处理（NLP）中的一项关键技术，其作用是将较长的文本切割成更？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1514",
              "question": "tokenizer：保存的模型的名称前缀？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1515",
              "question": "token映射到一个高维向量空间中，以便于模型对输入进行处理；o_proj则是多头注意力机制的？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1516",
              "question": "torch.multiprocessing 函数如何使用？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1517",
              "question": "vLLM 如何使⽤？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1518",
              "question": "vLLM 如何安装？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1519",
              "question": "vLLM 性能如何？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1520",
              "question": "vLLM 支持哪些 Huggingface 模型 ？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1521",
              "question": "一、LLMs 已经具备了较强能力了，存在哪些不足点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1522",
              "question": "一、为什么需要 StreamingLLM？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1523",
              "question": "一、什么是 LLMs 测试集数据泄露 问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1524",
              "question": "一、介绍一下 LLMs 的文本生成过程？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1525",
              "question": "三、StreamingLLM 优点是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1526",
              "question": "三、是否可以 避开训练集来处理 LLMs 测试集数据泄露 问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1527",
              "question": "不同 ZeRO 如何配置？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1528",
              "question": "不能使用Apex进行混合精度训练？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1529",
              "question": "与 AdamW 类似，可以配置其他官方支持的优化器。 请记住，它们可能具有不同的配置值？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1530",
              "question": "业政策落实中发挥重要作用。\\xa0[5]\\xa0\\n2022年5月，“超级谷物”藜麦在宁洱县试种成？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1531",
              "question": "中来。除此之外，FEFT可以缓解全量微调带来灾难性遗忘的问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1532",
              "question": "为了解决信息 “丢失在中间” 的问题，LongContextReorder 被设计用来重新排序检索到的节点，在？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1533",
              "question": "为了解决共享内存文件泄漏的问题，torch.multiprocessing将产生一个守护程序torch_shm_manager，它将自己？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1534",
              "question": "为了解决这个问题，我们需要为模型引入位置编码，让每个词向量都能够感知到它在输入序列中所？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1535",
              "question": "为什么 大模型分布式训练 需要 故障恢复？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1536",
              "question": "为什么 需要 FLARE？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1537",
              "question": "为什么 需要 适配器微调（Adapter-tuning）？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1538",
              "question": "为什么单卡的情况，也可以使用deepspeed？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1539",
              "question": "为什么第一块卡的显存会占用的更多一些？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1540",
              "question": "为什么要提取标题甚至是多级标题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1541",
              "question": "为什么要生成多个查询？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1542",
              "question": "为什么选择RRF？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1543",
              "question": "为什么需要 Deepspeed？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1544",
              "question": "为什么需要 StreamingLLM？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1545",
              "question": "为什么需要 ZeRO？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1546",
              "question": "为什么需要 accelerate 分布式训练？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1547",
              "question": "为什么需要 提示学习（Prompting）？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1548",
              "question": "为什么需要 构建中文tokenization？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1549",
              "question": "为什么需要位置编码？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1550",
              "question": "为什么需要使用大模型辅助召回？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1551",
              "question": "为什么需要对文本分块？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1552",
              "question": "为什么需要构建负难样本？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1553",
              "question": "为什么需要自动混合精度？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1554",
              "question": "为什么需要识别表格？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1555",
              "question": "主节点（master_ip+master_port）：在分布式计算环境中，主节点负责协调所有其他节点和进？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1556",
              "question": "之前的研究表明GPT3使用prompt训练方式可以显著提升few-shot 和 zero-shot的效果？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1557",
              "question": "也预测不出来下一个词应该是什么。因此模型会倾向从前面的word里面挑选。无论是专业翻译大模型？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1558",
              "question": "习特定于任务的高级特征，同时获得了一些优点，如对训练数据的需求较少，以及由于预先知道而？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1559",
              "question": "二、StreamingLLM 思路是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1560",
              "question": "二、如何解决 LLMs 测试集数据泄露 问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1561",
              "question": "五、MoE大模型具备哪些缺点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1562",
              "question": "交叉熵损失函数是二分类问题中最常用的损失函数，由于其定义出于信息学的角度，可以泛化到多分类问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1563",
              "question": "人员选择创建自己的令牌计数函数，但也有其他解决方案可以解决这个问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1564",
              "question": "什么是 Backtranslation？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1565",
              "question": "什么是 DistributedDataParallel 核心 —— Ring-AllReduce？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1566",
              "question": "什么是 LLMs 测试集数据泄露 问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1567",
              "question": "什么是 Self-Instruct？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1568",
              "question": "什么是 Sinusoidal位置编码？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1569",
              "question": "什么是 accelerate 分布式训练？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1570",
              "question": "什么是 大模型（LLMs）agent？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1571",
              "question": "什么是 提示学习（Prompting）？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1572",
              "question": "什么是 训练式位置编码？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1573",
              "question": "什么是 长度外推问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1574",
              "question": "什么是生成式大模型？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1575",
              "question": "什么是自动混合精度训练？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1576",
              "question": "仅能解决1.1节中阐述的前两种重复问题，无法解决输入多次相同prompt输出单调性的问题）？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1577",
              "question": "介绍一下 LLMs 的文本生成过程？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1578",
              "question": "介绍一下 gradient accumulation 显存优化方式？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1579",
              "question": "介绍一下 gradient checkpointing 显存优化方式？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1580",
              "question": "介绍一下 混合精度训练 动态损失缩放？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1581",
              "question": "介绍：将整个数据集切分为多份，每张GPU分配到不同的数据进行训练，每个进程都有一个完整的模型副？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1582",
              "question": "介绍：通过低秩分解来模拟参数的改变量，从而以极小的参数量来实现大模型的间接训练？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1583",
              "question": "从 induction head[1]机制的影响角度：也就是模型会倾向于从前面已经预测的word里面挑选最匹配的词？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1584",
              "question": "从上图可以看到：随着「训练模型」和「初始模型」之间的 KL（可简单理解为差异）越大，模型的「真实分？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1585",
              "question": "代码中，最核心的是以下这一块，该代码的作用是 将 原始词表 中没有的 新词 加入 词表中？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1586",
              "question": "代码实现可读性差：很多开源代码都是简单拷贝Transformer代码库，然后进行小修小补。这些拷贝也不使用？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1587",
              "question": "代码路径 执行训练的代码、模型、数据集等相关文件、路径要一致（如果不一致，考虑建立软？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1588",
              "question": "以下是如何定义 LongContextReorder 作为您查询引擎构建时节点后处理器的示例代码？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1589",
              "question": "众所周知，transformer模型之所以能够取得如此卓越的效果，其中的Attention机制功不可没，它的？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1590",
              "question": "优点：可以保证采样得到的负例是模型未能较好区分的较难负例？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1591",
              "question": "优点：最大的模型（约800MB）精度非常高？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1592",
              "question": "优点：有助于维护上下文并提高模型对对话的理解？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1593",
              "question": "优点：模型比较小，效果也还行？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1594",
              "question": "优点：节约HBM，高效利用SRAM，省显存，提速度？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1595",
              "question": "优点：训练效率高，zero-shot 能力更强，具有涌现能力？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1596",
              "question": "优点：通过这种方式，使 LLM 更好地利用相关背景知识，并训练 LLM 即使在检索错误块的情况下也能产生？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1597",
              "question": "作用于表提取的 FCN 体系结构。使用 Tesseract OCR 对图像进行预处理和修改？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1598",
              "question": "作用在每个 transformer 层的 self-attention 块，在计算完 Q/K 之后，旋转位置编码作用在 Q/K？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1599",
              "question": "作用： 负责将语言模型响应构建为更有用的格式？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1600",
              "question": "作用：当您想要在 Prompts 中动态包含示例时，Example Selectors 很有用。他们接受用户输入并返回一个？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1601",
              "question": "作用：相对距离越大，惩罚项越大相当于两个token的距离越远，相互贡献就越小？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1602",
              "question": "使用ZeRO-offload，将部分数据offload到CPU，降低对显存的需求？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1603",
              "question": "使用sentencepiece训练一个中文的词表？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1604",
              "question": "使用torch.cuda.amp.autocast （以及torch.cuda.amp.GradScaler）来进行训练。咦？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1605",
              "question": "例）。文档正例是和问题密切相关的文档片段，文档负例是和问题不相关的文档片段，可以是？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1606",
              "question": "公司于 2023 年共同训练的语言模型开发的。我的目标是通过回答用户提出的问题来帮助他们？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1607",
              "question": "关于 训练数据 加载到GPU器件上的时机 : 用于训练的数据为了方便 我们在这里统一？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1608",
              "question": "关于如何使用假设性文档嵌入（HyDE）这一查询改写技术，您可以参考下方示例代码。在这种方？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1609",
              "question": "其中对比loss通过在原loss基础上添加对比loss，即对比token间相似度的方式去解决生成式模型重复单调问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1610",
              "question": "其区别会影响policy网络的实现方式？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1611",
              "question": "其实ALiBi的方法就是一个比较简单优雅的方式，可以部？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1612",
              "question": "其实lora微调的代码本身并不复杂，相反是如何加速大模型训练，降低显存占用的一些技巧大家可能不太熟悉？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1613",
              "question": "其实，刚开始实践的时候，不需要太多样本，先收集GB量级的领域文本跑通流程即可？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1614",
              "question": "其次，vicuna使用flash-attention加速训练，暂不支持v100，需要turing架构之后的显卡？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1615",
              "question": "具体 PEFT 包装 包装，结合PEFT模块的源码，来看一下LORA是如何实现的？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1616",
              "question": "写模块的奖励机制。这样，重写模块可以调整检索查询，从而提高阅读器在后续任务中的表现？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1617",
              "question": "准备足够的训练语料;以及期望的词表大小？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1618",
              "question": "减少了通信时间：由于更新的参数量变少了，所以（尤其是多卡训练时）要传输的数据量也变少了，从而减？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1619",
              "question": "几种主流大模型的 loss 了解过吗？ 有哪些异同？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1620",
              "question": "出现解码退化的问题即生成的文本不自然的，并包含文本重复而提出的一种解决方案？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1621",
              "question": "分利用多个GPU进行训练。DistributedDataParallel的优点是在内存占用和数据通信方面优于nn.DataParallel，能？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1622",
              "question": "分成子词。这个参数的作用是将一些用户定义的特殊符号作为一个整体加入到生成的词表中，以便于？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1623",
              "question": "分析 Top-K负例采样策略（Top-K Hard Negative Sampling）方法 挖掘负例训练时对梯度的影？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1624",
              "question": "分类问题的目标是将输入样本分到不同的类别中，输出为类别的概率分布。交叉熵损失函数可以度量两个概率分？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1625",
              "question": "分解决扩展长度的问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1626",
              "question": "创建DDP模型进行分布式训练？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1627",
              "question": "初始化，A采用高斯分布初始化，B初始化为全0，保证训练开始时旁路为0矩阵？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1628",
              "question": "利用 对比学习微调 方式构建负例方法？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1629",
              "question": "前缀微调（Prefix-tining） vs 人工设计离散的 Prompts 无法更新参数：前缀微调（Prefix-tining） 可以学习？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1630",
              "question": "务类型单一，不会对其原有的能力造成大的影响，所以我认为是不会导致灾难性遗忘问题，我自己？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1631",
              "question": "动态显存在向前传播的过程中每个样本的每个神经元都会计算激活值并存储，用于向后传播时的梯度计算？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1632",
              "question": "动态选择损失标度 介绍：发生溢出，跳过优化器更新，损失标度减半;连续N个steps没有发生溢出，损失标？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1633",
              "question": "动机：仅仅使用领域数据集进行模型训练，模型很容易出现灾难性遗忘现象.？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1634",
              "question": "动机：目前绝大多数大模型支持的token最大长度为2048，因为序列长度直接影响Attention的计算复杂度？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1635",
              "question": "动机：虽然 切分micro-batch方法 能够 解决了GPU的空置问题，提升了GPU计算的整体效率。但是 如何解？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1636",
              "question": "占用加快训练速度:通信量？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1637",
              "question": "即使在没有给定上下文的情况下，问题也应该对人类有意义？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1638",
              "question": "卸载（Offload）技术：一种用通信换显存的方法，简单来说就是让模型参数、激活值等在CPU内存和GPU显？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1639",
              "question": "参数更新的方式不同。DDP在各进程梯度计算完成之后,各进程需要将梯度进行汇总平均,然后再由 rank=0 的？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1640",
              "question": "参数计算口径不一致：参数计算可以分为三类：可训练参数的数量、微调模型与原始模型相比改变的参数的？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1641",
              "question": "参考数据：论文中的训练速度或者吞吐量？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1642",
              "question": "另外，如果我们已经有了一批已经去重的人工处理过的高质量数据，那么我们如何寻找与这批数据？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1643",
              "question": "只在输入层加入prompt tokens，并且不需要加入 MLP 进行调整来解决难训练的问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1644",
              "question": "只是如何通过某条神经元的线索能更加精准的调动出大脑中最擅长Planning的部分？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1645",
              "question": "可以利用大量的无标注数据来训练一个通用的模型，然后再用少量的有标注数据来微调模型，以适应特定的？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1646",
              "question": "可插拔式的切换任务，当前任务W0+B1A1，将lora部分减掉，换成B2A2，即可实现任务切换？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1647",
              "question": "司于 2023 年共同训练的语言模型开发的。我的目标是通过回答用户提出的问题来帮助他们解？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1648",
              "question": "启用 offload_optimizer 时可以使用非 DeepSpeed 的优化器，只要它同时具有 CPU 和 GPU 的？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1649",
              "question": "四、MoE大模型具备哪些优势？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1650",
              "question": "回落到torch.FloatTensor，这就是混合一词的由来。那怎么知道什么时候用torch.FloatTensor，什么时候用半？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1651",
              "question": "回顾第二部分的两个目标，Gpipe真的实现了吗？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1652",
              "question": "因为每个GPU都需要计算模型参数的梯度并将其发送给其他GPU，因此需要使用同步机制来保？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1653",
              "question": "在使用大型语言模型时，你可能会担心如果模型出了问题怎么办，比如遇到了 OpenAI 模型的使用？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1654",
              "question": "在分布式计算环境中，需要理解几个非常基础的概念：节点编号、全局进程编号、局部进程编？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1655",
              "question": "在加载第一个适配器时，可以通过 PeftModel.from_pretrained 方法并指定 adapter_name 参数来给它命名？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1656",
              "question": "在微调大模型的时候，单机2卡的时候正常训练，但是采用4卡及其以上，就会卡住，卡在读完数？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1657",
              "question": "在电信公司的客服聊天机器人场景中，如果用户在对话中先是询问了账单问题，接着又谈到了网络连接问？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1658",
              "question": "在翻译上，由于input和output的天然差异性，你会发现容易出现重复的都是一些复杂度perplexity比较高的文？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1659",
              "question": "在训练脚本侧修改，在下一次更新参数或优化器状态之前，强制等待ckpt存储完成，这样可以？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1660",
              "question": "在这个例子中，我们制定一个简单的查询，“你能简要介绍一下马达引擎的工作原理吗？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1661",
              "question": "型展示新变化的表格。这种方法特别适用于解决包含多个信息点的复杂表格单元问题，通过有序地？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1662",
              "question": "基于批内负采样的对比学习方法？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1663",
              "question": "基类 LoraLayer 实现？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1664",
              "question": "多任务学习中，如果各任务的损失差异过大，可以通过动态调整损失权重、使用任务特定的损失函数、改变模型？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1665",
              "question": "多机训练不通，DeepSPeed配置问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1666",
              "question": "多查询生成 工作原理？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1667",
              "question": "多查询生成 技术实现（提示工程）？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1668",
              "question": "多种不同的高效微调方法对比？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1669",
              "question": "大模型 (LLMs) 评测有那些方法？如何衡量大模型的效果？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1670",
              "question": "大模型【LLMs】后面跟的 175B、60B、540B等 指什么？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1671",
              "question": "大模型外挂知识库优化——如何利用大模型辅助召回？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1672",
              "question": "大模型是怎么让生成的文本丰富而不单调的呢？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1673",
              "question": "大模型训练loss突刺原因和解决办法？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1674",
              "question": "大模型（LLMs）agent 主要 利用了 大模型 哪些能力？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1675",
              "question": "大模型（LLMs）agent 有哪些部分组成？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1676",
              "question": "大模型（LLMs）显存问题面？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1677",
              "question": "如何 使用模型？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1678",
              "question": "如何 利用 transformers 加载 Bert 模型？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1679",
              "question": "如何 利用 transformers 输出 Bert 指定 hidden_state？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1680",
              "question": "如何 向 模型 加入PEFT策略？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1681",
              "question": "如何 基于lora的llama2 微调？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1682",
              "question": "如何 构建模型？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1683",
              "question": "如何 解决 从复杂PDF文件中提取数据 问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1684",
              "question": "如何 解决 内容缺失问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1685",
              "question": "如何 解决 回答不全面 问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1686",
              "question": "如何 解决 备用模型 问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1687",
              "question": "如何 解决 数据处理能力的挑战 问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1688",
              "question": "如何 解决 格式错误 问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1689",
              "question": "如何 解决 特异性错误 问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1690",
              "question": "如何 解决 结构化数据查询的难题 问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1691",
              "question": "如何 解决 脱离上下文 — 整合策略的限制 问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1692",
              "question": "如何 解决 错过排名靠前的文档 问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1693",
              "question": "如何 配置 LoraConfig？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1694",
              "question": "如何 长文档（书籍）中关键信息？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1695",
              "question": "如何为每个位置的词向量注入位置信息呢？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1696",
              "question": "如何估算需要的显存？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1697",
              "question": "如何使用 AMP混合精度训练？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1698",
              "question": "如何使用transformers库加载sentencepiece模型？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1699",
              "question": "如何区分单栏还是双栏pdf？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1700",
              "question": "如何合并英文词表和中文词表？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1701",
              "question": "如何在PyTorch中使用自动混合精度？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1702",
              "question": "如何对 原始数据预处理？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1703",
              "question": "如何提取 文章标题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1704",
              "question": "如何提取表格和图片中的数据？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1705",
              "question": "如何构建中文的词库？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1706",
              "question": "如何精确地回答用户关于文档的问题，不重也不漏？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1707",
              "question": "如何给LLM注入领域知识？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1708",
              "question": "如何脱离已有代码库复用这些方法）？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1709",
              "question": "如何获取最优的ckpt存储间隔？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1710",
              "question": "如何解决 LLMs 测试集数据泄露 问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1711",
              "question": "如何选择一款分布式训练框架？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1712",
              "question": "如何选择不同的Zero stage和offload？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1713",
              "question": "如何配置 安装pdsh？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1714",
              "question": "如何配置 配置deepspeed文件？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1715",
              "question": "如何配置 配置ssh？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1716",
              "question": "如图所示模型训练过程中红框中突然上涨的loss尖峰 loss spike的现象会导致一系列的问题发生？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1717",
              "question": "如果OOM则，使用混合精度训练，在Ampere的GPU上使用bf16，在旧版本GPU上使用fp16？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1718",
              "question": "如果OOM则，尝试ZeRO stage 2 + offload_optimizer？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1719",
              "question": "如果OOM则，尝试ZeRO stage 2？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1720",
              "question": "如果仍然OOM，则使用ZeRO-Infinity ，使用offload_param和offload_optimizer到NVME？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1721",
              "question": "如果实现不了，又是因为什么原因呢？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1722",
              "question": "如果是用pytorch实现同步梯度更新，自研 数据接口，出现 第一个epoch结尾处程序卡死问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1723",
              "question": "如果模型是在ZeRO-2模式下保存的，模型参数会以fp16的形式存储在pytorch_model.bin中？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1724",
              "question": "如防止 LLM 被诱导泄露文档的来源、元数据或其他敏感信息——都是亟待解决的关键问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1725",
              "question": "存在问题：实际应用场景下，如果你的数据比较脏，难例挖掘用处可能不大？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1726",
              "question": "存在问题：很可能将潜在的正例也误判为负例，即假负例（False Negative）。如果训练模型？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1727",
              "question": "定义 Trainer 训练类 【注：这里有部分改动】？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1728",
              "question": "定义 获取 和 加载 训练集 函数？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1729",
              "question": "定义 训练集编码类？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1730",
              "question": "定功能。RRR 提出了一种重写 - 检索 - 阅读的流程，其中利用大语言模型（LLM）的性能作为强化学习中重？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1731",
              "question": "实现代码：run_clm_pt_with_peft.py？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1732",
              "question": "实现代码：run_clm_sft_with_peft.py？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1733",
              "question": "实现将豆包(云雀大模型)接入langchain体系？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1734",
              "question": "实验结果：当模型经过「充分」训练后，不管多长的预热步数最后的性能都差不多？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1735",
              "question": "对于关键任务或敏感场景，可以引入人工干预和控制机制，对生成的文本进行审查和筛选，确保生成结果的准确？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1736",
              "question": "对比实验设计：使用 不同 4 种不同预热步数（eg：0%, 0.5%, 1%, 2%）来进行实验？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1737",
              "question": "对比实验：使用了 4 种不同的最大学习率进行对比实验？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1738",
              "question": "将batch_size设置为1，通过梯度累积实现任意的有效batch_size？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1739",
              "question": "少控制权以及代价是什么非常重要？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1740",
              "question": "尽管梯度累积可以提供上述优势，但也需要注意一些问题。较大的累积步数可能导致更新频率过低，从而降低训？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1741",
              "question": "展示了一种有效的基于迭代迁移学习的方法，可以帮助模型使用少量的训练数据在不同类型的？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1742",
              "question": "左侧的图表说明了为什么使用更大的批次模型可以取得更多提升。但是当 batch size 太大时，我们？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1743",
              "question": "布之间的差异，使得模型更好地拟合真实的类别分布。它对概率的细微差异更敏感，可以更好地区分不同的类？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1744",
              "question": "常见的分布式训练框架哪一些，都有什么特点？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1745",
              "question": "开始优化参数，可以关闭offload参数，或者降低ZeRO stage，然后调整batch_size，然后继续？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1746",
              "question": "式，ConversationBufferWindowMemory 可以帮助 AI 只专注于最近的一两个问题（如配送方式），而不？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1747",
              "question": "强化学习 有哪些 动作空间（Action Spaces），他们之间的区别是什么？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1748",
              "question": "当M=1的时候，如前文所说，GPU的空置率太高，因此两个模型都没有实现训练速度和GPU个数间的线性关？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1749",
              "question": "当M=32时，表现最佳，且Transformer基本实现了训练速度和GPU个数的线性关系？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1750",
              "question": "当你从单卡穷人变成多卡富翁时，你做分布式训练的总体目标是什么呢？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1751",
              "question": "当分块引起的问题。这就是为何在处理长篇文档时，采用文本分块而非直接处理整个文档至关重要？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1752",
              "question": "当然这样可能有一个副作用，就是这批数据是质量比较差而不是模型学的不太好的？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1753",
              "question": "怎么合并中英文的词表，并使用transformers使用合并后的词表？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1754",
              "question": "想要训练1个LLM，如果只想用1张显卡，那么对显卡的要求是什么？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1755",
              "question": "据和开始训练之间？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1756",
              "question": "接下来再讲讲input_ids和labels。假设我们现在有样本：我爱北京天安门，你喜欢什么？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1757",
              "question": "推导一下 旋转位置编码 RoPE？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1758",
              "question": "提供了对显存的管理，减少显存中的碎片？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1759",
              "question": "提出了prompt ensembling：在一个批次（Batch）里同时训练同一个任务的不同 prompt（即采用多种不同方？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1760",
              "question": "搭建大模型应用遇到过那些问题？如何解决的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1761",
              "question": "改进多头机制。该系列研究探索了不同的替代多头机制？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1762",
              "question": "放后，再round到整数。这样就把浮点数映射到了INT8,逆向回到float的原理相同？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1763",
              "question": "效率：将普通模型训练代码变为分布式训练所需编写代码的行数，我们希望越少越好？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1764",
              "question": "数度量预测值与真实值之间的差异的平方，适用于连续数值的回归问题。在分类问题中使用MSE损失函数可能不？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1765",
              "question": "数据加载优化：可以使用 DataLoader 和 DataLoaderTransforms 来优化数据加载速度，从而减少训练时？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1766",
              "question": "数据库中。 完成这些步骤后，我们会指示 LLM 从指定主题中生成 num_questions 个问题，从而得？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1767",
              "question": "数据稀缺性：某些领域的数据可能相对较少，无法充分训练通用的大模型。针对特定领域进行训练的大模型？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1768",
              "question": "文本分块是自然语言处理（NLP）中的一项关键技术，其作用是将较长的文本切割成更？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1769",
              "question": "文档不完整：所有信息都可以在项目的自述文件中找到。尽管它涵盖了基础知识，但必须在问题或源代码中？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1770",
              "question": "方案一：和训练相同，直接加入Lora层？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1771",
              "question": "方法一：“self-instruct”的框架，通过自我生成来提升指令跟随能力。文章的流程是从语言模型？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1772",
              "question": "方法介绍：该类方法的核心在于仅仅通过优质数据集的获取和产生，以训练得到一个效果较好的？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1773",
              "question": "旋转位置编码 RoPE 被哪些 LLMs 应用？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1774",
              "question": "旋转位置编码 RoPE篇？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1775",
              "question": "无论我们如何准确地估计真实梯度，总存在一个最大步长？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1776",
              "question": "无论模型规模如何，最优 Reward 对应的 KL 值是一样的：这一点比较反直觉，我们通常会认为较大的模型？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1777",
              "question": "是为什么多gpu训练并不是负载均衡的,一般0卡会占用的多,这里还涉及到一个小知识点——如果程序？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1778",
              "question": "是否可以 避开训练集来处理 LLMs 测试集数据泄露 问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1779",
              "question": "显存不够：TP或者ZeRO或者PP？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1780",
              "question": "显存不够：上offload，用cpu？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1781",
              "question": "显存优化技术有哪一些，都有什么特点？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1782",
              "question": "显存优化策略篇？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1783",
              "question": "显存够用（模型能装进单卡）：DDP或ZeRO？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1784",
              "question": "显存够用： 直接用？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1785",
              "question": "显存效率:模型参数量太大，显存不够用？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1786",
              "question": "显存效率:每张卡上都保存了完整的模型、梯度、优化器状态，因此显存效率不高？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1787",
              "question": "最近关注的论文，多模态视觉大模型(CLIP,DALLE)？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1788",
              "question": "最近在探索ChatPDF和ChatDoc等方案的思路，也就是用LLM实现文档助手。在此记录一些难题和解决方案，首？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1789",
              "question": "有问题可以去看看d2的代码.？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1790",
              "question": "本节简要概述了不同的共享策略如何工作。请注意，它只适用于CPU张量 - CUDA张量将始终使用CUDA API？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1791",
              "question": "机制和生成策略可能导致模型更倾向于复制输入的文本？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1792",
              "question": "构建方法：如果是随机出来的话，完全可以用同一个batch里，其他问题的文档正例当作某一？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1793",
              "question": "样本量规模增大，训练出现OOM错？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1794",
              "question": "核心问题：选择一种策略从而最大化预期收益？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1795",
              "question": "梯度检查优化。该方式只要用于优化 动态显存？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1796",
              "question": "梯度检查点的原理：梯度检查点通过将计算图分段，在一部分计算完成后将部分中间结果存储起来，以便稍？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1797",
              "question": "梯度累积的原理：梯度累积的基本思想是，将多个小批量数据的梯度累积起来，然后一次性更新模型参数？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1798",
              "question": "检索后处理流程？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1799",
              "question": "模型显存占用 优化策略？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1800",
              "question": "模型显存占用的部分有哪些？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1801",
              "question": "模型结构+训练目标: Causal Decoder + LM。有很好的zero-shot和few-shot能力，涌现？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1802",
              "question": "模型：对比DeepLab系列，fcn，Unet，SegNet等，收敛最快的是Unet？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1803",
              "question": "此笔者开始时也十分好奇作者如何妥善处理泄露问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1804",
              "question": "每个epoch训练时整体数据分片shuffle一次，在每个进程同一时间只加载单个分段大小？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1805",
              "question": "每个进程各自读取各自的训练数据，DistributedSampler确保了进程两两之间读到的是不一样的数据？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1806",
              "question": "每张卡上的loss都是要汇总到第0张卡上求梯度，更新好以后把权重分发到其余卡。但是为什么会出现这个？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1807",
              "question": "每次训练选代中，在后向传递之后，优化器更新参数之前，插入reduce通信操作来规约梯度，确保所有？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1808",
              "question": "比特 Adam 减少 5 倍通信量： Adam 是一个在大规模深度学习模型训练场景下的有效的（也？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1809",
              "question": "没有处理过LLM文档对话的朋友可能不明白为什么要提取标题甚至是多级标题，因此我先来阐述提取标题对于？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1810",
              "question": "注意：target_modules中的作用目标名在不同模型中的名字是不一样的。query_key_value是在？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1811",
              "question": "注： (推导过程略，可参照第二部分的bubble推导流程)？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1812",
              "question": "注：bge2论文里，做基于批内负样本的对比学习时同时考虑了多任务问题。之前也介绍了，不同？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1813",
              "question": "注：为什么是在train中涉及merge_weights，其实在torch的源码中，nn.Linear.eval()实际上是调？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1814",
              "question": "注：使用DeepSpeed其实和写一个pytorch模型只有部分区别，一开始的流程是一样的？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1815",
              "question": "测试你的显卡利用率 实现细节篇？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1816",
              "question": "混合精度训练:采用bfloat16，而不是foat16来训练？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1817",
              "question": "混合精度训练是指在训练过程中同时使用FP16（半精度浮点数）和FP32（单精度浮点数）两？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1818",
              "question": "混合精度训练的关键技术是什么？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1819",
              "question": "混合精度训练的大致思路是在 forward pass 和 gradient computation 的时候使用 fp16 来加速，但是在更新参数？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1820",
              "question": "混合精度训练的缺点是什么？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1821",
              "question": "混合精度训练：可以使用半精度浮点数加速模型训练，从而减少 GPU 内存使用和提高训练速度？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1822",
              "question": "混合精度（BF16/FP16）：降低训练显存的消耗，还能将训练速度提升2-4倍？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1823",
              "question": "添加自定义模型：虽然可以合并自己的模型，但如果模型没有使用与vLLM中现有模型类似的架构，则过程会？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1824",
              "question": "然后你对该问题一丝不解，没事，下文将用图例把详细步骤介绍清楚？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1825",
              "question": "爱\", \"北京\", \"天安门\", \"你\", \"喜欢\", \"什么\", \"？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1826",
              "question": "理解此问题的情形是：已有的lora模型只训练了一部分数据，要训练另一部分数据的话，是在这个lora上继续训？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1827",
              "question": "理解能力：提出一些需要深入理解文本的问题，看模型是否能准确回答？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1828",
              "question": "生成性输出 用户意图保留 技术实现？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1829",
              "question": "用户可能会对特定新闻事件提出问题，如“最近的经济峰会有什么重要决策？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1830",
              "question": "由于 fp16 混合精度大大减少了内存需求，并可以实现更快的速度，因此只有在在此训练模式下？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1831",
              "question": "由于ckpt存储时间不可控，不能确定是否小于下一个step的执行时间，所以内存踩踏的问题不可避？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1832",
              "question": "的是基于对比学习构造的语义向量这套思路，当然简单的基于词袋或者tfidf的方案也是可以的。有？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1833",
              "question": "的结果是什么。比如一般看一下input_ids里面的特殊标记，labels是怎么构造的。举个例子，cpm-bee在forward？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1834",
              "question": "直接使用nn.DataParallel的时候，训练采用多卡训练，会出现一个warning？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1835",
              "question": "相信大家都不能接受这个结果。为了解决这个问题，混合检索是一种解决方案。该策略利用了矢量搜索和关键词？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1836",
              "question": "相同训练数据下，Reward Model 越大 actor 模型能够获得更高的真实 reward？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1837",
              "question": "相比之下，均方误差（MSE）损失函数更适用于回归问题，其中目标是预测连续数值而不是类别。MSE损失函？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1838",
              "question": "确保所有worker都从相同的初始化模型参数开始训练。在训练开始前，通常会将0号卡的1模型参数通信同步？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1839",
              "question": "确定关键负样本：根据具体任务的特点，可以重点关注一些关键的负样本，而不是对所有负样？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1840",
              "question": "确性和稳定性、如何平衡模型的效益和风险等。这些挑战需要多方面的研究和合作，以确保大模型能够健康？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1841",
              "question": "稀疏 attention。将稀疏偏差引入 attention 机制可以降低了复杂性？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1842",
              "question": "第二步：mark_only_lora_as_trainable(self.model, self.peft_config.bias)。保留lora部分的参数可训练，其余？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1843",
              "question": "红娘、小玉等通共熟套之旧稿。我师意为何如？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1844",
              "question": "纯大模型AI模式，最初直接是大模型机器人直接和用户对话，全流程都是大模型对话走流程？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1845",
              "question": "线性化 attention。解开 attention 矩阵与内核特征图，然后以相反的顺序计算 attention 以实现线性复杂度？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1846",
              "question": "练呢，还是跟base 模型合并后再套一层lora，或者从头开始训练一个lora？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1847",
              "question": "练更稳定，可以缓解训练不稳定问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1848",
              "question": "练的。如何进行deepspeed训练，可以参考基于deepspeed构建大模型分布式训练平台？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1849",
              "question": "经过充分训练后，学习率越大（紫色），下游性能最好，上游性能最差（忘得最多）？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1850",
              "question": "结合 代码 讲解 大模型（LLMs）agent 思路？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1851",
              "question": "络带宽是64Gps，后面把多机之间的网络带宽调整为800Gps，问题解决？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1852",
              "question": "缺乏多样性的训练数据：虽然大型语言模型可以处理大规模的数据，但如果训练数据中缺乏多样性的语言表？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1853",
              "question": "缺点很明显，参与训练的模型参数量不多，也就百万到千万级别的参数量，所以效果比全量微调差很多。可能在？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1854",
              "question": "缺点：在长文本生成任务上效果差，训练效率低？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1855",
              "question": "缺点：训练效率低？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1856",
              "question": "者的反馈与模型生成的文本进行比较，可以计算出一个差异度量，用作奖励信号的一部分。这样，模型可以？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1857",
              "question": "而是 多个模型，那么 如何 保持和加载 这些模型呢？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1858",
              "question": "背景：深度神经网络通常包含许多层和参数，模型的训练需要计算前向传播（forward pass）来获得预测结？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1859",
              "question": "能更快地训练模型。理想状况下，训练的速度和GPU的数量成线性关系。即GPU量提升x倍，训练速度也能？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1860",
              "question": "能训练更大的模型。理想状况下，模型的大小和GPU的数量成线性关系。即GPU量提升x倍，模型大小也能？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1861",
              "question": "节中阐述的前两种重复问题，无法解决输入多次相同prompt输出单调性的问题）？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1862",
              "question": "虽然 使用 nn.DataParallel 函数 能够进行多GPU运算，但是 会导致 程序花费的时间 不减反增，这是为什么呢？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1863",
              "question": "要。专门针对某个领域进行训练的大模型可以更好地掌握该领域的语言特点，生成更符合该领域要求的文？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1864",
              "question": "要求所有的GPU都在同一个节点上（不支持分布式）？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1865",
              "question": "解决了Prompt Tuning无法在小模型上有效提升的问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1866",
              "question": "解决方案：Tiktoken是OpenAI开发的Python库，用于更有效地解决令牌计数问题。它提供了一种简单的方法？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1867",
              "question": "解决方法：微调数据优化派？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1868",
              "question": "解决方法：训练过程改造派？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1869",
              "question": "解决方法：通常在领域训练的过程中加入通用数据集？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1870",
              "question": "解决问题。由于我是一个计算机程序，所以我没有实际的存在，只能通过互联网来与用户交？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1871",
              "question": "解股市最近的趋势以及如何分配我的投资组合”）。ConversationTokenBufferMemory 可以帮助 AI 聚焦于？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1872",
              "question": "解释一下这里为什么这么关注训练前期，是因为在真实训练中，我们可能不一定会增强图中所示的 250B 这么多？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1873",
              "question": "解释了为什么第一块卡的显存会占用的比其他卡要更多一些？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1874",
              "question": "言模型。这样的训练目标可能使得模型更倾向于生成与输入相似的文本，导致复读机问题的出现？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1875",
              "question": "计算效率:训练数据量多，模型参数量大，计算量大，单机训练时间久？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1876",
              "question": "训练 大语言模型 存在问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1877",
              "question": "训练任务比较稳定，效果比较好？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1878",
              "question": "训练大语言模型 (LLM) 裁判：然后，ARES 对轻量级语言模型进行微调，利用合成数据集训练？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1879",
              "question": "训练式位置编码篇 存在哪些问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1880",
              "question": "训练式位置编码篇 应用场景？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1881",
              "question": "训练式位置编码篇？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1882",
              "question": "训练效率方面: 多机多卡训练，增加训练机器可以线性缩短训练时间？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1883",
              "question": "训练数据介绍？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1884",
              "question": "训练时，原模型固定，只训练降维矩阵A和升维矩阵B？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1885",
              "question": "训练更大的模型时，每块GPU里不仅要存模型参数，还要存中间结果（用来做Backward）。而更大的模型意？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1886",
              "question": "训练目标的限制：大型语言模型的训练通常是基于自监督学习的方法，通过预测下一个词或掩盖词来学习语？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1887",
              "question": "训练目标相同：模型需要根据提供的文本来预测「下一个单词」？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1888",
              "question": "训练过程：不会显著影响训练过程，训练速度不变，会引起非常细微的模型效果损失？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1889",
              "question": "训练难度加大。不太好训练，省了显存，但不一定省时间。具体来讲，大部分prompt现在只是parameter？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1890",
              "question": "论文 提出了一种基于人类提供的规则列表的自我训练机制。与前面提到的InstructGPT论文类似，也使用了强化？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1891",
              "question": "话，半精度浮点型的tensor又容易overflow（变成inf或者NaN）。所以动态估计的原理就是在不出现inf或者NaN？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1892",
              "question": "该功能一般都会开启，来保证生成的句子不犯很离谱的连续重复问题。（该方法仅能解决1.1节中阐述的前两种？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1893",
              "question": "语序可能会出现一定的问题。（对其他领域生成结果的影响有待进一步探索）？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1894",
              "question": "语言风格和惯用语：各个领域通常有自己独特的语言风格和惯用语，这些特点对于模型的训练和生成都很重？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1895",
              "question": "调整模块间的工作流程在调整模块间流程的领域，重点在于加强语言模型与检索模型之间的互动？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1896",
              "question": "过队列发送或通过其他机制共享转移到共享内存中？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1897",
              "question": "这个问题其实暗含着这样的意思：为什么需要自动混合精度，也就是torch.FloatTensor和torch.HalfTensor的混？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1898",
              "question": "这些大模型在训练的时候就几乎把整个互联网的数据都扫了一个遍，因此，很难保证测试用的数据集？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1899",
              "question": "这样，随着卡数的增加，每张卡 用于 模型训练的显存占用将减低，能？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1900",
              "question": "这里可以看到，针对一些问题，skylark-chat 有时不是直接回复结果，而是会在前面解释一通，这？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1901",
              "question": "进行多任务同时进行训练的时候，要尽量使各个任务的数据量平衡？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1902",
              "question": "适配器微调（Adapter-tuning）特点是什么？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1903",
              "question": "选取的训练数据要干净、并具有代表性？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1904",
              "question": "通过查看服务器相关监控，发现是网络带宽打满，上不去了，其他系统监控基本正常。原理初始的多机之间的网？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1905",
              "question": "避免在问题中使用 \"和 \"字，因为它可以分解成多个问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1906",
              "question": "那么有什么办法 能够 解决 基于llama家族的模型 对于 中文的支持不太友好 问题呢？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1907",
              "question": "重复以上步骤 num_count 次,每次改变上下文并生成不同的问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1908",
              "question": "重复问题，无法解决输入多次相同prompt输出单调性的问题）？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1909",
              "question": "重视代码清晰度，以最小化进行实现？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1910",
              "question": "针对不同任务采用不同的提示长度。提示长度在提示优化方法的超参数搜索中起着核心作用。在实验中，发？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1911",
              "question": "长度外推问题 的 解决方法 有哪些？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1912",
              "question": "长期记忆：为agent提供保留和召回长期信息的能力，通常利用外部向量存储和检索实现？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1913",
              "question": "问题: 我有一张订单，一直没收到，可以帮我查询下吗？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1914",
              "question": "问题一解决方法：ALIBI、KERPLE、Sandwich、XPOS、PI、NTK-RoPE(目前看起来这个最？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1915",
              "question": "问题一：首先如何区分单双栏论文？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1916",
              "question": "问题二解决方法：softmax的时候加一个log512​n系数？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1917",
              "question": "问题二：双栏论文如何确定区块的先后顺序？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1918",
              "question": "问题定位：GPU显存不够？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1919",
              "question": "问题定位：训练时用的是bf16，使用时是fp16。常常发生于google在TPU上train的模型，如？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1920",
              "question": "问题描述：模型训练的样本数量从10万，增大300万，训练任务直接报OOM了？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1921",
              "question": "问题，无法解决输入多次相同prompt输出单调性的问题）？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1922",
              "question": "问题：健康饮食的主要特点是什么？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1923",
              "question": "问题：居里夫人的主要成就是什么？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1924",
              "question": "间距离远。文档召回场景下，做对比学习（有监督）需要三元组（问题，文档正例，文档负？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1925",
              "question": "需要消耗大量的计算资源和存储资源来训练和运行，这会增加经济和环境的负担。据估计，训练一个GPT-3？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1926",
              "question": "静态显存基本由模型参数量级决定？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1927",
              "question": "项目里面关于数据预处理的部分。找一份小的数据集，将这部分单独拿出来运行一下，看一下输出是什么。返回？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1928",
              "question": "领域数据训练后，通用能力往往会有所下降，如何缓解模型遗？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1929",
              "question": "领域词表扩增真实解决的问题是解码效率的问题，给模型效果带来的提升可能不会有很大？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1930",
              "question": "题1：如何让LLM简要、准确回答细粒度知识？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1931",
              "question": "题2：如何让LLM回答出全面的粗粒度（跨段落）知识？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1932",
              "question": "题处理的摘要，以便于更有效地识别和解决问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1933",
              "question": "题，ConversationBufferMemory 可以用来记住整个与用户的对话历史，可以帮助 AI 在回答网络问题时还？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1934",
              "question": "首先，我们需要了解如何根据参数量估计模型大致所需的 RAM，这在实践中有很重要的参考意义。我们需要通？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1935",
              "question": "高效的分布式训练框架和充沛优质的硬件资源？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1936",
              "question": "高质量的训练语料？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1937",
              "question": "默认情况下，半精度训练使用 fp16 作为reduction操作的默认值？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1938",
              "question": "（Graph of Operations，GoO),以LLM作为引擎自动执行，从而提供解决复杂问题的能力。某？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            }
          ]
        },
        {
          "subcategory": "LLM-应用与Agent",
          "count": 66,
          "questions": [
            {
              "id": "llm_1939",
              "question": "Agent 如何获取上下文对话信息？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1940",
              "question": "RAG Fusion 优化策略？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1941",
              "question": "RAG 与微调（Fine-tuning）的协同作用？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1942",
              "question": "RAG 中长上下文的处理问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1943",
              "question": "RAG 存在哪些局限性？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1944",
              "question": "RAG 工作流程？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1945",
              "question": "RAG 新模式 优化策略？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1946",
              "question": "RAG 有哪些优点？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1947",
              "question": "RAG 未来发展方向？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1948",
              "question": "RAG 流程之前，务必先要清理数据？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1949",
              "question": "RAG 生态系统？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1950",
              "question": "RAG 的工程应用？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1951",
              "question": "RAG 的水平扩展？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1952",
              "question": "RAG 的组织方法具有高度灵活性，能够根据特定问题的上下文，对 RAG 流程中的模块进行替换或重新配置。在？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1953",
              "question": "RAG 的鲁棒性研究？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1954",
              "question": "RAG-Fusion 优势？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1955",
              "question": "RAG-Fusion 挑战？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1956",
              "question": "RAG-Fusion 的优势和不足？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1957",
              "question": "RAGAS？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1958",
              "question": "RAG基础功能篇？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1959",
              "question": "RAG检索召回率低，一般都有哪些解决方案呀。尝试过不同大小的chunk，和混合检索。效果都不太好，然？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1960",
              "question": "RAG检索召回率低，一般都有哪些解决方案呀。尝试过不同大小的chunk，和混合检索？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1961",
              "question": "RAG（Retrieval-Augmented Generation）评测面？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1962",
              "question": "Self-RAG 的 代码实战？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1963",
              "question": "Self-RAG 的 创新点是什么？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1964",
              "question": "bert在RAG中具体是起到了一个什么作用，我刚搜了下nsp的内容，但有点没法将这几者？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1965",
              "question": "multi_choice_prompt = \"\"\"请针对 >>> 和 <<< 中间的用户问题，选择一个适合的工具去回答？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1966",
              "question": "一项复杂任务通常会包含很多步骤，Agent需要了解这些步骤是什么并提前规划？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1967",
              "question": "个人和专业效用：从个人应用（如浏览笔记）到更专业的集成，RAG在提高生产力和内容质量？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1968",
              "question": "为了解决这个问题，在送到 RAG 之前，我们先发生给 LLM 重写此查询？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1969",
              "question": "为什么 需要 RAG-Fusion？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1970",
              "question": "从RAG的工作流程看，RAG 模块有：文档块切分、文本嵌入模型、提示工程、大模型生成？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1971",
              "question": "任务适应模块： 该模块致力于将 RAG 调整以适应各种下游任务？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1972",
              "question": "众多研究表明，RAG 在开放式问题回答、事实验证等多种下游任务中表现优异。RAG 模型不仅提升了下游应用？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1973",
              "question": "伦理和用户体验考虑：拥有巨大力量的同时也伴随着巨大的责任。对于RAG Fusion来说，操作？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1974",
              "question": "使用过程中，对 RAG 的维护也很重要，还需要添加机制来更新过时的文档？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1975",
              "question": "典型RAG架构中，向量数据库 存在哪些问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1976",
              "question": "典型RAG架构中，向量数据库进行上下文增强 存在哪些问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1977",
              "question": "初始化 RAG？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1978",
              "question": "可能是一个欧雷卡时刻。这使RAG Fusion区别于其他传统搜索模型？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1979",
              "question": "向量搜索融合：RAG通过将向量搜索功能与生成模型相结合，引入了一种新颖的范式。这种融？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1980",
              "question": "图1 RAG工作流程（with memory）？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1981",
              "question": "在 RAG 技术流程中，处理大量数据时常会遇到一个难题：系统若无法高效地管理和加工这些数？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1982",
              "question": "基于置信区间对 RAG 系统排名：最后，ARES 使用这些裁判模型为 RAG 系统打分，并结合手？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1983",
              "question": "多向量检索器多模态RAG篇？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1984",
              "question": "大模型（LLMs）RAG —— 关键痛点及对应解决方案？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1985",
              "question": "大模型（LLMs）RAG 优化策略 —— RAG-Fusion篇？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1986",
              "question": "大模型（LLMs）RAG 版面分析——文本分块面？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1987",
              "question": "大模型（LLMs）RAG 版面分析——表格识别方法篇？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1988",
              "question": "如何合成 RAG 测试集？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1989",
              "question": "常见LLM Agent框架或者应用 有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_1990",
              "question": "当前搜索技术的限制：RAG受到限制的方面与我们的检索式基于词汇和向量的搜索技术相同？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1991",
              "question": "接下来，在RAG上运行这些问题以获得预测结果？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1992",
              "question": "更优质的源材料：使用RAG Fusion时，你的搜索深度不仅仅是“增强”——而是被放大。重新排？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1993",
              "question": "检索增强生成(RAG) 优化策略篇？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1994",
              "question": "模块化 RAG 优化策略？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1995",
              "question": "模型优化：可以使用 Apex 或 TorchScript 等工具来优化模型性能？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1996",
              "question": "此外，增强 RAG 模型的可解释性，让用户更清楚地理解模型如何以及为何作出特定反应，也是一项重要任务？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1997",
              "question": "的文本和格式。下面有一段示例代码，可以指导你如何下载、设置并使用这个工具包？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1998",
              "question": "自动纠正用户查询：该系统不仅解释，还优化用户查询。通过生成多个查询变体，RAG Fusion？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_1999",
              "question": "说一下 RAG-Fusion 工作流程？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2000",
              "question": "说一下 RAG-Fusion 核心技术？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2001",
              "question": "输入什么，输出什么。”如果源数据质量差，比如充斥着冲突信息，那么无论你如何构建 RAG 流？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2002",
              "question": "过于冗长的风险：RAG-Fusion的深度有时可能导致信息泛滥。输出可能过于详细，令人不堪重？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2003",
              "question": "这里可以看到，针对问题按预设的结果输出了所需要的工具，并做了格式，对格式化的json数据就？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2004",
              "question": "（RAG）流程中常见的五个额外痛点。更为关键的是，我们将深入讨论这些 RAG 痛点的解决策？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            }
          ]
        },
        {
          "subcategory": "LLM-推理与工程优化",
          "count": 83,
          "questions": [
            {
              "id": "llm_2005",
              "question": "ChatGML在A800单卡推理耗时统计？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2006",
              "question": "ChatGML在V100单卡推理耗时统计？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2007",
              "question": "ChatGML在V100单卡的推理耗时大约高出A800单卡推理的40%？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2008",
              "question": "LLMs 推理存在哪些挑战？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2009",
              "question": "LLMs 推理性能面？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2010",
              "question": "Memory-Efficient 的 LLMs 的训练/微调/推理方法？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2011",
              "question": "MoE为什么可以实现更大模型参数、更低训练成本？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2012",
              "question": "PP推理时，是一个串行的过程，1个GPU计算，其他空闲，有没有其他方式？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2013",
              "question": "Reflexion ：是一个让Agent具备动态记忆和自我反思能力以提高推理能力的框架。Reflexion采？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2014",
              "question": "Tree of Thoughts：进一步扩展CoT，在每一步都探索多种推理的可能性。它首先将问题分解为？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2015",
              "question": "bit量化优化。该方式只要用于优化 静态显存？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2016",
              "question": "cpu推理速度约10token/s？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2017",
              "question": "pdsh是一个并行分布式运维工具，它的优点是只需要在一台机上运行脚本就可以，pdsh会自动帮？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2018",
              "question": "vLLM 如何 优化 大模型并行推理加速？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2019",
              "question": "vLLM 用于大模型并行推理加速 存在什么问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2020",
              "question": "上节讲到 DP 只支持 单机多卡场景，主要原因是 DP 无法数据并行中通讯负载不均的问题， 而 DDP 能够解决？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2021",
              "question": "为什么需要流水线并行（Pipeline Parallelism）？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2022",
              "question": "什么是 张量并行 (intra-layer)？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2023",
              "question": "优化器相关的并行（如：ZeRO（零冗余优化器，在执行的逻辑上是数据并行，但可以达到模型并行的显存？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2024",
              "question": "优点：没有推理延时？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2025",
              "question": "使用 DistributedDataParallel（分布式并行）时，显存分布不均衡问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2026",
              "question": "使用flash attention（和v2）和Paged attention优化transformer推理代码：并非所有模型都内置了对这些优化？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2027",
              "question": "例展示了如何创建数据处理流程并设置num_workers，以实现并行处理？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2028",
              "question": "假如有超多的8卡A100节点（DGX A100），如何应用3D并行策略？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2029",
              "question": "六、MoE为什么可以实现更大模型参数、更低训练成本？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2030",
              "question": "冻结模型原始权重，只训练prompts参数，训练完成后，只用同一个模型可以做多任务推理？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2031",
              "question": "分布式并行及显存优化技术并行技术有哪一些，都有什么特点？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2032",
              "question": "分布式训练：可以在多个 GPU 或多台机器上并行训练模型，从而缩短训练时间和提高模型性能？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2033",
              "question": "动机：大模型（LLMs）现在是 NLP 领域的最主流方法之一，但是大模型的训练/微调/推理需要的内存也越来？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2034",
              "question": "单卡A6000和8核AMD的推理速度通常为 10:1？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2035",
              "question": "吞吐量：推理服务器在所有用户和请求中每秒可生成的输出词元数？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2036",
              "question": "四、LLMs 推理存在哪些挑战？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2037",
              "question": "在推理时如何先进行weight的合并在加载模型进行推理？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2038",
              "question": "在简单的RAG模型中，比较性问题往往处理得不够好。一个提升RAG推理能力的有效方法是加入？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2039",
              "question": "多GPU并行训练的原理就是将模型参数和数据分布到多个GPU上，同时利用多个GPU计算加速训练过程。具体？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2040",
              "question": "多节点（但节点间通信慢）：考虑用流水线并行，参考另一个大佬的实现？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2041",
              "question": "大模型训练的三种并行是什么？通讯开销比？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2042",
              "question": "大模型迭代：基于正反馈微调模型、量化感知训练、提供大context window的推理模型？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2043",
              "question": "大模型（LLMs）推理面？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2044",
              "question": "如何 使用 基于lora的llama2 做推理？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2045",
              "question": "如何准确衡量模型的推理速度呢？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2046",
              "question": "如何解决 人工产生的偏好数据集成本较高，很难量产问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2047",
              "question": "如果对整体推理时延有具体目标，有哪些有效的启发式方法来评估模型？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2048",
              "question": "将BA加到W上可以消除推理延迟？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2049",
              "question": "小，提高训练速度。Pipeline并行的缺点是，由于每个计算机只处理部分数据，因此每个计算机的结果都会有一？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2050",
              "question": "并行化训练加速？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2051",
              "question": "式询问同一个问题），这样相当于训练了不同模型，比模型集成的成本小多了？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2052",
              "question": "张量并行:因模型结构而异，实现难度大？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2053",
              "question": "张量并行只有在nvlink环境下才会起正向作用，但提升也不会太明显？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2054",
              "question": "当对多个任务执行推理时，动态地减少了运行时的计算开销，并在很大程度上保持了任务性能？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2055",
              "question": "推理时，可将BA加到原参数上，不引入额外的推理延迟？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2056",
              "question": "推理过程：反复加载 巨大 的 KV cache , 导致 内存开销大，性能是内存受限？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2057",
              "question": "推理速度会变慢？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2058",
              "question": "推理阶段不引入额外计算量？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2059",
              "question": "数据并行 如何 提升效率？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2060",
              "question": "数据并行:计算效率高、实现简单？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2061",
              "question": "数据并行（如：PyTorch DDP）？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2062",
              "question": "文本生成的速度：实验多次，发现vLLM的推理速度是最快的？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2063",
              "question": "明确这两个训练目标后，我们来看并行范式的设计者，是如何在现有硬件限制的条件下，完成这两个目标？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2064",
              "question": "显存效率:减少的显存与流水线并行度成正比。但流水线并行不会减少每层中间激活的显存占用？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2065",
              "question": "显存效率:随着并行度增加，成比例地减少显存占用。是减少单层神经网络中间激活的唯一方法？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2066",
              "question": "本文主要从数据角度来探讨如何降低 LLM 训练阶段的成本，提高数据效率。为了实现该目的，作者通过从现有？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2067",
              "question": "梯度爆炸的问题，影响模型的收敛性和训练效果。在推理阶段，生成长句子可能会增加模型的错误率和生成？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2068",
              "question": "模型全量微调对每个任务训练一个模型，开销和部署成本都比较高？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2069",
              "question": "流水线并行（Pipeline Parallelism） 优化目标是什么？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2070",
              "question": "用 3D 并行化实现万亿参数模型训练。DeepSpeed 实现了三种并行方法的灵活组合：ZeRO 支？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2071",
              "question": "目前 LLM推理框架 有 哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2072",
              "question": "缺点：不过会增加推理延时因为多了lora层的计算，适合线下测评用？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2073",
              "question": "而Gpipe提出的流水线并行，就是用来解决这两个主要问题的？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2074",
              "question": "行动（Action）：通过提示词使得LLM用自然语言生成整体的推理过程？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2075",
              "question": "解决方案，对数据并行处理，具体实现参考海量数据高效训练，核心思想自定义数据集本次的？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2076",
              "question": "计算效率:当增加并行度时，单卡的计算量是保持恒定的，可以实现近乎完美的线性扩展。但规约梯？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2077",
              "question": "训练成本：不同的训练工具，训练同样的大模型，成本是不一样的。对于大模型，训练一次动辄上百万/千万？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2078",
              "question": "训练速度。数据并行的优点是，每个计算机都会处理全部的模型，因此结果更加准确。缺点是，由于每个计算机？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2079",
              "question": "论文方法：在文档向量空间找到和文档正例最相近的文档片段当作文档负例，训练向量化模？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2080",
              "question": "该问题 的 核心在于 Ring-AllReduce。它由百度最先提出，非常有效地解决了数据并行中通讯负载不均的问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2081",
              "question": "通过从较低的 Transformer 层删除可变数量的Adaper来提升推理速度？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2082",
              "question": "通过在Transformer层中嵌入Adapter结构，在推理时会额外增加推理时长？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2083",
              "question": "通过直接提示进行文本推理？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2084",
              "question": "逻辑推理能力：请模型回答需要进行推理或逻辑分析的问题，如概率或逻辑推理等。这可以帮助判断模型对？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2085",
              "question": "重新训练时可以直接加载向量化后的数据？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2086",
              "question": "问题一：位置编码不一致（推理的时候有训练没见过的位置编码）？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2087",
              "question": "问题二：attention span大小不一致（推理的时候attention span更大，导致墒增）？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            }
          ]
        },
        {
          "subcategory": "LLM-训练与对齐",
          "count": 111,
          "questions": [
            {
              "id": "llm_2088",
              "question": "AdaLoRA篇？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2089",
              "question": "ChatGLM-6B LoRA后的权重多大？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2090",
              "question": "Fine-tuning 需要改变预训练阶段模型参数， 可能带量灾难性遗忘问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2091",
              "question": "LLaMA 2 的 RLHF 篇？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2092",
              "question": "LoRA 微调参数量怎么确定？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2093",
              "question": "LoRA 系列篇？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2094",
              "question": "LoRA 缺点是什么？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2095",
              "question": "LoRA 高效微调 如何避免过拟合？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2096",
              "question": "LoRA微调⽅法为啥能加速训练？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2097",
              "question": "LoRA微调方法为啥能加速训练？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2098",
              "question": "LoRA权重是否可以合⼊原模型？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2099",
              "question": "LoRA权重是否可以合入原模型？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2100",
              "question": "LoRA这种微调方法和全参数比起来有什么劣势吗？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2101",
              "question": "PEFT库 中 LoRA 模块 _find_and_replace() 实现思路？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2102",
              "question": "PEFT库 中 LoRA 模块 代码介绍？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2103",
              "question": "PEFT库 中 LoRA 模块 整体实现思路？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2104",
              "question": "PEFT库 中 LoRA 模块使用？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2105",
              "question": "PPO 中 采样过程：学生回答问题的过程，是模型根据提示（prompt）输出回答（response）的过程，或者说是？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2106",
              "question": "RAG 结合 SFT？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2107",
              "question": "RLHF 在实践过程中存在哪些不足？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2108",
              "question": "RLHF 实践篇？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2109",
              "question": "RLHF 替代方案篇？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2110",
              "question": "RLHF 的具体工程是什么？包含了哪几个模型？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2111",
              "question": "RLHF：在训练instructGPT时，首先使用有人类生成的示例对模型进行预训练。然后，通过与人类评估者进？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2112",
              "question": "RL发展路径（至PPO）？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2113",
              "question": "Reinforcement Learning with Human Feedback (RLHF)篇？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2114",
              "question": "SFT数据集如何生成？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2115",
              "question": "huggingface大模型如何加载多个LoRA并随时切换？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2116",
              "question": "instructGPT是一种基于强化学习的文本生成模型，其核心原理涉及两个概念：RLHF（Reinforcement Learning？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2117",
              "question": "model_name_or_path：预训练模型地址？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2118",
              "question": "tokenizer_name_or_path：：预训练模型 tokenizer 地址？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2119",
              "question": "七、huggingface大模型如何加载多个LoRA并随时切换？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2120",
              "question": "不足点2：三个阶段的训练（SFT->RM->PPO）过程较长，更新迭代较慢？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2121",
              "question": "不足点3：PPO 的训练过程同时存在4个模型（2训练，2推理），对计算资源的要求较高？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2122",
              "question": "为什么需要 对 llama2 做 基于lora的二次预训练？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2123",
              "question": "为什么需要对预训练模型进行指令微调？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2124",
              "question": "为什么需要进行继续预训练？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2125",
              "question": "主流是LLaMA，因为scaling法则，可能LLaMA做了充分预训练。（当然有版权问题）？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2126",
              "question": "举例描述一下 大语言模型的RLHF？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2127",
              "question": "五、使用 LoRA 对 大模型进行 高效参数微调，如何进行存储？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2128",
              "question": "介绍一下 LLM的经典预训练Pipeline？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2129",
              "question": "传统的微调范式利用预训练模型去对不同的下游任务进行微调，对每个任务都要保存一份微调后的模型权？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2130",
              "question": "使用 LoRA 对 大模型进行 推理，如何进行加载？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2131",
              "question": "使用 LoRA 对 大模型进行 高效参数微调，如何进行存储？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2132",
              "question": "使用一种新颖的高精度技术将预训练模型量化为 4 bit？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2133",
              "question": "六、使用 LoRA 对 大模型进行 推理，如何进行加载？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2134",
              "question": "单节点或者多节点（节点间通信快）：直接deepspeed ZeRO吧。（笔者用了linly的增量预训练代码，但有？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2135",
              "question": "取决于预训练数据和微调任务的数据分布是否一致，分布一致，100条就够，分布差异大就需要多？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2136",
              "question": "只更新了部分参数：比如LoRA原论文就选择只更新Self Attention的参数，实际使用时我们还可以选择只更新？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2137",
              "question": "在NLU上，prompt tuning对于正常大小的预训练模型表现不佳？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2138",
              "question": "在之前讲过的继续预训练之后，我们应该对数据处理到训练、预测的整个流程有所了解，其实，基本上过程是差？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2139",
              "question": "在训练时，固定住原来预训练模型的参数不变，只对新增的Adapter结构进行微调。同时为了保证训练的高效？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2140",
              "question": "在预训练好的模型上进行「有监督微调」（SFT）？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2141",
              "question": "在预训练数据集上训练出的基础模型？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2142",
              "question": "在预训练阶段，模型会从大量无标注文本数据集中学习通用知识？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2143",
              "question": "基于RM模型使用PPO算法微调SFT模型？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2144",
              "question": "基于lora的llama2二次预训练 参数介绍？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2145",
              "question": "基于lora的llama2二次预训练 的思想是什么？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2146",
              "question": "基于lora的llama2二次预训练 语料构建思路？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2147",
              "question": "基于lora的llama2二次预训练？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2148",
              "question": "增强用户意图：对齐RAG Fusion的核心设计是作为一个富有同情心的人工智能，揭示用户努力？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2149",
              "question": "增量预训练（Pretrain）样本拼接篇？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2150",
              "question": "大模型（LLMs）增量预训练篇？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2151",
              "question": "大模型（LLMs）强化学习—— PPO 面？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2152",
              "question": "大模型（LLMs）强化学习——RLHF及其变种面？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2153",
              "question": "大语言模型RLHF 采样篇？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2154",
              "question": "大语言模型RLHF中的PPO主要分哪些步骤？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2155",
              "question": "如何 基于lora的llama2二次预训练？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2156",
              "question": "如何 让Prompt Tuning能够在不同参数规模的预训练模型、针对不同下游任务的结果上都达到匹敌Fine-tuning的？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2157",
              "question": "如何使用 PEFT库 中 LoRA？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2158",
              "question": "如何在已有LoRA模型上继续训练？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2159",
              "question": "如何对 继续预训练 数据预处理？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2160",
              "question": "对预训练模型进⾏指令微调 tokenization 如何构建？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2161",
              "question": "对预训练模型进⾏指令微调 模型 如何构建？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2162",
              "question": "对预训练模型进行指令微调 tokenization 如何构建？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2163",
              "question": "对预训练模型进行指令微调 数据 如何处理？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2164",
              "question": "对预训练模型进行指令微调 模型 如何构建？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2165",
              "question": "对齐则是教会它与用户交互时如何选择子分布。如果假说正确，对齐主要有关于学习方式，那么该假说的一个推？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2166",
              "question": "对齐模块： 在 RAG 的应用中，查询与文本之间的对齐一直是影响效果的关键因素。在模块化 RAG？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2167",
              "question": "对齐（Alignment）：通过微调的方式，将语言模型与人类的偏好、价值观进行对齐，这也是RLHF机制发挥的地？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2168",
              "question": "引入多任务学习，先在多任务的prompt上进行预训练，然后再适配下游任务？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2169",
              "question": "引入重参数化，如：LoRA、AdaLoRA、QLoRA？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2170",
              "question": "指示微调（Prompt-tuning） 不改变预训练阶段模型参数，而是通过微调寻找更好的连续 prompt，来引导已？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2171",
              "question": "数据偏差：大型语言模型通常是通过预训练阶段使用大规模无标签数据进行训练的。如果训练数据中存在大？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2172",
              "question": "是否可以逐层调整LoRA的最优rank？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2173",
              "question": "未经过预训练的模型（蓝色）无论是上游任务还是下游任务，都不如预训练过的模型效果？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2174",
              "question": "标准InstructGPT 中 RLHF PPO方法 思路：对同一个提示下的4-9个模型输出并进行排序？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2175",
              "question": "特点。LoRA的优点是它的低秩分解很直观，在不少场景下跟全量微调的效果一致，以及在预测阶段不增加推理？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2176",
              "question": "的发展中，研究者们发现，在检索器中添加一个可训练的 Adapter 模块能有效解决对齐问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2177",
              "question": "第一阶段：扩充领域词表，比如金融领域词表，在海量领域文档数据上二次预训练LLaMA模？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2178",
              "question": "第二阶段：构造指令微调数据集，在第一阶段的预训练模型基础上做指令精调。还可以把指令？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2179",
              "question": "简单介绍一下 RLHF？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2180",
              "question": "练。预训练的方式一般都是相同的，简单来说，就是根据上一个字预测下一个字是什么。为了方便起见，我们这？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2181",
              "question": "缺乏对适配器（LoRA、QLoRA等）的支持：当针对特定任务进行微调时，开源LLM具有重要价值。然而？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2182",
              "question": "自回归语言模型，预训练和下游应用是完全一致的，严格遵守只有后面的token才能看到前面的？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2183",
              "question": "训练数据格式不同：有监督微调（Supervised Tinetuning）需要人工标注的训练数据，预训练（Pre-？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2184",
              "question": "训练数据量不同：有监督微调（Supervised Tinetuning）需要训练数据量比 预训练（Pre-training）？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2185",
              "question": "迁移学习和预训练模型：利用预训练模型或迁移学习的方法，可以在其他领域或任务中利用已？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2186",
              "question": "问题，不要大于预训练时的学习率？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2187",
              "question": "阵。LoRA 的作者根据这一特点将 update matrix reparametrize 为两个低秩矩阵的积积 ？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2188",
              "question": "随着预训练模型参数量的增加，Prompt Tuning的方法会逼近全参数微调的结果？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2189",
              "question": "预训练 数据参数介绍？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2190",
              "question": "预训练 数据集 下载？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2191",
              "question": "预训练 模型参数介绍？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2192",
              "question": "预训练模型参数量变多，在特定任务下进行全量微调即昂贵又耗时？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2193",
              "question": "预训练模型参数？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2194",
              "question": "预训练（Pre-training）篇？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2195",
              "question": "领域模型Continue PreTrain ，如何 让模型在预训练过程中就？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2196",
              "question": "高质量、大规模、高覆盖度的预训练数据集？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2197",
              "question": "（三） —— 对预训练模型进行指令微调？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2198",
              "question": "（二） —— 继续预训练篇？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            }
          ]
        },
        {
          "subcategory": "LLM-评估与安全",
          "count": 27,
          "questions": [
            {
              "id": "llm_2199",
              "question": "1 介绍一下 大语言模型（LLM）的安全挑战 问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2200",
              "question": "LLMs什么时候最容易产生幻觉？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2201",
              "question": "OpenAI evals：OpenAI的自动化评估脚本，核心思路就是通过写prompt模版来自动化评估？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2202",
              "question": "Reward shaping：为了更好地引导模型的训练，reward shaping用于调整模型的奖励信号。通过将人类评估？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2203",
              "question": "query相关性较弱的内容，如果模型没有这部分知识，容易产生模型幻觉问题。一种解决思路是随？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2204",
              "question": "question（问题）：想要评估的RAG的问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2205",
              "question": "一份详尽的指南，指导如何一步步微调开源嵌入模型，并证明了微调可以在各项评估指标上持续改？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2206",
              "question": "上下文相关性评估：运用大语言模型 (LLM) 筛选出直接与问题相关的句子，以这些句子占上下？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2207",
              "question": "为什么 会 出现 大模型幻觉问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2208",
              "question": "为什么 会 出现 大模型幻觉？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2209",
              "question": "为什么LLM会产生幻觉？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2210",
              "question": "为什么需要解决LLM的幻觉问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2211",
              "question": "什么是 大模型幻觉问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2212",
              "question": "什么是大模型幻觉？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2213",
              "question": "以减少在危险或有害环境中的错误和增加安全，在工业流程的检查或维修期间等。由于其多样？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2214",
              "question": "减少幻觉现象：RAG显著降低了LLM产生幻觉的倾向，使生成的文本更加基于数据？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2215",
              "question": "在工程实践中，诸如如何在大规模知识库场景中提高检索效率和文档召回率，以及如何保障企业数据安全——例？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2216",
              "question": "在探索和优化 RAG（检索增强生成器）的过程中，如何有效评估其性能已经成为关键问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2217",
              "question": "基于以上步骤，我们已经为评估 RAG 做好了准备，接下来我们讲解如何进行 RAG 评估？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2218",
              "question": "如何 缓解 大模型幻觉问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2219",
              "question": "如何 缓解 大模型幻觉？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2220",
              "question": "如何 解决 大语言模型（LLM）的安全挑战 问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2221",
              "question": "如何 评估 大模型幻觉问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2222",
              "question": "如何缓解LLM幻觉？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "llm_2223",
              "question": "要评估一个大型语言模型的水平，可以从以下几个维度提出具有代表性的问题？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2224",
              "question": "评估：很难用传统指标评估生成模型。这就是为什么 LangChain 提供提示和链来帮助开发者自己使用 LLM？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            },
            {
              "id": "llm_2225",
              "question": "面对如何防止恶意输入操控、处理潜在的不安全输出和避免敏感信息泄露等问题，每位 AI 架构师？",
              "variants": [],
              "sources": [
                "llm_tech_questions_FINAL.json"
              ]
            }
          ]
        }
      ]
    },
    {
      "category": "Machine Learning",
      "count": 501,
      "subcategories": [
        {
          "subcategory": "General AI-General",
          "count": 39,
          "questions": [
            {
              "id": "ml_0001",
              "question": "1 为什么要用 层次化Softmax回归(Hierarchical Softmax)？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0002",
              "question": "14 RF与GBDT之间的区别与联系？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0003",
              "question": "16 简述KNN最近邻分类算法的过程？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0004",
              "question": "1）既然图1和图2 最终效果相同，为何还需要GBDT呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0005",
              "question": "2 层次化Softmax回归(Hierarchical Softmax) 的思想是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0006",
              "question": "21 Gradient boosting算法(GBM)和随机森林都是基于树的算法,它们有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0007",
              "question": "22 如何理解模型的过拟合与⽋拟合，以及如何解决？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0008",
              "question": "26 机器学习中的正则化到底是什么意思？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0009",
              "question": "27 请简单阐述下决策树、回归、SVM、神经⽹络等算法各⾃的优缺点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0010",
              "question": "29 如何通俗理解贝叶斯⽅法和贝叶斯⽹络？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0011",
              "question": "3 层次化Softmax回归(Hierarchical Softmax) 的步骤？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0012",
              "question": "4 机器学习中使⽤正则化来防⽌过拟合是什么原理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0013",
              "question": "6）是否过拟合？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0014",
              "question": "AUC刻画的什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0015",
              "question": "AUC和ROC是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0016",
              "question": "GBDT咋办呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0017",
              "question": "L1、L2正则化的效果、区别、原理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0018",
              "question": "L1，L2正则化的区别，岭回归是L1正则化还是L2正则化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0019",
              "question": "SVM的原理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0020",
              "question": "boosting集成学习由多个相关联的决策树联合决策，什么叫相关联？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0021",
              "question": "⼀篇才决定⾃⼰的研究⽅向为SVM的。--http://weibo.com/1580904460/zraWk0u6u？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0022",
              "question": "上⽂中，我们已经了解到了SVM处理线性可分的情况，那对于⾮线性的数据SVM咋处理呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0023",
              "question": "为什么AUC和logloss⽐accuracy更常⽤呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0024",
              "question": "什么是AUC？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0025",
              "question": "介绍一下XGBoost，与GBDT相比有什么不同？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0026",
              "question": "介绍决策树、信息熵？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0027",
              "question": "但⼀直没有⼀篇好的⽂章理清到底什么是正则化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0028",
              "question": "可以直接优化AUC来训练分类器吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0029",
              "question": "回归树又是什么呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0030",
              "question": "如何构造每个逻辑回归单元的输入？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0031",
              "question": "如何求解凸⽬标函数，SVM,boosting等算法只是这些通⽤⽅法的⼀个具体组建⽽已。”？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0032",
              "question": "如何解决过拟合和欠拟合。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0033",
              "question": "过拟合如何去解决？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0034",
              "question": "过拟合应该怎样处理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0035",
              "question": "过拟合怎么处理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0036",
              "question": "那为什么L2正则化可以获得值很⼩的参数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0037",
              "question": "那添加L1和L2正则化有什么⽤？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0038",
              "question": "重要（尽管GBDT调整后也可⽤于分类但不代表GBDT的树是分类树）。那么回归树是如何⼯作的呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0039",
              "question": "随机森林“随机”二字体现在什么地方？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            }
          ]
        },
        {
          "subcategory": "ML-SVM与核方法",
          "count": 12,
          "questions": [
            {
              "id": "ml_0040",
              "question": "5 SVM 核函数之间的区别？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0041",
              "question": "SMO 算法在求解SVM 的超平面方程中，扮演了一个什么样的角色？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0042",
              "question": "SVM 为什么使用拉格朗日乘子法？软间隔怎么做？",
              "variants": [],
              "sources": [
                "recovered_tech_from_removed_v2.json"
              ]
            },
            {
              "id": "ml_0043",
              "question": "SVM 怎么处理过拟合？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0044",
              "question": "SVM 有哪些可以调节的参数？",
              "variants": [],
              "sources": [
                "recovered_tech_from_removed_v2.json"
              ]
            },
            {
              "id": "ml_0045",
              "question": "SVM的原理？怎么找到最优的线性分类器？支持向量是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0046",
              "question": "SVM的推导？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0047",
              "question": "如何使用SVM 处理多分类问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0048",
              "question": "常用的SVM 核函数？",
              "variants": [],
              "sources": [
                "recovered_tech_from_removed_v2.json"
              ]
            },
            {
              "id": "ml_0049",
              "question": "注：以上各SVM的数学推导应该熟悉：硬间隔最大化（几何间隔）---学习的对偶问题---软间隔最大化（引入松弛变量）--- 非线性支持向量机（核技巧）？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0050",
              "question": "请你简单介绍一下SVM？",
              "variants": [],
              "sources": [
                "recovered_tech_from_removed_v2.json"
              ]
            },
            {
              "id": "ml_0051",
              "question": "请你说说支持向量机（SVM）？SVM 与LR 之间有什么样的区别和联系？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            }
          ]
        },
        {
          "subcategory": "ML-回归与分类",
          "count": 13,
          "questions": [
            {
              "id": "ml_0052",
              "question": "DFL能否用于实例分割的Mask回归？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0053",
              "question": "为什么逻辑回归用sigmoid激活函数？多分类逻辑回归是否也是sigmoid？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0054",
              "question": "什么是回归？业务应用场景？常见回归算法？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0055",
              "question": "什么是回归？哪些模型可用于解决回归问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0056",
              "question": "什么是线性回归？什么时候使用它？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0057",
              "question": "回归模型和分类模型的评价指标都有什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0058",
              "question": "回归，分类，聚类方法的区别和联系并举例，简要介绍算法思路？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0059",
              "question": "时间序列预测需要注意的点？和回归有何区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0060",
              "question": "简单介绍一下逻辑回归？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0061",
              "question": "线性回归的推导？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0062",
              "question": "说一说逻辑回归和SVM 的区别和联系？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0063",
              "question": "逻辑回归与线性回归的联系、异同？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0064",
              "question": "逻辑回归为什么要对特征进行离散化？为什么要对特征进行标准化？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            }
          ]
        },
        {
          "subcategory": "ML-基础",
          "count": 38,
          "questions": [
            {
              "id": "ml_0065",
              "question": "1 什么是特征选择？为什么需要它？特征选择的目标？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0066",
              "question": "11 LR如何解决多分类问题？（OvR vs OvO）？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0067",
              "question": "12在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0068",
              "question": "2 有哪些特征选择技术？‍？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0069",
              "question": "3梯度提升的如何调参？‍？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0070",
              "question": "4 LR参数求解的优化方法？(机器学习中常用的最优化方法)？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0071",
              "question": "4为什么要处理类别特征？怎么处理？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0072",
              "question": "4什么是监督学习？什么是非监督学习？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0073",
              "question": "6 为了解决KNN算法计算量过大的问题，可以使用分组的方式进行计算，简述一下该方式的原理？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0074",
              "question": "8 在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用曼哈顿距离？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0075",
              "question": "8什么是正规方程？‍？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0076",
              "question": "8归一化和标准化的区别？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0077",
              "question": "F（来店消费的频率）和M（购买金额）。如果这是一家奢侈品商店，你会发现M的量级（可能几万元）远大于F（可能平均10次以下），如果不归一化就算K-means，相当于F这个特征完全无效。如果我希望能把常客与其他顾客区别开来，不归一化就做不到？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0078",
              "question": "K-means是否会一直陷入选择质心的循环停不下来（为什么迭代次数后会收敛）？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0079",
              "question": "什么是正态分布？为什么要重视它？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0080",
              "question": "什么是深度学习？深度学习的训练过程是什么？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0081",
              "question": "什么是聚类？业务应用场景？常见算法？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0082",
              "question": "原理: 用线性函数拟合数据，用 MSE 计算损失，然后用梯度下降法(GD)找到一组使 MSE 最小的权重？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0083",
              "question": "原理：One-hot编码可以在预处理阶段或在训练期间完成; 处理类别型特征棒？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0084",
              "question": "原理：对大量未知标注的数据集，按数据的内在相似性将数据集划分为多个类别，使类别内的数据相似度较大而类别间的数据相似度较小？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0085",
              "question": "去掉高度相关的特征会让模型的可解释性更好？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0086",
              "question": "名词解释： 点击率CTR 转化率CVR？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0087",
              "question": "如何建立价格预测模型？价格是否正态分布？需要对价格进行预处理吗？‍？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0088",
              "question": "如何检查变量是否遵循正态分布？‍？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0089",
              "question": "如何选择m？一般m取2的幂次方能充分利用矩阵运算操作？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0090",
              "question": "对比最开始完全不调参的拟合效果，可见精确度稍有下降，主要原理是我们使用了0.8的子采样，20%的数据没有参与拟合？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0091",
              "question": "对统计这一块了解吗？p值是什么？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0092",
              "question": "广义模型推导所得 2.满足统计的最大熵模型 3.性质优秀，方便使用？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0093",
              "question": "无监督学习里典型的例子就是聚类了。聚类的目的在于把相似的东西聚在一起，一个聚类算法通常只需要知道如何计算相似度就可以开始工作了？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0094",
              "question": "本质上就是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本，该方法比较适用于样本容量比较大时的情况？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0095",
              "question": "朴素贝叶斯，怎么平滑？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0096",
              "question": "正规方程组是根据最小二乘法原理得到的关于参数估计值的线性方程组？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0097",
              "question": "没有本质区别。LR是最大熵对应类别为二类时的特殊情况，也就是当LR类别扩展到多类别时，就是最大熵模型？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0098",
              "question": "熟悉哪些聚类算法，解释下K-means 算法？什么时候或条件下停止迭代？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0099",
              "question": "简单介绍下聚类？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0100",
              "question": "维度灾难是什么？为什么要关心它？‍？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0101",
              "question": "请说一说朴素贝叶斯模型？它的假设条件是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0102",
              "question": "采用分箱技术时，需要确定的两个主要问题就是：如何分箱以及如何对每个箱子中的数据进行平滑处理？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            }
          ]
        },
        {
          "subcategory": "ML-树模型与集成",
          "count": 10,
          "questions": [
            {
              "id": "ml_0103",
              "question": "11 XGBoost在什么地方做的剪枝？ 如何进行剪枝？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0104",
              "question": "16 XGBoost如何寻找最优特征？是有放回还是无放回？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0105",
              "question": "GBDT使用基学习器是CART树，CART树是二叉树，每次使用yes or no进行特征选择，数值连续特征使用的最小均方误差，离散值使用的gini指数。在每次划分特征的时候会遍历所有可能的划分点找到最有的特征分裂点，这是用为什么gbdt会比rf慢的主要原因之一？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0106",
              "question": "GBDT如何防止过拟合？由于gbdt是前向加法模型，前面的树往往起到决定性的作用，如何改进这个问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0107",
              "question": "XGBoost 如何分布式？特征分布式和数据分布式？各有什么问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0108",
              "question": "不需要，后剪枝是为了避免过拟合，随机森林随机选择变量与树的数量，已经避免了过拟合，没必要去剪枝了。一般rf要控制的是树的规模，而不是树的置信度，剩下的每棵树需要做的就是尽可能的在自己所对应的数据(特征)集情况下尽可能的做到最好的预测结果。剪枝的作用其实被集成方法消解了，所以用处不大？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0109",
              "question": "为什么使用贪心和其发生搜索建立决策树，为什么不直接使用暴力搜索建立最优的决策树？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0110",
              "question": "基于决策树算法的分布式梯度提升框架；是对GBDT的高效实现，原理上它和GBDT及XGBoost类似，都采用损失函数的负梯度作为当前决策树的残差近似值，去拟合新的决策树？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0111",
              "question": "如何避免决策树过拟合？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0112",
              "question": "定义: 随机森林就是通过集成学习的思想将多棵树集成的一种算法，它的基本单元是决策树，而它的本质属于集成学习方法。它的工作原理是生成多个分类器/模型，各自独立地学习和作出预测。这些预测最后结合成单预测，因此优于任何一个单分类的做出预测？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            }
          ]
        },
        {
          "subcategory": "ML-泛化与正则化",
          "count": 23,
          "questions": [
            {
              "id": "ml_0113",
              "question": "3 L1和L2正则化有什么区别？‍？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0114",
              "question": "L1 正则化产生稀疏性的原因？对稀疏矩阵的理解？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0115",
              "question": "MoE如何解决Fine-Tuning过程中的过拟合问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0116",
              "question": "t-SNE 图好看但指标掉点，如何防止“可视化过拟合”？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0117",
              "question": "xgboost 中哪些参数可以控制过拟合？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0118",
              "question": "什么是正则化？如何理解正则化？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0119",
              "question": "什么是过拟合？产生过拟合原因？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0120",
              "question": "作用: 1）交叉验证是用来评估模型在新的数据集上的预测效果,也可以一定程度上减小模型的过拟合？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0121",
              "question": "八、MoE如何解决Fine-Tuning过程中的过拟合问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0122",
              "question": "另外，还可以使用PCA，并挑选可以解释在数据集中有最大偏差的成分？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0123",
              "question": "如何检验过拟合，数据量很小怎么办？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0124",
              "question": "如何简单理解过拟合？如何防止过拟合？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0125",
              "question": "如何防止过拟合的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0126",
              "question": "如果解决过拟合现象？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0127",
              "question": "当节点的数据量小于一个指定的数量时，不继续分裂。两个原因：一是数据量较少时，再做分裂容易强化噪声数据的作用；二是降低树生长的复杂性。提前结束分裂一定程度上有利于降低过拟合的影响？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0128",
              "question": "怎么解决过拟合的问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0129",
              "question": "有遇到过拟合的情况吗？过拟合是怎样解决的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0130",
              "question": "神经网络处理过拟合的方法？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0131",
              "question": "神经网络的正则化方法？过拟合的解决方法？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0132",
              "question": "而另一方面，方差量化了在同一个观察上进行的预测是如何彼此不同的？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0133",
              "question": "请说一下你是通过什么方法来判断模型欠拟合和过拟合的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0134",
              "question": "过拟合怎么解决？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "ml_0135",
              "question": "这也就是为什么在高维稀疏特征的时候，线性模型会比非线性模型好的原因了：带正则化的线性模型比较不容易对稀疏特征过拟合？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            }
          ]
        },
        {
          "subcategory": "ML-评估指标",
          "count": 2,
          "questions": [
            {
              "id": "ml_0136",
              "question": "1什么是ROC曲线？如何判断 ROC 曲线的好坏？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            },
            {
              "id": "ml_0137",
              "question": "1什么是准确率,精准率,召回率和F1分数？混淆矩阵？",
              "variants": [],
              "sources": [
                "ml_questions_theory_v3.json"
              ]
            }
          ]
        },
        {
          "subcategory": "Machine Learning-General",
          "count": 364,
          "questions": [
            {
              "id": "ml_0138",
              "question": "+ β2X2 + ··· + βnXn， 则 下 列 说 法 正 确 的 是 （ 多 选 ）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0139",
              "question": ". 分类属性选择完成，对训练样本分类，发现属性缺失怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0140",
              "question": ". 训练完成，给测试集样本分类，有缺失值怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0141",
              "question": "1 为什么 LR 要使用 sigmoid 函数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0142",
              "question": "1 为什么信息增益偏向取值较多的特征(缺点)？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0143",
              "question": "1 什么是准确率,精准率,召回率和 F1 分数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0144",
              "question": "1 什么是回归？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0145",
              "question": "1 什么是特征选择？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0146",
              "question": "1 什么是过拟合？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0147",
              "question": "1 你是怎么理解偏差和方差的平衡的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0148",
              "question": "1 决策树的数据 split 原理或者流程？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0149",
              "question": "1 基尼指数和信息熵都表示数据不确定性，为什么 CART 使用基尼指数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0150",
              "question": "1 如何使用信息增益比？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0151",
              "question": "1 既然信息增益可以计算，为什么 C4.5 还使用信息增益比？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0152",
              "question": "1 简述一下 KNN 算法的原理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0153",
              "question": "1.1 GBDT 是训练过程如何选择特征？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0154",
              "question": "1.1 K 值的如何选取？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0155",
              "question": "1.1 PCA 其优化目标是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0156",
              "question": "1.1 随机森林的随机性指的是？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0157",
              "question": "1.10 RF 为什么比 bagging 效率高？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0158",
              "question": "1.11 RF 为什么能够更鲁棒？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0159",
              "question": "1.12 RF 分类和回归问题如何预测 y 值？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0160",
              "question": "1.13 为什么 RF 的树比 GBDT 的要深一点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0161",
              "question": "1.2 GBDT 如何防止过拟合？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0162",
              "question": "1.2 K-means 算法中初始点的选择对最终结果的影响？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0163",
              "question": "1.2 PCA 白化是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0164",
              "question": "1.2 为什么随机抽样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0165",
              "question": "1.3 K-means 不适用哪些数据？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0166",
              "question": "1.3 为什么要有放回的抽样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0167",
              "question": "1.3 梯度提升的如何调参？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0168",
              "question": "1.4 GBDT 对标量特征要不要 one-hot 编码？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0169",
              "question": "1.4 K-means 中常用的距离度量？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0170",
              "question": "1.4 为什么不用全样本训练？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0171",
              "question": "1.5 为什么 GBDT 用负梯度当做残差？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0172",
              "question": "1.5 为什么在计算 K-means 之前要将数据点在各维度上归一化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0173",
              "question": "1.5 为什么要随机特征？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0174",
              "question": "1.6 聚类和分类区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0175",
              "question": "1.6 需要剪枝吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0176",
              "question": "1.7 随机森林如何处理缺失值？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0177",
              "question": "1.8 随机森林如何评估特征重要性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0178",
              "question": "1.9 RF 与决策树的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0179",
              "question": "10 MBGD 需要注意什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0180",
              "question": "10 为什么要将求解 SVM 的原始问题转换为对偶问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0181",
              "question": "10 决策树算法的停止条件？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0182",
              "question": "10 树形结构的不需要归一化的原因？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0183",
              "question": "10 用什么来评估 LR 模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0184",
              "question": "10 请简要说说一个完整机器学习项目的流程？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0185",
              "question": "100 遍，会造成怎样的影响？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0186",
              "question": "103用过哪些移动端深度学习框架？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0187",
              "question": "107RNN容易梯度消失，怎么解决？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0188",
              "question": "11 LR 如何解决多分类问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0189",
              "question": "11 SVM 怎么输出预测概率？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0190",
              "question": "11 为什么决策树之前用 PCA 会好一点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0191",
              "question": "12 如何处理数据偏斜？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0192",
              "question": "121模型压缩效果评价指标有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0193",
              "question": "123压缩和加速方法如何选择？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0194",
              "question": "13 为什么在训练的过程当中将高度相关的特征去掉？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0195",
              "question": "133对一千万个整数排序,整数范围在[-1000,1000]间,用什么排序最快？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0196",
              "question": "141什么是python的生成器？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0197",
              "question": "145如何判断两个dict是否一样,list头上删除元素,字符串拼接？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0198",
              "question": "146pytorch中cuda()作用,两个Tensor,一个加了cuda(),一个没加,相加后很怎样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0199",
              "question": "151在程序里面智能指针的名字是啥？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0200",
              "question": "155函数后面接const是什么意思？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0201",
              "question": "157c＋＋的一些库吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0202",
              "question": "160ip报文经过一个路由器改变哪些字段？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0203",
              "question": "1分类算法常用哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0204",
              "question": "2 L0、L1、L2 正则化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0205",
              "question": "2 为什么常常要做特征组合（特征交叉）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0206",
              "question": "2 为什么朴素贝叶斯如此“朴素”？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0207",
              "question": "2 回归算法一般用哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0208",
              "question": "2 基尼系数(Gini)存在的问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0209",
              "question": "2 如何理解 kNN 中的 k 的取值？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0210",
              "question": "2 如何避免过拟合问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0211",
              "question": "2 有哪些特征选择技术？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0212",
              "question": "2 构造决策树的步骤？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0213",
              "question": "2 模型常用的评估指标有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0214",
              "question": "2 线性回归的损失函数为什么是均方差？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0215",
              "question": "2.1 为什么 Adaboost 方式能够提高整体模型的学习精度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0216",
              "question": "2.1 为什么要用 SVD 进行降维？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0217",
              "question": "2.2 使用 m 个基学习器和加权平均使用 m 个学习器之间有什么不同？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0218",
              "question": "2.3 adaboost 的迭代次数(基学习器的个数)如何控制？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0219",
              "question": "2.4 adaboost 算法中基学习器是否很重要，应该怎么选择基学习器？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0220",
              "question": "22SVM中什么时候用线性核什么时候用高斯核？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0221",
              "question": "23什么是支持向量机,SVM与LR的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0222",
              "question": "25机器学习中的距离计算方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0223",
              "question": "26朴素贝叶斯（naive Bayes）法的要求是？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0224",
              "question": "27训练集中类别不均衡，哪个参数最不准确？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0225",
              "question": "3 L1 和 L2 正则化有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0226",
              "question": "3 为什么 LR 比线性回归要好？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0227",
              "question": "3 什么是机器学习的欠拟合？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0228",
              "question": "3 什么是线性回归？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0229",
              "question": "3 决策树算法中如何避免过拟合和欠拟合？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0230",
              "question": "3 回归方法中使用的评价指标是哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0231",
              "question": "3 在 kNN 的样本搜索中，如何进行高效的匹配查找？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0232",
              "question": "3 多标签分类怎么解决？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0233",
              "question": "3 朴素贝叶斯的优缺点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0234",
              "question": "3.1 XGBoost 使用泰勒二阶展开的原因？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0235",
              "question": "3.10 比较 LR 和 GBDT，说说什么情景下 GBDT 不如 LR？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0236",
              "question": "3.11 XGBoost 在什么地方做的剪枝？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0237",
              "question": "3.12 XGBoost 如何选择最佳分裂点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0238",
              "question": "3.13 XGBoost 的 Scalable 性如何体现？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0239",
              "question": "3.14 XGBooost 参数调优的一般步骤？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0240",
              "question": "3.15 XGBoost 模型如果过拟合了怎么解决？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0241",
              "question": "3.16 XGBoost 如何寻找最优特征？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0242",
              "question": "3.17 XGBoost 如何分布式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0243",
              "question": "3.18 为什么 XGBoost 的近似算法比 lightgbm 慢很多呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0244",
              "question": "3.2 XGBoost 可以并行训练的原因？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0245",
              "question": "3.3 XGBoost 为什么快？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0246",
              "question": "3.4 XGBoost 防止过拟合的方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0247",
              "question": "3.5 XGBoost 如何处理缺失值？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0248",
              "question": "3.6 为什么 XGBoost 相比某些模型对缺失值不敏感？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0249",
              "question": "3.7 XGBoost 如何处理不平衡数据？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0250",
              "question": "3.8 XGBoost 中叶子结点的权重如何计算出来？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0251",
              "question": "3.9 XGBoost 中的一棵树的停止生长条件？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0252",
              "question": "35生成模型和判别模型基本形式，有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0253",
              "question": "38分类算法列一下有多少种？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0254",
              "question": "4 KNN 算法有哪些优点和缺点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0255",
              "question": "4 L1 在 0 处不可导是怎么处理的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0256",
              "question": "4 LR 参数求解的优化方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0257",
              "question": "4 为什么 SVM 对缺失数据敏感？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0258",
              "question": "4 为什么引入条件独立性假设？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0259",
              "question": "4 为什么要处理类别特征？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0260",
              "question": "4 什么是梯度下降？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0261",
              "question": "4 什么是监督学习？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0262",
              "question": "4 决策树怎么剪枝？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0263",
              "question": "4 分类中使用的损失函数是哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0264",
              "question": "4 如何避免欠拟合问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0265",
              "question": "4 模型受到低偏差和高方差问题时，应该使用哪种算法来解决问题呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0266",
              "question": "41SVM和全部数据有关还是和局部数据有关？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0267",
              "question": "45Loss Function有哪些，怎么用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0268",
              "question": "5 L1 正则化产生稀疏性的原因？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0269",
              "question": "5 不平衡的样本可以给 KNN 的预测结果造成哪些问题，有没有什么好的解决方式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0270",
              "question": "5 什么是最小二乘法（最小平方法）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0271",
              "question": "5 什么是组合特征？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0272",
              "question": "5 决策树的优缺点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0273",
              "question": "5 在估计条件概率 P(X|Y)时出现概率为 0 的情况怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0274",
              "question": "5 怎么理解偏差方差的平衡的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0275",
              "question": "5 降维的作用是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0276",
              "question": "5.1 什么是 ROC 曲线？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0277",
              "question": "5.3 如何解释 AU ROC 分数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0278",
              "question": "59stacking和blending的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0279",
              "question": "5对于小数据集一般怎么处理呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0280",
              "question": "6 LR 如何解决低维不可分问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0281",
              "question": "6 SVM 如何处理多分类问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0282",
              "question": "6 为什么属性独立性假设在实际情况中很难成立，但 NB 仍能取得较好效果？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0283",
              "question": "6 为何要常对数据做归一化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0284",
              "question": "6 交叉验证主要有哪几种方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0285",
              "question": "6 决策树和条件概率分布的关系？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0286",
              "question": "6 协方差和相关性有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0287",
              "question": "6 怎么有效地找到组合特征？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0288",
              "question": "6 矩阵的特征值和特征向量的物理意义是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0289",
              "question": "67HMM隐马尔可夫模型的参数估计方法是？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0290",
              "question": "68Bootstrap方法是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0291",
              "question": "69如何防止过拟合？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0292",
              "question": "6常用什么激活函数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0293",
              "question": "7 LR 与最大熵模型 MaxEnt 的关系？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0294",
              "question": "7 sigmoid函数有使用过吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0295",
              "question": "7 什么是 K 折交叉验证？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0296",
              "question": "7 如何优化 Kmeans？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0297",
              "question": "7 如何处理高维组合特征？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0298",
              "question": "7 带核的 SVM 为什么能分类非线性问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0299",
              "question": "7 把分类变量当成连续型变量会更得到一个更好的预测模型吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0300",
              "question": "7 有哪些评估回归模型的指标？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0301",
              "question": "73正负样本不平衡的解决办法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0302",
              "question": "8 RBF 核一定是线性可分的吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0303",
              "question": "8 为什么 LR 用交叉熵损失而不是平方损失（MSE）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0304",
              "question": "8 什么是正规方程？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0305",
              "question": "8 如何在 K 折交叉验证中选择 K？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0306",
              "question": "8 如何解决数据不平衡问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0307",
              "question": "8 如果特征很多，决策树中最后没有用到的特征一定是无用吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0308",
              "question": "8 机器学习中分类器指的是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0309",
              "question": "88循环神经网络，为什么好？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0310",
              "question": "9 LR 能否解决非线性分类问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0311",
              "question": "9 决策树怎么做回归？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0312",
              "question": "9 常用核函数及核函数的条件？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0313",
              "question": "9 数据中有噪声如何处理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0314",
              "question": "9 梯度下降法找到的一定是下降最快的方向吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0315",
              "question": "9 需要归一化的算法有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0316",
              "question": "90训练过程中,若一个模型不收敛,那么是否说明这个模型无效？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0317",
              "question": "93神经网络中权重共享的是？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0318",
              "question": "94神经网络激活函数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0319",
              "question": "97在深度学习中，通常会finetuning（微调）已有的成熟模型，再基于新数据，修改最后几层神经网络权值，为什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0320",
              "question": "98微调时候网络参数是否更新？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0321",
              "question": "AUC 计算 auc 为什么稳定？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0322",
              "question": "Adaboost、SVM、LR、Knn、KMeans 之类则需要归⼀化呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0323",
              "question": "EM算法是否一定收敛？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0324",
              "question": "GBDT 和 xgboost 之间差别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0325",
              "question": "GBDT 和随机森林的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0326",
              "question": "LR 的推导，特性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0327",
              "question": "LR/SVM/softmax/Adaboost 损失函数之间的差别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0328",
              "question": "LightGBM 对类别型是怎么处理的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0329",
              "question": "MLE） ， 说 法 正 确 的 是 （ 多 选 ）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0330",
              "question": "RF 与 boosting 之间差别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0331",
              "question": "SGD 的推导？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0332",
              "question": "SVM 为什么采用间隔最大化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0333",
              "question": "SVM 对缺失数据敏感吗，为什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0334",
              "question": "SVM 的原理是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0335",
              "question": "SVM 的损失函数是什么形式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0336",
              "question": "SVM 的推导，特性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0337",
              "question": "SVM 的高斯核为什么会把原始维度映射到无穷多维？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0338",
              "question": "SVM、LR、决策树的对比？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0339",
              "question": "TF-IDF 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0340",
              "question": "[链接](7. https://blog.csdn.net/weixin_42057852/article/details/84644348？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0341",
              "question": "level-wise 的生长和 leaf-wise 的生长有什么不同，优缺点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0342",
              "question": "lgb 二分类的损失函数是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0343",
              "question": "lgb,xgb 如何防止过拟合，有哪些参数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0344",
              "question": "lr 使用注意事项，相比别的分类模型为什么使用它？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0345",
              "question": "lr 模型是线性模型还是非线性， 为什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0346",
              "question": "p 值是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0347",
              "question": "xgboost 和 LightGBM 有哪些控制过拟合的手段，通常需要调整的参数有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0348",
              "question": "xgboost 对于缺失值，训练和预测的时候都是怎么处理的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0349",
              "question": "xgboost 有哪些参数会影响模型的复杂度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0350",
              "question": "xgboost 的多分类如何做？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0351",
              "question": "xgboost 的并⾏化体现在哪？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0352",
              "question": "xgboost 的树生长时的精确分裂与近似分裂分别是怎么做的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0353",
              "question": "一 般 来 说 ， 下 列 哪 种 方 法 常 用 来 预 测 连 续 独 立 变 量？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0354",
              "question": "一下原理么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0355",
              "question": "七、python常用的加锁方式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0356",
              "question": "下 列 关 于 异 方 差 （ Heteroskedasticity） 说 法 正 确 的 是？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0357",
              "question": "下 列 关 于 线 性 回 归 分 析 中 的 残 差 （ Residuals） 说 法 正 确 的 是？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0358",
              "question": "下 列 哪 一 种 偏 移 ， 是 我 们 在 最 小 二 乘 直 线 拟 合 的 情 况 下 使 用 的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0359",
              "question": "下 列 哪 一 项 能 反 映 出 X 和 Y 之 间 的 强 相 关 性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0360",
              "question": "下 列 哪 些 假 设 是 我 们 推 导 线 性 回 归 参 数 时 遵 循 的 （ 多 选 ）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0361",
              "question": "下 列 哪 些 指 标 可 以 用 来 评 估 线 性 回 归 模 型 （ 多 选 ）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0362",
              "question": "与relu激活函数有什么不同？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0363",
              "question": "两 个 变 量 相 关 ， 它 们 的 相 关 系 数 r 可 能 为 0。 这 句 话 是 否 正 确？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0364",
              "question": "个标准偏差的范围内。百分之多少的数据不会受到影响？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0365",
              "question": "为什么 SVM 要引入核函数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0366",
              "question": "为什么 xgboost 的近似算法比 lightgbm 还是慢很多呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0367",
              "question": "为什么会产生过拟合，有哪些方法可以预防或克服过拟合？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0368",
              "question": "为什么是交叉熵而不是mae？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0369",
              "question": "为什么此时树模型就过拟合的更严重呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0370",
              "question": "为什么用同步锁？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0371",
              "question": "为什么要关心它？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0372",
              "question": "为什么要将求解 SVM 的原始问题转换为其对偶问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0373",
              "question": "为什么要重视它？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0374",
              "question": "为什么需要它？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0375",
              "question": "为零），则下面哪个说法是正确的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0376",
              "question": "么不用曼哈顿距离？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0377",
              "question": "五、什么是乐观锁？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0378",
              "question": "交叉熵代价函数是如何产生的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0379",
              "question": "交叉验证的作用是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0380",
              "question": "产生过拟合原因？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0381",
              "question": "什么 xgboost 的近似算法比 lightgbm 还是慢很多呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0382",
              "question": "什么时候使用它？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0383",
              "question": "什么是偏差与方差？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0384",
              "question": "什么是全局解释器锁？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0385",
              "question": "什么是凸优化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0386",
              "question": "什么是同步锁？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0387",
              "question": "什么是正态分布？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0388",
              "question": "什么是死锁？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0389",
              "question": "什么是深度学习？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0390",
              "question": "什么是非监督学习？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0391",
              "question": "什么样的函数可以作为 SVM 的核函数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0392",
              "question": "介绍卷积神经网络，和 DBN 有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0393",
              "question": "代表什么。特征向量有什么性质，是单位向量吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0394",
              "question": "价格是否正态分布？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0395",
              "question": "值 与 真 实 值 的 残 差 。 计 算 SSE 为 多 少？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0396",
              "question": "假 设 变 量 Var1 和 Var2 是 正 相 关 的 ， 那 么 下 面 那 张 图 是 正 确 的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0397",
              "question": "偏差的范围内。百分之多少的数据不会受到影响？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0398",
              "question": "六、什么是悲观锁？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0399",
              "question": "关 于 特 征 选 择 ， 下 列 对 Ridge 回 归 和 Lasso 回 归 说 法 正 确 的 是？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0400",
              "question": "关于 A 和 B 各自的残差之和，下列说法正确的是？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0401",
              "question": "关于这两句话，下列说法正确的是？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0402",
              "question": "其中 a 是属性，aj 是该属性的所有取值。如果选择在某一节点上用哪个特征呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0403",
              "question": "写一下贝叶斯公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0404",
              "question": "决定性的作用，如何改进这个问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0405",
              "question": "决策树/随机森林的特征重要度是怎么获得的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0406",
              "question": "决策树怎么控制过拟合？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0407",
              "question": "决策树的特性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0408",
              "question": "决策树的缺失值和数值型特征分别是怎么处理的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0409",
              "question": "变。然后重新训练测试。则下列说法正确的是？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0410",
              "question": "可以用梯度下降优化吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0411",
              "question": "各有什么问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0412",
              "question": "合 （ over-fitting） 中 影 响 最 大？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0413",
              "question": "合度。此时，如果增加一个特征，模型不变，则下面说法正确的是？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0414",
              "question": "向量代表什么。特征向量有什么性质，是单位向量吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0415",
              "question": "哪些模型可用于解决回归问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0416",
              "question": "哪种图形比较适合？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0417",
              "question": "四、什么是递归锁？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0418",
              "question": "处理死锁的基本方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0419",
              "question": "多分类怎么处理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0420",
              "question": "如 果 XTX 矩 阵 不 可 逆 ， 是 奇 异 矩 阵 怎 么 办 呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0421",
              "question": "如 果 两 个 变 量 相 关 ， 那 么 它 们 一 定 是 线 性 关 系 吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0422",
              "question": "如何判断 ROC 曲线的好坏？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0423",
              "question": "如何判断函数凸或非凸？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0424",
              "question": "如何对贝叶斯网络进行采样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0425",
              "question": "如何建立价格预测模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0426",
              "question": "如何检查变量是否遵循正态分布？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0427",
              "question": "如何理解正则化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0428",
              "question": "如何观察过拟合？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0429",
              "question": "如何解决类别不平衡问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0430",
              "question": "如何进行剪枝？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0431",
              "question": "如何进行特征选择？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0432",
              "question": "如何选择 m？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0433",
              "question": "如果出现计数为 0 导致概率为 0，这种情况在朴素贝叶斯计算里是怎么解决的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0434",
              "question": "对稀疏矩阵的理解？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0435",
              "question": "对统计这一块了解吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0436",
              "question": "小数据集中如何防止过拟合？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0437",
              "question": "常用的防止过拟合的技术手段有哪些， l1-norm 和 l2-norm 的区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0438",
              "question": "式，而不使用 β 的平方约束呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0439",
              "question": "影响是怎样的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0440",
              "question": "怎么使用同步锁？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0441",
              "question": "文本中的余弦距离是什么，有哪些作用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0442",
              "question": "无监督和有监督算法的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0443",
              "question": "是有放回还是无放回？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0444",
              "question": "曼哈顿距离？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0445",
              "question": "有几种不同的决策树，区别在哪？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0446",
              "question": "有哪些处理异常值的⽅法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0447",
              "question": "有哪些并行化的工具？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0448",
              "question": "有哪些异常值检测的⽅法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0449",
              "question": "有哪些模型评估⽅法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0450",
              "question": "有哪些特征选择的方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0451",
              "question": "有哪些评估准则？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0452",
              "question": "有限。你会怎么做？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0453",
              "question": "朴素贝叶斯是一个什么算法，能解释一下吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0454",
              "question": "构 建 一 个 最 简 单 的 线 性 回 归 模 型 需 要 几 个 系 数 （ 只 有 一 个 特 征 ）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0455",
              "question": "树模型对于缺失值如何处理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0456",
              "question": "梯度下降中局部最优解和全局最优解的关系？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0457",
              "question": "死锁产生的必要条件？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0458",
              "question": "比如该节点是根据 a 属性划分，但是待分类样本 a 属性缺失，怎么办呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0459",
              "question": "比较 偏向取值较多的特征。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0460",
              "question": "汇总不同 worker 的不同 feature 的直方图(原理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0461",
              "question": "法怎么做的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0462",
              "question": "深度学习与机器学习有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0463",
              "question": "深度学习的训练过程是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0464",
              "question": "特征分布式和数据分布式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0465",
              "question": "特征工程有什么作用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0466",
              "question": "特征选择的目标？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0467",
              "question": "神经网络的原理，如何进行训练？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0468",
              "question": "立最优的决策树？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0469",
              "question": "系数。下列关于正规方程说法正确的是？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0470",
              "question": "维度灾难是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0471",
              "question": "聚类算法中的距离度量有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0472",
              "question": "能写一下逻辑回归的损失函数吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0473",
              "question": "能很好地拟合数据）。那么，下列说法正确的是（多选）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0474",
              "question": "能推导它的原理吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0475",
              "question": "能解释一下 LightGBM 里基于 histogram 的决策树算法吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0476",
              "question": "解释 L1 和 L2 正则化的作用。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0477",
              "question": "解释 balabala）。当然现在也有⼀些对 ReLU 的改进，⽐如 PReLU，random ReLU？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0478",
              "question": "解释一下决策树的建模过程？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0479",
              "question": "解释对偶的概念。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0480",
              "question": "解释贝叶斯公式和朴素贝叶斯分类。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0481",
              "question": "证得到的均方误差是多少？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0482",
              "question": "评价指标的参考价值？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0483",
              "question": "话概述，就是“投影后类内方差最小，类间方差最大”，什幺意思呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0484",
              "question": "说明准确程度，只能大概定量。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0485",
              "question": "请简述有哪些你知道的特征工程和他们的操作？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0486",
              "question": "这些模型需要归一化的主要原因？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0487",
              "question": "逻辑回归中样本不均衡我们怎么处理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0488",
              "question": "逻辑回归和最大似然有什么关系？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0489",
              "question": "逻辑回归用梯度下降优化，学习率对结果有什么影响？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0490",
              "question": "那 么 它 可 以 被 看 成 是 异 常 值 （ Outlier） 吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0491",
              "question": "那么，下列说法正确的是？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0492",
              "question": "采用 EM 算法求解的模型有哪些，为什么不用牛顿法或梯度下降法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0493",
              "question": "需要对价格进行预处理吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0494",
              "question": "预测函数与代价函数的关系，在矩阵分解中如何体现的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0495",
              "question": "（天眼查）为什么逻辑回归做对数转换时选用以 e 为底？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0496",
              "question": "（欢聚时代 YY）LR 和 DNN 联系和区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0497",
              "question": "（每日优鲜）lr 为什么不采用 mse 而是采用交叉熵损失？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0498",
              "question": "（滴滴）CART 树是如何做分类的，是如何做回归的，是如何处理多分类的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0499",
              "question": "（百度） Xgboost 的原理能讲一下么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0500",
              "question": "（网易游戏）PCA 算法原理，跟 svd 的区别。特征值越大代表什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "ml_0501",
              "question": "（蓝色曲线）。那么，我们可以得出哪些结论（多选）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            }
          ]
        }
      ]
    },
    {
      "category": "Multimodal",
      "count": 5,
      "subcategories": [
        {
          "subcategory": "Multimodal-General",
          "count": 5,
          "questions": [
            {
              "id": "q_0001",
              "question": "一、最近关注的论文，多模态视觉大模型(CLIP,DALLE)？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "q_0002",
              "question": "三、多模态融合后，怎样知道最终结果受哪种模态影响更大？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "q_0003",
              "question": "二、blip2的架构，优势和之前多模态模型的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "q_0004",
              "question": "五、介绍一下stable diffusion的原理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "q_0005",
              "question": "四、多模态中常见的SOTA模型有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            }
          ]
        }
      ]
    },
    {
      "category": "NLP",
      "count": 129,
      "subcategories": [
        {
          "subcategory": "General AI-General",
          "count": 52,
          "questions": [
            {
              "id": "nlp_0001",
              "question": "1 GNN 图神经网络如何应用于文本分类领域？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0002",
              "question": "1 【BERT】Bert 输入输出表征长啥样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0003",
              "question": "1 【BERT】为什么 Bert 需要 fine-turning？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0004",
              "question": "1 如何解决长文本分类任务？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0005",
              "question": "1 文本分类任务使用的评估算法和指标有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0006",
              "question": "1 文本分类任务的数据预处理方法有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0007",
              "question": "1 （一个具体的）文本分类任务可以使用哪些特征？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0008",
              "question": "1【BERT】Bert 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0009",
              "question": "2 【BERT】 Bert 如何 fine-turning？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0010",
              "question": "2 【BERT】Bert 预训练任务？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0011",
              "question": "2 为什么说 「词汇增强」 方法对于中文 NER 任务有效呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0012",
              "question": "2 什么是ROUGE？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0013",
              "question": "2 你使用过哪些分词方法和工具？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0014",
              "question": "2 文本分类任务相较于其他领域的分类任务有何不同之处？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0015",
              "question": "2.1 【BERT】 Bert 为什么需要预训练任务 Masked LM？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0016",
              "question": "2.2 【BERT】 Bert 预训练任务 Masked LM 怎么做？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0017",
              "question": "2.3 【BERT】 Bert 预训练任务 Masked LM 存在问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0018",
              "question": "2.4 【BERT】 预训练和微调之间的不匹配的解决方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0019",
              "question": "2【BERT】Bert 三个关键点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0020",
              "question": "3 ALBERT 所所用的方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0021",
              "question": "3 【对比】GPT和BERT有什么不同？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0022",
              "question": "3 中文文本分词的方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0023",
              "question": "3 几种ROUGE指标之间的区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0024",
              "question": "3 文本分类任务和文本领域的其他任务相比有何不同之处？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0025",
              "question": "3.1 【BERT】Bert 为什么需要预训练任务 Next Sentence Prediction？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0026",
              "question": "3.2 【BERT】 Bert 预训练任务 Next Sentence Prediction 怎么做？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0027",
              "question": "4 BLEU和ROUGE有什么不同？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0028",
              "question": "4 Q：介绍下词向量空间中的平移不变现象？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0029",
              "question": "4 【BERT】 fine-turning 篇？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0030",
              "question": "4 什么是 LEX-BERT？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0031",
              "question": "4 基于字符串匹配的分词方法的原理 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0032",
              "question": "4 文本分类的过程？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0033",
              "question": "5 统计语言模型如何应用于分词？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0034",
              "question": "6 基于序列标注的分词方法 是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0035",
              "question": "BERT的base版本的原始模型，训练的时候，第一个epoch模型的判定结果很可能是错的，这个时候熵还可信吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0036",
              "question": "BERT的缺点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0037",
              "question": "BERT的输入有哪几种Embedding？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0038",
              "question": "N-gram最大概率分词？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0039",
              "question": "NLP中常见的分词方法有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0040",
              "question": "RoBERTa相比BERT有哪些改进？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0041",
              "question": "trick 10: NER 标注数据噪声问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0042",
              "question": "trick 12： NER 标注数据不均衡问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0043",
              "question": "trick 3：词向量选取：词向量 or 字向量？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0044",
              "question": "trick 9：NER实体span过长怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0045",
              "question": "⑦ 事件抽取与其他信息抽取任务（关系抽取、NER 等）有什么联系，难点在哪？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0046",
              "question": "不过也许有的读者会问，为什么不直接蒸馏为一个浅层BERT呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0047",
              "question": "参考：在文本分类任务中，有哪些论文中很少提及却对性能有重要影响的tricks？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0048",
              "question": "如何在基于字符的NER系统中引入词汇信息，是近年来NER的一个研究重点。本文将这种引入词汇的方？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0049",
              "question": "工业界如何解决NER问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0050",
              "question": "意力机制如何应用于文本分类领域？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0051",
              "question": "率比较高，那么分词结果会把两个词合并，那么合并与否对LDA的训练是否有影响呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0052",
              "question": "讲一下BERT的结构？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            }
          ]
        },
        {
          "subcategory": "NLP-General",
          "count": 69,
          "questions": [
            {
              "id": "nlp_0053",
              "question": "4 大模型 LLM 的架构介绍？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0054",
              "question": "Deep Norm 代码实现？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0055",
              "question": "France？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0056",
              "question": "LLMs 中的不同位置 有什么区别么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0057",
              "question": "Layer Norm 有什么特点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0058",
              "question": "Layer normalization？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0059",
              "question": "Norm 的计算公式写一下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0060",
              "question": "TF（词频）和 IDF（逆文档频率）的乘积的正确值是多少？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0061",
              "question": "Transformer 架构首先是由下列哪项引入的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0062",
              "question": "i. llama 输入句子长度理论上可以无限长吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0063",
              "question": "i. 什么是 LLMs 复读机问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0064",
              "question": "ii. 为什么会出现 LLMs 复读机问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0065",
              "question": "iii. 如何缓解 LLMs 复读机问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0066",
              "question": "ntry}？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0067",
              "question": "term matrix）。以下哪项可用于减少数据维度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0068",
              "question": "|Deep Norm 有什么优点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0069",
              "question": "|RMS Norm 的计算公式写一下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0070",
              "question": "• 1 Byte-Pair Encoding(BPE) 如何构建词典？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0071",
              "question": "• 1 WordPiece 与 BPE 异同点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0072",
              "question": "• 1 举例 介绍一下 不同 大模型 LLMs 的分词方式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0073",
              "question": "• 2 介绍一下 不同 大模型 LLMs 的分词方式 的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0074",
              "question": "• [ ] 各 LLMs 都使用哪种激活函数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0075",
              "question": "• [[#1 介绍一下 FFN 块 计算公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0076",
              "question": "• [[#2 介绍一下 GeLU 计算公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0077",
              "question": "• [[#3 介绍一下 Swish 计算公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0078",
              "question": "• [[#5 介绍一下 使用 GeLU 的 GLU 块 计算公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0079",
              "question": "• [[#6 介绍一下 使用 Swish 的 GLU 块 计算公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0080",
              "question": "• [[#Deep Norm 篇#Deep Norm 思路？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0081",
              "question": "• [[#Deep Norm 篇#写一下 Deep Norm 代码实现？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0082",
              "question": "• [[#Layer Norm 篇#Layer Norm 的计算公式写一下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0083",
              "question": "• [[#Layer normalization-方法篇#Deep Norm 有什么优点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0084",
              "question": "• [[#各 LLMs 都使用哪种激活函数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0085",
              "question": "• [x] 1 介绍一下 FFN 块 计算公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0086",
              "question": "• [x] 2 介绍一下 GeLU 计算公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0087",
              "question": "• [x] 3 介绍一下 Swish 计算公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0088",
              "question": "• 简单介绍一下 SentencePiece 思路？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0089",
              "question": "下列哪一项不是预处理技术？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0090",
              "question": "下列哪些技术能被用于计算两个词向量之间的距离？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0091",
              "question": "下列哪种嵌入方式支持双向上下文（Bidirectional Context）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0092",
              "question": "下列哪种词嵌入可以自定义训练特定主题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0093",
              "question": "下列哪种词嵌入支持上下文建模（Context Modeling）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0094",
              "question": "下列哪项是关键词归一化技术？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0095",
              "question": "下面哪个是 NLP 用例？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0096",
              "question": "介绍一下 使用 GeLU 作为激活函数的 GLU 块 计算公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0097",
              "question": "介绍一下 使用 Swish 作为激活函数的 GLU 块 计算公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0098",
              "question": "从句子中删除“and”、“is”、“a”、“an”、“the” 这样的词的过程被称为？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0099",
              "question": "从给定的句子、段落中识别人名、组织名的过程称为？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0100",
              "question": "以下哪种 NLP 模型的准确性最高？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0101",
              "question": "以下哪种架构可以更快地训练，且需要更少的训练数据？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0102",
              "question": "关。这是哪种架构？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0103",
              "question": "块 计算公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0104",
              "question": "如果有，能介绍一下区别么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0105",
              "question": "将词表示成向量被称为神经词嵌入（Neural Word Embeddings）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0106",
              "question": "层归一化 Layer Norm 在 大语言模型 LLMs 中的不同位置 有什么区别么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0107",
              "question": "排列语言模型（Permutation Language Models）是下列哪项的特点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0108",
              "question": "文本语料库的可能特征是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0109",
              "question": "有什么特点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0110",
              "question": "板 如何构建？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0111",
              "question": "相同的词可以通过___________来实现多个词嵌入？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0112",
              "question": "种 Layer normalization？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0113",
              "question": "置 有什么区别么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0114",
              "question": "词嵌入向量有助于确定 2 个 tokens 之间的距离？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0115",
              "question": "词嵌入捕获多维数据，并表示为向量？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0116",
              "question": "词转化为其基本形式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0117",
              "question": "请注意，Chat Message History 的具体用法和实现细节可？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0118",
              "question": "请注意，您可以根据需要添加、删除和修改嵌入向量。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0119",
              "question": "请注意，您可以根据需要添加、删除和修改组件。**Chain**？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0120",
              "question": "请注意，您可以根据需要添加、删除和修改聊天消息提？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "nlp_0121",
              "question": "转换为整数或浮点向量的操作？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            }
          ]
        },
        {
          "subcategory": "NLP-基础",
          "count": 6,
          "questions": [
            {
              "id": "nlp_0122",
              "question": "一、文本分类任务有哪些应用场景？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "nlp_0123",
              "question": "二、文本分类的具体流程？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "nlp_0124",
              "question": "六、文本分类任务使用的评估指标有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "nlp_0125",
              "question": "文本分类任务使用的评估指标有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "nlp_0126",
              "question": "文本分类任务有哪些应用场景？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "nlp_0127",
              "question": "文本分类的具体流程？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            }
          ]
        },
        {
          "subcategory": "NLP-经典任务",
          "count": 1,
          "questions": [
            {
              "id": "nlp_0128",
              "question": "在做NER 任务时，lstm 后面可以不用加CRF 吗？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            }
          ]
        },
        {
          "subcategory": "NLP-表示学习",
          "count": 1,
          "questions": [
            {
              "id": "nlp_0129",
              "question": "统计语言模型如何应用于分词？N-gram最大概率分词？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            }
          ]
        }
      ]
    },
    {
      "category": "Python",
      "count": 10,
      "subcategories": [
        {
          "subcategory": "General AI-General",
          "count": 3,
          "questions": [
            {
              "id": "py_0001",
              "question": "Python的深拷贝和浅拷贝的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "py_0002",
              "question": "Python迭代器是什么，构成是怎样的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "py_0003",
              "question": "赋值时浅拷贝还是深拷贝？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            }
          ]
        },
        {
          "subcategory": "Python-基础",
          "count": 1,
          "questions": [
            {
              "id": "py_0004",
              "question": "Python的深拷贝和浅拷贝的区别？赋值时浅拷贝还是深拷贝？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            }
          ]
        },
        {
          "subcategory": "Python-进阶",
          "count": 6,
          "questions": [
            {
              "id": "py_0005",
              "question": "了解python 的深浅拷贝吗？装饰器呢？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "py_0006",
              "question": "什么是Python 中的生成器和迭代器？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "py_0007",
              "question": "什么是迭代器(Iterator)？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "py_0008",
              "question": "装饰器怎么用？装饰器解释下，基本要求是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "py_0009",
              "question": "请解释python中生成器有什么作用？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "py_0010",
              "question": "谈下python的GIL？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            }
          ]
        }
      ]
    },
    {
      "category": "RAG",
      "count": 95,
      "subcategories": [
        {
          "subcategory": "General AI-General",
          "count": 4,
          "questions": [
            {
              "id": "rag_0001",
              "question": "24 如何离线评价召回阶段各种模型算法的好坏？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0002",
              "question": "embedding后多义词问题解决了吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0003",
              "question": "已。听说过word embedding吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0004",
              "question": "那么，我们可不可以采取类似于上面的subword的思路来产生更好的word embedding呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            }
          ]
        },
        {
          "subcategory": "RAG-Query 改写/扩写",
          "count": 3,
          "questions": [
            {
              "id": "rag_0005",
              "question": "Query 扩写的 Prompt 如何构建？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0006",
              "question": "为什么进行 Query 扩写？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0007",
              "question": "介绍下 HyDE？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            }
          ]
        },
        {
          "subcategory": "RAG-向量化与检索",
          "count": 13,
          "questions": [
            {
              "id": "rag_0008",
              "question": "embedding 物理意义？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0009",
              "question": "embedding 的作用是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0010",
              "question": "一、为什么需要使用大模型辅助召回？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0011",
              "question": "为什么要使用检索增强的语言模型（Retrieval-based LMs）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0012",
              "question": "什么是检索增强的语言模型（Retrieval-based LMs）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0013",
              "question": "向量化模型都有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0014",
              "question": "向量库有那些？各自优点与区别？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0015",
              "question": "基于LLM+向量库的文档对话 prompt 模板 如何构建？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0016",
              "question": "基于LLM+向量库的文档对话 思路是怎么样？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0017",
              "question": "基于LLM+向量库的文档对话 核心技术是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0018",
              "question": "如何构造微调 Bert 相似度向量模型数据？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0019",
              "question": "检索模块的评估指标有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0020",
              "question": "长文本如何存储用于检索？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            }
          ]
        },
        {
          "subcategory": "RAG-基础",
          "count": 33,
          "questions": [
            {
              "id": "rag_0021",
              "question": "Graph RAG 思路介绍？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0022",
              "question": "RAG 存在什么问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0023",
              "question": "⽤ 示例 介绍 Graph RAG ？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0024",
              "question": "⽤代码 介绍 Graph RAG ？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0025",
              "question": "一、RAG 有哪些优点？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0026",
              "question": "一、为什么需要 Graph RAG？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0027",
              "question": "一、为什么需要 对 RAG 进行评测？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0028",
              "question": "七、RAG 存在什么问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0029",
              "question": "三、RAG 架构优化有哪些优化策略？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0030",
              "question": "三、为什么 需要 RAG-Fusion？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0031",
              "question": "三、使用 RAG 的好处？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0032",
              "question": "为什么需要 Graph RAG？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0033",
              "question": "二、RAG 各模块有哪些优化策略？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0034",
              "question": "二、RAG 存在哪些局限性？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0035",
              "question": "二、什么是 Graph RAG？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0036",
              "question": "二、什么是 RAG？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0037",
              "question": "二、如何合成 RAG 测试集？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0038",
              "question": "五、RAG 索引数据优化有哪些优化策略？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0039",
              "question": "五、介绍一下 RAG 典型实现方法？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0040",
              "question": "五、用 示例 介绍 Graph RAG ？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0041",
              "question": "五、说一下 RAG-Fusion 工作流程？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0042",
              "question": "什么是 Graph RAG？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0043",
              "question": "什么是 RAG？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0044",
              "question": "介绍一下 RAG 典型实现方法？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0045",
              "question": "介绍一下 RAG 典型案例？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0046",
              "question": "使用 RAG 的好处？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0047",
              "question": "六、介绍一下 RAG 典型案例？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0048",
              "question": "四、RAG 索引优化有哪些优化策略？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0049",
              "question": "四、用代码 介绍 Graph RAG ？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0050",
              "question": "四、说一下 RAG-Fusion 核心技术？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0051",
              "question": "在做RAG 项目过程中遇到哪些问题？怎么解决的？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0052",
              "question": "用 示例 介绍 Graph RAG ？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0053",
              "question": "用代码 介绍 Graph RAG ？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            }
          ]
        },
        {
          "subcategory": "RAG-基础与对比",
          "count": 26,
          "questions": [
            {
              "id": "rag_0054",
              "question": "2 RAG 有哪些评其方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0055",
              "question": "RAG 和 SFT 微调有什么不同？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0056",
              "question": "RAG 调用模式有几种？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0057",
              "question": "Self-RAG 的推理过中？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0058",
              "question": "Self-RAG 的训练过中？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0059",
              "question": "”， “什么是糖尿病？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0060",
              "question": "”， “冠心病怎么治疗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0061",
              "question": "”， “冠心病的治疗方法有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0062",
              "question": "■ （“什么是冠心病？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0063",
              "question": "■ （“冠心病的症状是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0064",
              "question": "○ 问题：“什么是大模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0065",
              "question": "上下文长度过长怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0066",
              "question": "为什么要微调 Bert 模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0067",
              "question": "什么是交互型模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0068",
              "question": "介绍下 RAG-Fusion？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0069",
              "question": "介绍下 SELF-RAG？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0070",
              "question": "使用什么相似度模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0071",
              "question": "分别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0072",
              "question": "基于 Retrieval-based LMs 的对话流程是怎么样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0073",
              "question": "如何判断上下文是否关联？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0074",
              "question": "如何提高搜索质量和大语言模型的推理能力？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0075",
              "question": "生成对应的问题：“什么是大模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0076",
              "question": "请添加微信：nlpdreampai，获得专属职业咨询服务。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0077",
              "question": "请说 “根据已知信息无法回答该问题” 或 “没有提供足够的相关信息”，不允许在？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0078",
              "question": "输入文档的顺序对大模型是否有影响？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0079",
              "question": "长篇知识如何产生问答对？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            }
          ]
        },
        {
          "subcategory": "RAG-排序与过滤（Rerank）",
          "count": 4,
          "questions": [
            {
              "id": "rag_0080",
              "question": "Graph RAG 排序优化⽅式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0081",
              "question": "Graph RAG 排序优化方式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0082",
              "question": "介绍下 Rerank 模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0083",
              "question": "六、Graph RAG 排序优化方式？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            }
          ]
        },
        {
          "subcategory": "RAG-生成与Prompt",
          "count": 1,
          "questions": [
            {
              "id": "rag_0084",
              "question": "prompt 模板如何构建？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            }
          ]
        },
        {
          "subcategory": "RAG-知识库构建与切块",
          "count": 6,
          "questions": [
            {
              "id": "rag_0085",
              "question": "为什么要进行文本切块？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0086",
              "question": "什么是句子窗口检索？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0087",
              "question": "什么是父文档检索器？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0088",
              "question": "分块策略都有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0089",
              "question": "语义分块模型都有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0090",
              "question": "选择分块策略时，需要考虑哪些要素？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            }
          ]
        },
        {
          "subcategory": "RAG-评估与挑战",
          "count": 5,
          "questions": [
            {
              "id": "rag_0091",
              "question": "4 RAG 有哪些评其框架？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rag_0092",
              "question": "三、RAG 有哪些评估方法？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0093",
              "question": "五、RAG 有哪些评估框架？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0094",
              "question": "四、RAG 有哪些关键指标和能力？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rag_0095",
              "question": "如何评估 RAG 系统的准确率上下限问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            }
          ]
        }
      ]
    },
    {
      "category": "Reinforcement Learning",
      "count": 228,
      "subcategories": [
        {
          "subcategory": "RL-基础",
          "count": 9,
          "questions": [
            {
              "id": "rl_0001",
              "question": "1 介绍一下 强化学习 中 优化方法 Value-based？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rl_0002",
              "question": "2 介绍一下 强化学习 中 贝尔曼方程？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rl_0003",
              "question": "3 介绍一下 强化学习 中 优势函数Advantage Functions？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rl_0004",
              "question": "4 强化学习 有哪些 Policy策略？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rl_0005",
              "question": "5 介绍一下 强化学习 的 轨迹？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rl_0006",
              "question": "6 介绍一下 强化学习 的 奖赏函数？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rl_0007",
              "question": "7 介绍一下 强化学习问题？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rl_0008",
              "question": "介绍一下强化学习 的 状态（States） 和 观测（Observations）？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "rl_0009",
              "question": "介绍一下强化学习？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            }
          ]
        },
        {
          "subcategory": "Reinforcement Learning-General",
          "count": 219,
          "questions": [
            {
              "id": "rl_0010",
              "question": "1 RLHF 训练过程，怎么选取最优 checkpoint？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0011",
              "question": "1 为什么需要 RLHF 替代方案？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0012",
              "question": "1 什么是 PPO 中 采样过程？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0013",
              "question": "1 介绍一下 LLaMA 2 的 RLHF？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0014",
              "question": "1 具体介绍一下 有监督微调（Supervised Tinetuning）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0015",
              "question": "1 具体介绍一下 预训练（Pre-training）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0016",
              "question": "1 简单介绍一下 RLHF 流程？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0017",
              "question": "1 简单介绍一下 对齐（Alignment）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0018",
              "question": "2 LLaMA 2 中 Margin Loss 的 实现逻辑？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0019",
              "question": "2 RLHF 有哪些替代方案？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0020",
              "question": "2 介绍一下 PPO 中 采样策略？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0021",
              "question": "2 如何在在预训练好的模型上进行有监督微调？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0022",
              "question": "2 有监督微调（Supervised Tinetuning）的训练数据格式是什么样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0023",
              "question": "3 LLaMA 2 中 两个RM模型 的 实现逻辑？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0024",
              "question": "3 PPO 中 采样策略中，如何评估“收益”？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0025",
              "question": "3 如何在有监督微调模型基础上创建一个RM模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0026",
              "question": "3 预训练（Pre-training） vs 有监督微调（Supervised Tinetuning）区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0027",
              "question": "4 LLaMA 2 中 拒绝采样 逻辑？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0028",
              "question": "4 如何基于RM模型使用PPO算法微调SFT模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0029",
              "question": "5 instructGPT的原理，讲讲rlhf和reward？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0030",
              "question": "<!--A3C中多线程如何更新梯度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0031",
              "question": "<!--A3C和DDPG区别和共同点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0032",
              "question": "<!--A3C算法是如何异步更新的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0033",
              "question": "<!--Actor-Critic两者的区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0034",
              "question": "<!--Actor-Critic的优点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0035",
              "question": "<!--Actor和Critic两者的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0036",
              "question": "<!--AlphaStar的League，能否解释一下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0037",
              "question": "<!--AlphaStar的scatter connection？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0038",
              "question": "<!--DDPG是on-policy还是off-policy，为什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0039",
              "question": "<!--DPG、DDPG、D3PG、D4PG之间的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0040",
              "question": "<!--DQN和Sarsa的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0041",
              "question": "<!--DQN的trick有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0042",
              "question": "<!--DQN的两个关键trick分别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0043",
              "question": "<!--DQN的几个变种以及各自解决了那些问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0044",
              "question": "<!--DQN的原理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0045",
              "question": "<!--DQN都有哪些变种？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0046",
              "question": "<!--DRL要实现足够的泛化Generalization有哪些做法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0047",
              "question": "<!--GA3C算法的queue如何实现？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0048",
              "question": "<!--Inverse RL 能否解决奖励问题，如何解决的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0049",
              "question": "<!--MADDPG如何解决离散action的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0050",
              "question": "<!--MC、TD谁的方差大，为什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0051",
              "question": "<!--MC和TD分别是无偏估计吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0052",
              "question": "<!--Meta RL不好应用的原因有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0053",
              "question": "<!--Model-based和model-free的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0054",
              "question": "<!--POMDP是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0055",
              "question": "<!--PPO在实现上是怎么采样的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0056",
              "question": "<!--PPO算法中的clip如何实现的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0057",
              "question": "<!--PPO算法中的损失函由那些组成？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0058",
              "question": "<!--PPO里使用的GAE是怎么实现的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0059",
              "question": "<!--Q-learning是off-policy的方法，为什么不使用重要性采样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0060",
              "question": "<!--RL不同于其它学习算法的原因？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0061",
              "question": "<!--RL的马尔科夫性质？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0062",
              "question": "<!--Ray怎么做梯度并行运算的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0063",
              "question": "<!--TD(λ)方法：当λ=0时实际上与哪种方法等价，λ=1呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0064",
              "question": "<!--TD3和DDPG的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0065",
              "question": "<!--TD3如何解决过估计？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0066",
              "question": "<!--TD（λ）方法：当λ=0时实际上与哪种方法等价，λ=1呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0067",
              "question": "<!--actor-critic框架中的critic起了什么作用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0068",
              "question": "<!--advantage(优势函数)推导过程，如何计算？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0069",
              "question": "<!--off-policy和on-policy的好与坏？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0070",
              "question": "<!--on-policy 和off-policy的区别与联系？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0071",
              "question": "<!--sarsa的公式以及和Q-leaning的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0072",
              "question": "<!--value-based和policy-based关系？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0073",
              "question": "<!--value-based和policy-based的区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0074",
              "question": "<!--不打破数据相关性，神经网络的训练效果为什么就不好？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0075",
              "question": "<!--为什么Policy中输出的动作需要sample，而不是直接使用呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0076",
              "question": "<!--为什么TRPO能保证新策略的回报函数单调不减？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0077",
              "question": "<!--为什么使用优势函数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0078",
              "question": "<!--为什么连续动作环境下使用DDPG的表现还没有直接动作离散化后Q-learning表现好？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0079",
              "question": "<!--什么是DDPG，并画出DDPG框架结构图？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0080",
              "question": "<!--什么是Importance Sampling？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0081",
              "question": "<!--什么是强化学习？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0082",
              "question": "<!--什么是重要性采样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0083",
              "question": "<!--你做过的强化学习项目有哪些，遇到的难点有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0084",
              "question": "<!--你在强化学习模型调试中，有哪些调优技巧？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0085",
              "question": "<!--你熟悉的多智能体环境有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0086",
              "question": "<!--值迭代和策略迭代的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0087",
              "question": "<!--写出用第n步的值函数更新当前值函数的公式（1-step，2-step，n-step的意思）。当n的取值变大时，期望和方差分别变大、变小？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0088",
              "question": "<!--写出蒙特卡洛、TD和TD（λ）这三种方法更新值函数的公式？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0089",
              "question": "<!--写出贝尔曼期望方程和贝尔曼最优方程？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0090",
              "question": "<!--分层强化学习的原理是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0091",
              "question": "<!--场景中状态是什么，当前状态怎么转移到下一状态？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0092",
              "question": "<!--基于值函数方法的算法有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0093",
              "question": "<!--多智能体之间如何通信、如何竞争？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0094",
              "question": "<!--多智能体强化学习算法有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0095",
              "question": "<!--多臂老虎机和强化学习算法的差别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0096",
              "question": "<!--多臂老虎机算法的分类？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0097",
              "question": "<!--如何理解利用平均KL散度代替最大KL散度？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0098",
              "question": "<!--如果不满足马尔科夫性怎么办？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0099",
              "question": "<!--对于drl在机器人上的应用怎么看？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0100",
              "question": "<!--对于hard exploration的问题，要怎么处理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0101",
              "question": "<!--对于多个entity的observation，你会怎么预处理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0102",
              "question": "<!--常见的平衡探索与利用的方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0103",
              "question": "<!--强化学习中如何处理归一化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0104",
              "question": "<!--强化学习中如何解决高纬度输入输出问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0105",
              "question": "<!--强化学习和监督学习、无监督学习的区别是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0106",
              "question": "<!--强化学习在机器人的局限性有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0107",
              "question": "<!--强化学习如何如何确定收敛？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0108",
              "question": "<!--强化学习如何用在推荐系统中？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0109",
              "question": "<!--强化学习如何观察收敛曲线？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0110",
              "question": "<!--强化学习是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0111",
              "question": "<!--强化学习用来解决什么问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0112",
              "question": "<!--强化学习的损失函数是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0113",
              "question": "<!--强化学习的损失函数（loss function）是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0114",
              "question": "<!--强化学习适合解决什么样子的问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0115",
              "question": "<!--强化学习需要大量数据，如何生成或采集到这些数据？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0116",
              "question": "<!--影响强化学习算法收敛的因素有哪些，如何调优？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0117",
              "question": "<!--手工推导策略梯度过程？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0118",
              "question": "<!--推荐场景中奖赏函数如何设计？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0119",
              "question": "<!--描述随机策略和确定性策略的特点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0120",
              "question": "<!--是否了解RLlib？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0121",
              "question": "<!--是否了解α−Rank算法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0122",
              "question": "<!--是否了解过D4PG算法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0123",
              "question": "<!--是否了解过奖励函数的设置(reward shaping)？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0124",
              "question": "<!--是否理解Entropy，KL divergence和Mutual Information的含义？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0125",
              "question": "<!--是否用某种DRL算法玩过Torcs游戏？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0126",
              "question": "<!--最优值函数和最优策略为什么等价？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0127",
              "question": "<!--有哪些方法可以使得RL训练稳定？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0128",
              "question": "<!--有那几种Bandit算法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0129",
              "question": "<!--求解马尔科夫决策过程都有哪些方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0130",
              "question": "<!--深度强化学习中的DQN和A3C区别与联系？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0131",
              "question": "<!--画出DQN玩Flappy Bird的流程图。在这个游戏中，状态是什么，状态是怎么转移的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0132",
              "question": "<!--确定性策略和 随机性策略的区别与联系？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0133",
              "question": "<!--离散action和连续action在处理上有什么相似和不同的地方？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0134",
              "question": "<!--策略梯度和actor-critic的关系与对比？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0135",
              "question": "<!--策略梯度方法中基线baseline如何确定？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0136",
              "question": "<!--策略梯度的推导过程？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0137",
              "question": "<!--策略梯度算法的目标函数和策略梯度计算？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0138",
              "question": "<!--简述A3C的优势函数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0139",
              "question": "<!--简述A3C算法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0140",
              "question": "<!--简述AI-GAs？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0141",
              "question": "<!--简述DPPO和PPO的关系？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0142",
              "question": "<!--简述DRL的一些最新改进？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0143",
              "question": "<!--简述Imitation Learning？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0144",
              "question": "<!--简述MADDPG算法的过程和伪代码？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0145",
              "question": "<!--简述Meta Gradient Reinforcement Learning？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0146",
              "question": "<!--简述Meta Reinforcement Learning？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0147",
              "question": "<!--简述Model Based Learning？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0148",
              "question": "<!--简述Multi-Agent Reinforcement Learning？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0149",
              "question": "<!--简述Multi-Task Reinforcement Learning？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0150",
              "question": "<!--简述Neural-Symbolic Learning的方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0151",
              "question": "<!--简述Out-of-Distributon Generalization？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0152",
              "question": "<!--简述PER算法、HER算法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0153",
              "question": "<!--简述PPO、DPPO算法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0154",
              "question": "<!--简述PPO算法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0155",
              "question": "<!--简述Pointer Network？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0156",
              "question": "<!--简述Transformer？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0157",
              "question": "<!--简述UCB算法 （Upper Confidence Bound)？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0158",
              "question": "<!--简述double DQN原理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0159",
              "question": "<!--简述go-explore？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0160",
              "question": "<!--简述offline reinforcement learning？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0161",
              "question": "<!--简述seed rl？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0162",
              "question": "<!--简述sim2real？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0163",
              "question": "<!--简述unsupervised reinforcement learning？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0164",
              "question": "<!--简述值函数逼近的想法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0165",
              "question": "<!--简述分层强化学习中基于目标的(goal-reach)和基于目标的(goal-reach）的区别与联系？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0166",
              "question": "<!--简述动态规划(DP)算法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0167",
              "question": "<!--简述模仿学习与强化学习的区别、联系？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0168",
              "question": "<!--简述重要性采样，Thompson sampling采样？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0169",
              "question": "<!--自动驾驶和机器人的场景如何建模成强化学习问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0170",
              "question": "<!--蒙特卡洛、TD、动态规划的关系？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0171",
              "question": "<!--蒙特卡洛和时间差分的对比：MC和TD分别是无偏估计吗，为什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0172",
              "question": "<!--表格式到函数近似的理解？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0173",
              "question": "<!--请简述IQL（independent Q-learning算法过程？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0174",
              "question": "<!--请简述QMIX算法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0175",
              "question": "<!--请简述造成强化学习inefficient的原因？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0176",
              "question": "<!--贝尔曼方程的具体数学表达式是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0177",
              "question": "<!--贝尔曼期望方程和贝尔曼最优方程什么时候用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0178",
              "question": "<!--重要性采样的推导过程、作用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0179",
              "question": "<!--阐述目标网络和experience replay的作用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0180",
              "question": "A3C是on-policy还是off-policy，为什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0181",
              "question": "Coach？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0182",
              "question": "Deepminic？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0183",
              "question": "Diversity is all you need？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0184",
              "question": "Dream？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0185",
              "question": "HIR是如何工作的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0186",
              "question": "LASER？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0187",
              "question": "MC、TD谁的方差大，为什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0188",
              "question": "MDP各元素对应真实场景中的哪些变量？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0189",
              "question": "Modularity？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0190",
              "question": "MuZero？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0191",
              "question": "Policy Distillation？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0192",
              "question": "Randomization？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0193",
              "question": "SL靠的是样本标签训练模型，RL依靠的是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0194",
              "question": "Tinetuning）区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0195",
              "question": "一、介绍一下 LLM的经典预训练Pipeline？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0196",
              "question": "一、大语言模型RLHF中的PPO主要分哪些步骤？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0197",
              "question": "两者在使用方式上有何不同？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0198",
              "question": "为什么PPO和IMPALA要使用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0199",
              "question": "二、举例描述一下 大语言模型的RLHF？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0200",
              "question": "从Q-learning的算法中可以看出，其行为策略为-greedy策略，目标策略是greedy策略，因此属于off-policy方法。那么为什么没有用重要性采样呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0201",
              "question": "但，现在的问题是：根本无法获得「真实分数」，我们该如何找到这个「最高点」呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0202",
              "question": "你对这个理论有什么看法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0203",
              "question": "其损失函数是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0204",
              "question": "具体怎么解决？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0205",
              "question": "动态规划是怎么回事？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0206",
              "question": "和一般的Attention有什么不同？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0207",
              "question": "和一般的DRL有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0208",
              "question": "和有监督学习的异同？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0209",
              "question": "和深度学习的损失函数有何关系？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0210",
              "question": "奖赏函数如何设计，有没有奖赏延迟问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0211",
              "question": "如何让agent足够diverse？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0212",
              "question": "对于大规模分布式强化学习，还有更好的提高throughput的方法吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0213",
              "question": "引入状态奖励的是哪种？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0214",
              "question": "当前时刻的状态和它之前很多很多个状态都有关之间关系？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0215",
              "question": "怎么实现的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0216",
              "question": "恰好，如果我们用上Fixed Q-targets，我们不就是有两个Q网络了吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0217",
              "question": "是否能够阐述GA3C和A3C的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0218",
              "question": "有什么新的进展？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0219",
              "question": "有哪些方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0220",
              "question": "有模型用什么方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0221",
              "question": "比如MADDPG比较早的，思想是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0222",
              "question": "比如World Model？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0223",
              "question": "神经网络要怎么构建？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0224",
              "question": "能否具体介绍一下实现方法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0225",
              "question": "能否写出计算过程？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0226",
              "question": "里面的“马尔科夫”体现了什么性质？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0227",
              "question": "马尔科夫决策过程是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "rl_0228",
              "question": "马尔科夫过程是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            }
          ]
        }
      ]
    },
    {
      "category": "SLAM/Robotics",
      "count": 80,
      "subcategories": [
        {
          "subcategory": "SLAM-基础",
          "count": 80,
          "questions": [
            {
              "id": "slam_0001",
              "question": "Filters）方法，所以需要了解粒子滤波的方法（利用统计特性描述物理表达式下的结果） ？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0002",
              "question": "Gmapping 是应用最为广泛的 2D slam 方法，主要是利用 RBPF（Rao-Blackwellized Particle？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0003",
              "question": "Hector slam 对传感器的要求比较高，它主要是利用高斯牛顿方法来解决 scan-matching 的？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0004",
              "question": "K-dimensional Tree （Kd树）是一种常用的建立特征向量索引的方法。建立 Kd树时，要不？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0005",
              "question": "KartoSLAM 是基于图优化 的方法，用高度优化和非迭代 cholesky 矩阵进行稀疏系统解耦作？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0006",
              "question": "LSD -SLAM：将直接法应用到了半稠密单目 SLAM中。 （所以拥有直接法的优缺点，对特征？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0007",
              "question": "LagoSLAM 是线性近似图优化 ， 不需要初始假设， 优化器的方法可以有三种选择 Tree-based？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0008",
              "question": "ORB-SLAM 等算法。 稠密方法： 使用图像的总体亮度以及 DTAM、LSD-SLAM、DSO 和 SVO？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0009",
              "question": "P3P方法是通过 3对3D/2D匹配点，求解出四种可能的姿态，在 OpenCV calib3d 模块中有？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0010",
              "question": "SLAM最核心的地方在算法，侧重点在于如何构建出效果好的地图，并为机器人导航提供更？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0011",
              "question": "g2o、Ceres Solver 等实现的 Graph SLAM ） 。这些方法通过递归或批量优化的方式，迭代更？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0012",
              "question": "netORK Optimizer(TORO), g2o,LAGO 。基本的图优化 slam的方法就是利用最小化非线性非？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0013",
              "question": "new Block( linearSolver ); // 矩阵块求解器 // 梯度下降方法，从 GN, LM, DogLeg 中选？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0014",
              "question": "p2相机外参分为旋转矩阵 R和平移矩阵 t，旋转矩阵和平移矩阵共同描述了如何把点从世？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0015",
              "question": "sobel算子：一阶导数算子，引入局部平均运算，对噪声具有平滑作用，抗噪声能力强，计？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0016",
              "question": "• query过程（可以理解为射线渲染）需要大量的采样，渲染方法成本很高？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0017",
              "question": "• 导致“遗忘”问题？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0018",
              "question": "一阶梯度下降， G-N和L-M三种方法？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0019",
              "question": "为了解决这个问题，引入了占据栅格地图（ Occupancy Grid Map ）的概念。我们将地图栅格？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0020",
              "question": "为什么使用占据栅格地图构建算法构？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0021",
              "question": "为什么是Gaussian-Based SLAM？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "slam_0022",
              "question": "为什么是NeRF-Based SLAM？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "slam_0023",
              "question": "为解，图优化方法利用图的均值表示地图，每个节点表示机器人轨迹的一个位置点和传感器？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0024",
              "question": "于gmapping 。这是由于 hector过分依赖 scan-match。特别是在长廊问题中，误差更加明？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0025",
              "question": "仅仅使用单个传感器，就可以解决 SLAM的问题，如果是这样的话，这反而变得简单了。但？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0026",
              "question": "但是没有办法消弭全局范围内降低误差呢。所以为了解决这个问题，早期的研发人员又想出？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0027",
              "question": "位置信息的丢失，在这种情况下如何重新确定自己的位置？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0028",
              "question": "作）->局部地图跟踪 （EPNP）->判断是否生成关键帧 ->生成关键帧？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0029",
              "question": "使得优化变的困难，通过李群李代数的转换关系，把位姿估计变成无约束的优化问题。优化？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0030",
              "question": "假设，给定一组正确的数据，存在可以计算出符合这些数据的模型参数的方法。举个例子通？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0031",
              "question": "关系恰好是与指数相乘的关系，用求导的定义式子就可以求解优化问题了？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0032",
              "question": "其中，阻尼因子 μ的作用是，它大于 0保持正定， μ大就接近最速下降法， μ小就接近高斯？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0033",
              "question": "凸代价函数，每次迭代，解决局部凸近似的初始问题来更新图配置，过程迭代一定次数直到？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0034",
              "question": "创建一个 Option，配置一下求解器的配置，创建一个 Summary 。最后调用 Solve方法，求？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0035",
              "question": "单目视觉 slam中尺寸漂移是怎么产生的？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0036",
              "question": "回环检测判断自动驾驶车辆或机器人是否到达过先前的位置。 SLAM系统内嵌有强大的闭环？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0037",
              "question": "好的数据保障。 ROS帮忙解决传感器驱动、显示、各种核心算法间的沟通协调问题。如果做？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0038",
              "question": "如何优化重投影误差？采用什么方法求解？如果误匹配的点重投影之后误差很大， 如？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0039",
              "question": "如何优化重投影误差？采用什么方法？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0040",
              "question": "如何处理关键帧 （可以参考 ORBSLAM2？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0041",
              "question": "如何对匹配好的点做进一步的处理，更？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0042",
              "question": "姿的方法使得误差目标函数达到最小，所以它求得的是当前帧相对于上一帧的位姿变换，都？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0043",
              "question": "导致跟丢车辆。此外，点云匹配通常需要高处理能力，因此必须优化流程来提高速度。鉴于？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0044",
              "question": "小误匹配对整个方法的影响？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0045",
              "question": "就可以实现这一目的。而回环检测，就是在全局优化和实时处理之间的一种妥协方法。这种？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0046",
              "question": "度。Flash原本的意思为快闪。而 Flash激光雷达的原理也是快闪，不像 MEMS或OPA的方？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0047",
              "question": "很多情况下，传统 CG方法重建地图都能有相当好的效果，但是对于地图上的未知区域，进？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0048",
              "question": "很大，如何解决它对整个优化问题的影响？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0049",
              "question": "想怎么个实现？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0050",
              "question": "择。信息感知，需要考虑如何全面的感知这个环境， RGBD摄像头 FOV通常比较小，但激光？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0051",
              "question": "方法 1，用数学优化方法来解决。一般来说，简单的 SLAM定位只考虑传感器数据前后帧的？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0052",
              "question": "方法 3，这也是比较容易想到的办法，那就是一个传感器不行，就多个传感器一起运行。最？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0053",
              "question": "机器人学中地图的表示方法有四种：特征地图、拓扑地图、栅格地图以及直接表征法？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0054",
              "question": "来了回环检测的方法。它的道理还是比较简单的，主要是第一次制图的时候使用，它就是告？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0055",
              "question": "某个 SLAM框架工作原理，优缺点，改？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0056",
              "question": "此时难以定义深度。这个问题可以通过以下方式解决：检测待定位图像中的 AR 标记、棋盘？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0057",
              "question": "求解 PnP问题目前主要有直接线性变换（ DLT），P3P，EPnP，UPnP以及非线性优化方法？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0058",
              "question": "注意，那 SH和SF怎么算呢？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0059",
              "question": "测量单元和视觉、激光雷达的融合可以解决视觉里程计的漂移和尺度丢失问题 ,提高系统在？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0060",
              "question": "然要想解决这个问题，我们就必须要一定的传感器输入，通过算法计算，最终输出自己的位？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0061",
              "question": "用线性三角形法中的齐次方法？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0062",
              "question": "用高斯牛顿方法解决 scan-matching 问题，获得激光点集映射到已有地图的刚体变换；为？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0063",
              "question": "的概率。这种地图看起来和人们所认知的地图没什么区别，它最早由 NASA的Alberto Elfes？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0064",
              "question": "的迹与行列式的比值判断关键点是否在边缘，剔除这些不稳定的边缘响应点？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0065",
              "question": "的问题则是最基本的问题，归纳起来就是两句话，第一，这是什么地方；第二，我在哪。当？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0066",
              "question": "确定深度的方法有很多种，人眼就是一个典型的双目相机模型：通过左右眼看到的景物的差？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0067",
              "question": "称为后端 (Back End) 。4. 回环检测 (Loop Closing) ，回环检测判断机器人是否到达过先前的位？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0068",
              "question": "系，归一化坐标， 像平面坐标系 和像素坐标系。要注意它们之间的区别和两两之间的转换关？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0069",
              "question": "组成镜头达到小孔成像的原理，本质还是小孔成像。感光显像是通过胶片、 CCD和CMOS？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0070",
              "question": "绑架问题就是重定位，是指机器人在缺少之前位置信息的情况下，如何去确定当前位姿。例？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0071",
              "question": "统的各个模块 ,如相对姿势估计、地图表示、闭环检测和后端优化等。学习方法与传统方法的？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0072",
              "question": "解决 slam问题，通常需要安装一个或者是多个传感器。不同的传感器有不同的用途，一般？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0073",
              "question": "较好)，还是λ I占据主导地位 (一阶近似较好 )，避免非奇异和病态问题，提供更稳定，更准确？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0074",
              "question": "那么两束光束坐标系相差 T0i。那么如何消除其影响呢，可以使用编码器或 IMU这样高频？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0075",
              "question": "问题。 Hector slam 无需使用里程计，所以在不平坦区域实现建图的空中无人机及地面小车？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0076",
              "question": "需求越大，然而图优化方式相比其他方法在大环境下制图优势更大，在某些情况下？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0077",
              "question": "预测：如何从上一时刻的状态，根据输入信息推断当前时刻的状态分布（先验）计算协方差？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0078",
              "question": "（2）距离最近关键帧的距离是否足够远（空间） /运动？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0079",
              "question": "（3）距离上一关键帧的帧数是否足够多（时间）？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            },
            {
              "id": "slam_0080",
              "question": "（4）尺度不确定性： 纯视觉 SLAM通常面临尺度不确定性问题，即无法确定绝对距离，需？",
              "variants": [],
              "sources": [
                "SLAM_算法_面试题_only.json"
              ]
            }
          ]
        }
      ]
    },
    {
      "category": "Security",
      "count": 41,
      "subcategories": [
        {
          "subcategory": "Security-基础",
          "count": 41,
          "questions": [
            {
              "id": "sec_0001",
              "question": "SSRF 有哪些常见的利用方式？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0002",
              "question": "SSRF 的攻击原理是什么？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0003",
              "question": "redis 哈希槽的概念？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "sec_0004",
              "question": "什么是 DDoS 攻击？如何防范？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0005",
              "question": "什么是 Metasploit 框架？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0006",
              "question": "什么是 SSRF（服务器端请求伪造）？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0007",
              "question": "什么是交换机？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0008",
              "question": "什么是加密算法？有哪些常见的加密算法？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "sec_0009",
              "question": "什么是安全事件响应？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0010",
              "question": "什么是安全运营工程师？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0011",
              "question": "什么是数字证书？有什么作用？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0012",
              "question": "什么是渗透测试？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0013",
              "question": "什么是漏洞扫描？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0014",
              "question": "什么是社会工程学攻击？如何预防？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0015",
              "question": "什么是跨站点脚本攻击？如何防范？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0016",
              "question": "什么是路由器？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0017",
              "question": "什么是远程桌面协议（RDP）攻击？如何避免？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "sec_0018",
              "question": "什么是防火墙？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0019",
              "question": "什么是防火墙？它的作用是什么？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0020",
              "question": "什么是黑客攻击？如何预防？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0021",
              "question": "以下链接存在 sql 注入漏洞，对于这个变形注入，你有什么思路？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "sec_0022",
              "question": "加密在网络上的重要性是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "sec_0023",
              "question": "动态场景：当点云随时间移动，如何增量更新哈希表？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "sec_0024",
              "question": "如何使用Python 实现哈希表？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "sec_0025",
              "question": "如何检测webshell？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0026",
              "question": "如何解决 XSS 攻击问题？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0027",
              "question": "如何进行安全漏洞扫描？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0028",
              "question": "如何进行安全漏洞管理？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0029",
              "question": "如何防御和修复 SSRF 漏洞？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0030",
              "question": "如何防止网络钓鱼攻击？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "sec_0031",
              "question": "如何防范恶意软件攻击？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0032",
              "question": "如何验证存在xss漏洞？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0033",
              "question": "怎么检测csrf？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0034",
              "question": "怎么进行横向移动？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0035",
              "question": "描述什么是端口号？",
              "variants": [],
              "sources": [
                "网安_合并题目_only_questions_strict_v2.json"
              ]
            },
            {
              "id": "sec_0036",
              "question": "支持一致性哈希的客户端有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "sec_0037",
              "question": "方案必须落地，能回答：缓存目录怎么组织？哈希怎么算？回滚怎么做？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "sec_0038",
              "question": "渗透测试中常用的网络协议分析工具有哪些？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "sec_0039",
              "question": "渗透测试中的网络钓鱼攻击是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "sec_0040",
              "question": "说说Redis 哈希槽的概念？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "sec_0041",
              "question": "，如果看到一个告警IP，如何判断是否是真实攻击？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            }
          ]
        }
      ]
    },
    {
      "category": "Transformer",
      "count": 184,
      "subcategories": [
        {
          "subcategory": "General AI-General",
          "count": 27,
          "questions": [
            {
              "id": "tr_0001",
              "question": "1 基于Transformer的预训练模型如何应用于文本分类领域？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0002",
              "question": "Transformer encoder和decoder的区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0003",
              "question": "Transformer为什么要用Layer norm？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0004",
              "question": "Transformer为何能够有效地处理长距离依赖问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0005",
              "question": "Transformer模型中注意力权重如何解释模型的决策？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0006",
              "question": "Transformer模型中的前馈网络(Feed-Forward Networks)的作用是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0007",
              "question": "Transformer模型中的自注意力机制在计算效率和表示能力之间是如何权衡的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0008",
              "question": "Transformer模型在实践中如何优化以处理超长序列？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0009",
              "question": "Transformer模型在自注意力层中如何解决多尺度表示问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0010",
              "question": "Transformer模型如何处理变长输入序列？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0011",
              "question": "Transformer模型如何平衡模型性能与计算资源的消耗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0012",
              "question": "Transformer模型的参数共享策略对模型性能有何影响？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0013",
              "question": "Transformer模型的基本结构是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0014",
              "question": "Transformer模型的缩放点积注意力(Scaled Dot-Product Attention)是什么，其重要性在哪里？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0015",
              "question": "Transformer模型的自注意力机制如何实现并行处理？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0016",
              "question": "Transformer的两个mask机制是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0017",
              "question": "Transformer网络很深，是怎么避免过拟合问题的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0018",
              "question": "为什么Transformer可以处理多种模态，它是怎么处理的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0019",
              "question": "十一、LORA应该作用于Transformer的哪个参数矩阵？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0020",
              "question": "多头注意力机制和单个注意力机制时间复杂度会变吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0021",
              "question": "多头注意力的作用是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0022",
              "question": "如何在自注意力机制中平衡局部信息和全局信息的捕获？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0023",
              "question": "如何提高Transformer模型中自注意力机制的计算效率？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0024",
              "question": "如何解决Transformer长度限制的问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0025",
              "question": "如何设计更有效的注意力机制来处理层次化或结构化数据？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0026",
              "question": "描述下Transformer的结构？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0027",
              "question": "请务必检查Tensor2Tensor notebook ，在⾥⾯你可以下载⼀个Transformer模型，并⽤交互式可视化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            }
          ]
        },
        {
          "subcategory": "TR-注意力机制",
          "count": 1,
          "questions": [
            {
              "id": "tr_0028",
              "question": "多头注意力机制（Multi-head Attention）是什么？它相比单头注意力有什么优势？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            }
          ]
        },
        {
          "subcategory": "TR-结构",
          "count": 9,
          "questions": [
            {
              "id": "tr_0029",
              "question": "FasterTransformer 介绍一下？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "tr_0030",
              "question": "FasterTransformer 优化？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "tr_0031",
              "question": "FasterTransformer 核⼼是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "tr_0032",
              "question": "Transformer 判别器如何加 SN？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "tr_0033",
              "question": "与Transformer-based检测头结合时，DFL是否还有优势？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "tr_0034",
              "question": "为什么需要 FasterTransformer？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "tr_0035",
              "question": "什么是层归一化（Layer Normalization），它在Transformer 中有什么作用？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "tr_0036",
              "question": "什么是预训练和微调（Fine-tuning），在Transformer 模型中如何应用？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "tr_0037",
              "question": "如果未来Transformer Decoder替代 CNN，通道数还会是 2 的幂次吗？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            }
          ]
        },
        {
          "subcategory": "Transformer-General",
          "count": 147,
          "questions": [
            {
              "id": "tr_0038",
              "question": "Connection）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0039",
              "question": "Decoder 端可以做并行化么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0040",
              "question": "Decoder 端可以做并行化吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0041",
              "question": "Decoder 阶段的多头自注意力和 encoder 的多头自注意力有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0042",
              "question": "Encoder 和 Decoder 端是如何进行交互的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0043",
              "question": "Encoder 端和 Decoder 端是如何进行交互的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0044",
              "question": "GPT 模型的架构和特点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0045",
              "question": "LoRA怎么做的，讲一下？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0046",
              "question": "Position-wise feed-forward networks 具体是怎么设计的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0047",
              "question": "Transformer 中有什么作用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0048",
              "question": "Transformer 中的前馈神经网络有什么作用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0049",
              "question": "Transformer 中的自注意力机制是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0050",
              "question": "Transformer 中的自注意力机制是如何工作的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0051",
              "question": "Transformer 中需要它？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0052",
              "question": "Transformer 为何使用多头注意力机制？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0053",
              "question": "Transformer 在机器翻译任务中的应用是怎样的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0054",
              "question": "Transformer 是如何工作的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0055",
              "question": "Transformer 是如何训练的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0056",
              "question": "Transformer 有何特点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0057",
              "question": "Transformer 模型中的多头注意力机制有什么优点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0058",
              "question": "Transformer 模型的编码器和解码器结构是怎样的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0059",
              "question": "Transformer 的优势是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0060",
              "question": "Transformer 的局限性是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0061",
              "question": "Transformer 的并行化体现在什么地方？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0062",
              "question": "Transformer 的并行化提现在哪个地方？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0063",
              "question": "Transformer 计算 attention 的时候为何选择点乘而不是加法？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0064",
              "question": "bert 的 mask 为何不学习 transformer 在 attention 处进行屏蔽 score 的操作？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0065",
              "question": "c.self-attention一定要这样表达吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0066",
              "question": "d.有其他方法不用除根号 吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0067",
              "question": "decoder 阶段的多头注意力机制和 encoder 的多头注意力机制有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0068",
              "question": "e.为什么transformer用Layer Norm？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0069",
              "question": "f.为什么不用BN？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0070",
              "question": "g.Bert为什么要搞一个position embedding？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0071",
              "question": "h.Bert为什么三个embedding可以相加？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0072",
              "question": "i.transformer为什么要用三个不一样的QKV？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0073",
              "question": "j.为什么要多头？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0074",
              "question": "k.为什么Bert中要用WordPiece/BPE这样的subword Token？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0075",
              "question": "l.Bert中为什么要在开头加个[CLS]？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0076",
              "question": "m.不用[CLS]的语义输出，有其他方式可以代替吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0077",
              "question": "mutil_head_self_attention 里，为什么这样做？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0078",
              "question": "n.Bert中有哪些地方用到了mask？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0079",
              "question": "o.预训练阶段的mask有什么用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0080",
              "question": "p.attention中的mask有什么用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0081",
              "question": "scaled dot production 为什么要除以一个根号 dk？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0082",
              "question": "self attention 无法学习到序列信息？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0083",
              "question": "self attention 部分怎么计算的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0084",
              "question": "size 的开方？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0085",
              "question": "t.Bert是如何处理传统方法难以搞定的溢出词表词(oov)的语义学习的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0086",
              "question": "transformer 中的前馈神经网络？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0087",
              "question": "transformer 其它的一些细节问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0088",
              "question": "transformer 哪里做了权重共享？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0089",
              "question": "transformer 的输入是什么样的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0090",
              "question": "transformer 这个黑盒子里面都有什么呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0091",
              "question": "u.中文是如何处理溢出词表词(oov)的语义学习的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0092",
              "question": "x.为什么说GPT是单向的Bert是双向的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0093",
              "question": "y.Bert如何处理一词多义？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0094",
              "question": "z+.Albert是通过什么方法压缩网络参数的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0095",
              "question": "z.Bert中的transformer和原生的transformer有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0096",
              "question": "——为什么无明显语义？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0097",
              "question": "——为什么要公平？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0098",
              "question": "“为”、“我”、“母”归一到同一分布后，第一句话中的“为”和“中”就没有可比性了，何谈同一句子之间的注意力机制？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0099",
              "question": "两者计算复杂度和效果上有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0100",
              "question": "为什么 Bert 的三个 Embedding 可以进行相加？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0101",
              "question": "为什么 decoder 需要 sequence mask？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0102",
              "question": "为什么可以做权重共享？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0103",
              "question": "为什么可以用LoRA？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0104",
              "question": "为什么后续有不少工作尝试对 softmax 进行替换？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0105",
              "question": "为什么在进行 softmax 之前需要对 attention 进行 scaled（为什么除以 dk 的平方？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0106",
              "question": "为什么在进行 softmax 之前需要对 attention 进行？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0107",
              "question": "为什么在进行多头注意力的时候需要对每个 head 进行降维？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0108",
              "question": "为什么要使用 qeury，value，key 矩阵？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0109",
              "question": "为什么要做 position embedding/encoding？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0110",
              "question": "为什么要做 softmax 标准化？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0111",
              "question": "为什么要加 ffn？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0112",
              "question": "为什么要引入 Transformer？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0113",
              "question": "为什么要设计多头注意力而不用单头注意力？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0114",
              "question": "为什么需要 Decoder 自注意力进行序列掩码？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0115",
              "question": "为什么需要进行 scaled？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0116",
              "question": "为何不能使用同一个值进行自身的点乘？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0117",
              "question": "为何在获取输入词向量之后需要对矩阵乘以 embedding size 的开方？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0118",
              "question": "什么是 BERT 模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0119",
              "question": "什么是 Transformer 及其架构？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0120",
              "question": "什么是 Transformer 模型中的头（Head），它的作用是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0121",
              "question": "什么是 Transformer 模型？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0122",
              "question": "什么是 transformer 呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0123",
              "question": "什么是LLM的复读机问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0124",
              "question": "什么是位置编码（Positional Encoding），为什么在 Transformer 中需要它？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0125",
              "question": "什么是位置编码（Positional Encoding），为什么在？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0126",
              "question": "什么是多任务学习，Transformer 如何应用于多任务学习？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0127",
              "question": "什么是层归一化（Layer Normalization）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0128",
              "question": "什么是注意力矩阵，如何计算？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0129",
              "question": "什么是预训练和微调（Fine-tuning），在 Transformer？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0130",
              "question": "什么是预训练和微调（Fine-tuning）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0131",
              "question": "以做并行化么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0132",
              "question": "以做并行化吗？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0133",
              "question": "任何norm的意义都是为了让使用norm的网络的输入的数据分布变得更好，也就是转换为标准正态分布，数值进入敏感度区间，以减缓梯度消失，从而更容易训练。当然，这也意味着舍弃了除此维度之外其他维度的其他信息。为什么能舍弃呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0134",
              "question": "你了解哪些 attention 机制？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0135",
              "question": "你怎么理解注意力机制？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0136",
              "question": "你还了解哪些关于位置编码的技术，各自的优缺点是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0137",
              "question": "其中使用了什么激活函数？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0138",
              "question": "出现复读机问题的可能原因有哪些？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0139",
              "question": "力有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0140",
              "question": "力机制有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0141",
              "question": "因为训练的时候BERT发现每个句子头都有，这样他能学到什么语义呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0142",
              "question": "国外开源的LLaMA的词表实际上兼容中文效果可能会大打折扣，那么扩充词表该怎么做？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0143",
              "question": "在 Transformer 模型中，如何处理长序列数据？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0144",
              "question": "在 Transformer 模型的训练过程中，为什么要使用掩码（Mask）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0145",
              "question": "在自然语言生成任务中，Transformer 如何确保生成文本的连贯性和一致性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0146",
              "question": "在计算 attention score 的时候如何对 padding 做 mask 操作？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0147",
              "question": "大概讲一下 Transformer 的 Encoder 模块？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0148",
              "question": "好处是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0149",
              "question": "如何决定 Transformer 中的层数和注意力头的数量？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0150",
              "question": "如何处理 Transformer 中的不同长度的输入序列？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0151",
              "question": "如何处理 Transformer 中的缺失/损坏数据并解决过拟合问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0152",
              "question": "如何微调预训练的 Transformer 以适应特定任务？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0153",
              "question": "如何理解 Transformer 模型中的残差连接（Residual Connection）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0154",
              "question": "如何理解 Transformer 模型中的残差连接（Residual？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0155",
              "question": "如何确定 Transformer 的适当容量水平？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0156",
              "question": "如何缓解大模型的复读机问题是一个复杂的任务，并没有一个通用的解决方案。不同的方法可能适用于不同的业务场景和任务，需要根据具体的情况进行选择和调整。下面是几种用于缓解大模型复读机问题的几种解决方案。？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0157",
              "question": "如何评估和改进 Transformer 模型的性能？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0158",
              "question": "它与传统的 RNN 和 CNN 有何不同？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0159",
              "question": "它与传统神经网络有何不同？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0160",
              "question": "它与原始的 Transformer 有何不同？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0161",
              "question": "当然，不放在句子开头的其他位置是否可行？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0162",
              "question": "微调后的大模型出现灾难性遗忘是什么原因？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0163",
              "question": "想让模型学习垂直领域的知识，是应该预训练还是微调？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0164",
              "question": "意义是什么？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0165",
              "question": "意力而不用单头注意力？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0166",
              "question": "既然前面说了是CV中用BN，那为什么NLP中不用BN，而用LN呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0167",
              "question": "有什么意义和优缺点？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0168",
              "question": "有什么问题？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0169",
              "question": "果上有什么区别？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0170",
              "question": "模型中如何应用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0171",
              "question": "由此可以再深入想一想，在一串文本中，如果每个词的特征都可以用叠加波来表示，整个序列又可以进一步叠加。哪些是低频信号（比如词性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0172",
              "question": "的连贯性和一致性？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0173",
              "question": "简单介绍一下 Transformer 的位置编码？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0174",
              "question": "自身的点乘？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0175",
              "question": "解决大模型复读机问题可用哪些策略？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0176",
              "question": "训练和实现 Transformer 时有哪些常见挑战，如何改进其性能？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0177",
              "question": "进行屏蔽 score 的操作？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0178",
              "question": "那关键点就在于，为什么[CLS]可以建模整句话的语义表征呢？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0179",
              "question": "预训练和微调是哪个阶段注入知识的？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0180",
              "question": "马东什么：kaggle 上的 attention layer 到底实现的是啥？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0181",
              "question": "（Mask）？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0182",
              "question": "），哪些是高频信号（比如语义？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0183",
              "question": "，在 Transformer 模型中如何应用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            },
            {
              "id": "tr_0184",
              "question": "，它在 Transformer 中有什么作用？",
              "variants": [],
              "sources": [
                "CLEAN_ALL.json"
              ]
            }
          ]
        }
      ]
    },
    {
      "category": "Web3",
      "count": 150,
      "subcategories": [
        {
          "subcategory": "Web3-基础",
          "count": 150,
          "questions": [
            {
              "id": "web3_0001",
              "question": "DAO将如何在web环境中工作？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0002",
              "question": "Monero）是如何工作的？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0003",
              "question": "NFT是如何出现在web3中的？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0004",
              "question": "PoW(工作量证明)和 PoS(权益证明)有什么区别？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0005",
              "question": "opensea和looksrare的区别？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0006",
              "question": "web3将如何彻底改变在线用户体验？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0007",
              "question": "web3扩张的最大贡献者是去中心化金融(DeFi)。从加密货币到去中心化的贷款/借款平台，DeFi改变了许多传统的金融体系。DeFi是web3.0的突出例子之一，展示了web3概念如何在传统服务中引入效率和生产力的新基准。去中心化的金融符合web3的基本目标，即授权用户在不涉及集中中介的情况下访问其资产和数据？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0008",
              "question": "“去中心化”的基本性质是什么？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0009",
              "question": "一，展示了 web3概念如何在传统服务中引入效率和生产力的新基准。去中心化？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0010",
              "question": "与PoS有何不同？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0011",
              "question": "什么是 DeFi（去中心化金融） ，它如何？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0012",
              "question": "什么是 web3？web2 和 web3 的区别？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0013",
              "question": "什么是DeFi（去中心化金融），它如何运作？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0014",
              "question": "什么是ERC-20标准，它为什么重要？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0015",
              "question": "什么是代币化，它在区块链上如何工作？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0016",
              "question": "什么是侧链，它如何帮助区块链扩容？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0017",
              "question": "什么是共识算法？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "web3_0018",
              "question": "什么是加密货币的原子交换，它是如何工作的？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0019",
              "question": "什么是加密货币的原子交换，它是如何？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0020",
              "question": "什么是加密货币的去中心化交易所（DEX），与传统交易所有何不同？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0021",
              "question": "什么是加密货币的去中心化交易所？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0022",
              "question": "什么是加密货币钱包的原子交换，它是如何工作的？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0023",
              "question": "什么是加密货币钱包的原子交换，它是？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0024",
              "question": "什么是区块链智能合约？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0025",
              "question": "什么是区块链的 Layer 2解决方案，举？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0026",
              "question": "什么是区块链的Oracles，它们如何与智能合约交互？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0027",
              "question": "什么是去中心化？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0028",
              "question": "什么是智能合约？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0029",
              "question": "什么是智能合约？智能合约的安全隐患有哪些？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0030",
              "question": "什么是智能合约？智能合约的安全隐患？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0031",
              "question": "何为Delegated Proof of Stake（DPoS），与PoS有何不同？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0032",
              "question": "你什么时候会使用私有区块链而不是公共区块链？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0033",
              "question": "你对密码学的理解程度如何？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0034",
              "question": "你有使用智能合约的经验吗？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0035",
              "question": "你熟悉区块链的概念吗 ？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0036",
              "question": "你玩过一些什么应用？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0037",
              "question": "你用一句话讲一下amm机制？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0038",
              "question": "你认为区块链技术的未来是什么？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0039",
              "question": "公共区块链和私有区块链有什么区别 ？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0040",
              "question": "其次，谈论您用来促进团队合作和沟通的特定技术或工具。比如用于项目管理、定期签到或每日会议的软件。分享您从过去项目中学到的任何经验教训，以及如何在以后的计划中使用这些经验？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0041",
              "question": "决项目研发期间的问题？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0042",
              "question": "区块链上的隐私层（如Zcash和Monero）是如何工作的？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0043",
              "question": "区块链如何促进版权保护和知识产权管理？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0044",
              "question": "区块链如何促进版权保护和知识产权？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0045",
              "question": "区块链技术如何促进供应链管理的透明度？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0046",
              "question": "区块链技术如何促进供应链管理的透明？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0047",
              "question": "区块链技术如何解决数字身份认证问题？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0048",
              "question": "区块链技术如何解决数字身份认证问？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0049",
              "question": "区块链技术有了新的发展，可以改进你当前的代码。您将如何决定是否值得花费时间和精力进行升级？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0050",
              "question": "区块链技术有了新的发展，可以改进你当前的代码。您将如何决定是否值得花费时间？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0051",
              "question": "区块链有哪些不同类型 ？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0052",
              "question": "去中心化是在网络中所有参与者之间分配权力和权威的过程。在web3中，去中心化的重点是将数据管理的权力从谷歌和Facebook等集中化的科技巨头手中转移出去。与web3有关的最常见的区块链问题集中在去中心化以及它如何帮助创建用户数据存储和管理的分布式模型。去中心化以可信和透明的环境以及数据调节的准确性的形式为web3提供了显著的好处？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0053",
              "question": "合约中的安全风险？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0054",
              "question": "和软件方面的经验？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0055",
              "question": "在区块链中，什么是孤块，它们如何产生？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0056",
              "question": "在区块链中，如何处理交易的隐私保护？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0057",
              "question": "在智能合约中产生随机数有哪些困难？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0058",
              "question": "在讲解这段代码如何运行之前，我们先回顾下传统java程序的运行方式？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0059",
              "question": "在面试之前，列出你是这份工作的最佳人选的原因。想想你有哪些其他候选人可能没有的技能？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0060",
              "question": "如何在区块链上实现匿名交易？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0061",
              "question": "如何在区块链项目中处理交易拥堵和高手续费？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0062",
              "question": "如何在区块链项目中实现跨链通信？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0063",
              "question": "如何在区块链项目中管理和减少智能合约中的安全风险？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0064",
              "question": "如何在区块链项目中管理和减少智能？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0065",
              "question": "如何帮助您担任以前的角色？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0066",
              "question": "如何通过区块链技术提高供应链的效率和透明度？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0067",
              "question": "如何通过区块链技术提高供应链的效？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0068",
              "question": "如果您在区块链代码中发现安全漏洞，您会怎么做？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0069",
              "question": "如果您有使用智能合约的经验，请描述您使用它们解决问题或成功完成项目的时间？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0070",
              "question": "如果要您从头开始创建区块链，您会做的第一件事是什么？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0071",
              "question": "当前的代码。您将如何决定是否值得花？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0072",
              "question": "怎样理解Web3？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0073",
              "question": "性等挑战， 对此您有哪些看法或解决方？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0074",
              "question": "您可以利用这个机会 解释您对区块链开发所使用的特定语言或软件的任何经验， 以及该经验？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0075",
              "question": "您可以利用这个机会解释您对区块链开发所使用的特定语言或软件的任何经验，以及该经验如何帮助您担任以前的角色？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0076",
              "question": "您可以通过定义不同类型的区块链以及它们之间的区别来回答这个问题？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0077",
              "question": "您多久更新一次对区块链技术的了解？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0078",
              "question": "您如何描述区块链网络？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "web3_0079",
              "question": "您将允许用户提交哪些类型的数据？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0080",
              "question": "您应该通过描述您允许用户 提交哪些类型的数据 、为什么允许该类型的数据 以及您对该数据？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0081",
              "question": "您应该通过描述您允许用户提交哪些类型的数据、为什么允许该类型的数据以及您对该数据采取的任何安全措施来回答这个问题？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0082",
              "question": "您是如何应对 Web3 的安全性、 可扩展？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0083",
              "question": "您是如何应对 Web3 的安全性、可扩展性等挑战，对此您有哪些看法或解决方案？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0084",
              "question": "您认为区块链开发人员最需要了解哪些编程语言？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0085",
              "question": "我们想使用我们的区块链来收集数据，您将允许用户提交哪些类型的数据？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0086",
              "question": "描述一种用于提高区块链交易隐私的技？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0087",
              "question": "描述什么是区块链的“气体费用”，以及它的作用？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0088",
              "question": "描述加密货币钱包的工作原理及其安全性措施？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0089",
              "question": "描述加密货币钱包的工作原理及其安全？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0090",
              "question": "描述区块链上的 DAO（去中心化自治组？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0091",
              "question": "描述区块链上的DAO（去中心化自治组织）及其工作原理？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0092",
              "question": "描述区块链上的交易如何被验证？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0093",
              "question": "描述区块链上的智能合约安全最佳实践？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0094",
              "question": "描述区块链中的Merkle树及其重要性？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0095",
              "question": "描述区块链中的“永久性”是如何实现的？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0096",
              "question": "描述区块链的分片技术及其如何解决扩展性问题？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0097",
              "question": "描述区块链的分片技术及其如何解决扩？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0098",
              "question": "描述如何使用区块链技术进行身份验证和授权？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0099",
              "question": "描述如何使用区块链技术进行身份验证？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0100",
              "question": "描述您在用于区块链开发的编程语言和软件方面的经验？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0101",
              "question": "描述您在用于区块链开发的编程语言？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0102",
              "question": "教训，以及如何在以后的计划中使用这些经验？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0103",
              "question": "是什么让你很适合这个角色？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0104",
              "question": "权益证明（PoS）机制的工作原理是什么？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "web3_0105",
              "question": "比特币和以太坊有什么区别？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0106",
              "question": "比特币和以太坊的区别在哪里？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0107",
              "question": "流行的共识算法之间有什么不同？",
              "variants": [],
              "sources": [
                "questions_tech_clean_v2.json"
              ]
            },
            {
              "id": "web3_0108",
              "question": "然而，智能合约的本体是一份代码，非常容易被篡改，如何为其提供强力的存储介质就成了问题。这正好是区块链擅长解决的——通过比特币的实践，证明了区块链可以在分布式环境下让电子记录不可被篡改？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0109",
              "question": "然而，智能合约的本体是一份代码，非常容易被篡改，如何为其提供强力的存储介质就成了？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0110",
              "question": "的第一件事是什么？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0111",
              "question": "移出去。与 web3有关的最常见的区块链问题集中在去中心化以及它如何帮助创？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0112",
              "question": "织）及其工作原理？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0113",
              "question": "经验，结合具体案例进行阐释，如可以描述如何采用特定的扩展解决方案来提高？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0114",
              "question": "自的优缺点是什么？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0115",
              "question": "解释什么是“链上治理”和“链下治理”，以及它们的重要性？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0116",
              "question": "解释什么是区块链的分叉，以及它为什么会发生？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0117",
              "question": "解释什么是区块链的分叉，以及它为什？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0118",
              "question": "解释什么是工作证明（ PoW）和如何实？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0119",
              "question": "解释什么是闪电网络，以及它如何实现即时交易？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0120",
              "question": "解释什么是闪电网络，以及它如何实现？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0121",
              "question": "解释什么是非同质化代币（ NFT）及其？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0122",
              "question": "解释什么是非同质化代币（NFT）及其在区块链上的应用？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0123",
              "question": "解释区块链中的交易池是如何工作的？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0124",
              "question": "解释区块链技术如何实现跨境支付，并阐述其优势？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0125",
              "question": "解释区块链的共识机制和它为什么重要？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0126",
              "question": "解释区块链的共识算法有哪些，它们各自的优缺点是什么？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0127",
              "question": "解释区块链的共识算法有哪些，它们各？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0128",
              "question": "解释区块链的可扩展性问题及其当前解决方案？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0129",
              "question": "解释区块链的可扩展性问题及其当前？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0130",
              "question": "解释在区块链中如何实现数据的去中心化存储？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0131",
              "question": "解释在区块链中如何实现数据的去中心？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0132",
              "question": "解释硬分叉和软分叉的区别及其对网络？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0133",
              "question": "费时间和精力进行升级？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0134",
              "question": "这个问题可以帮助面试官确定你如何处理错误和挑战？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0135",
              "question": "这个问题可以帮助面试官确定你对职业的投入程度以及你是否有可能在他们的公司工作很长时间？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0136",
              "question": "这个问题可以帮助面试官评估您对区块链技术的了解以及您如何将其应用到您的工作中？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0137",
              "question": "这个问题可以让你展示你对区块链的了解以及如何使用它？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0138",
              "question": "这个问题是展示您对区块链及其工作原理的了解的机会。您可以通过 定义每种类型的区块？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0139",
              "question": "这个问题是测试你对区块链开发知识的好方法。 它还允许您向面试官展示 您将如何处理此任？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0140",
              "question": "这个问题是测试你对区块链开发知识的好方法。它还允许您向面试官展示您将如何处理此任务以及您将采取哪些步骤？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0141",
              "question": "这个问题是测试你对区块链知识以及如何在现实世界中使用它的好方法。在回答这个问题？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0142",
              "question": "这个问题是面试官评估你对区块链知识以及如何在工作中应用它的好方法？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0143",
              "question": "这个问题有助于面试官评估您对区块链技术的了解以及您如何将其应用于现实世界的情况？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0144",
              "question": "这些问题很可能跟你所面试的岗位有直接关联。如果没有直接关联，也很有可能成为面试官考察你开发能力的重要话题之一。如果您尚未遇到相关问题，可以结合当下相关技术进展谈谈你的看法。如果您曾经遇到过相关开发问题，请根据您的实践经验，结合具体案例进行阐释，如可以描述如何采用特定的扩展解决方案来提高 DApp 的性能，或者如何创建安全协议来保护基于区块链的应用程序，这可能涉及执行安全审计、遵守行业标准、对项目进行压力测试等内容？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0145",
              "question": "那为什么在这 10年中，智能合约与区块链却产生了如此紧密的关联？因为区块链可以保证？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0146",
              "question": "那为什么在这10年中，智能合约与区块链却产生了如此紧密的关联？因为区块链可以保证智能合约的不可篡改，不仅合约内容不可篡改，每次调用记录亦不可篡改？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0147",
              "question": "钱打给你，结果你毁约了，不承认怎么 办？所以智能合约一直没办？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0148",
              "question": "雇主提出这个问题是为了更多地了解您的资格以及您觉得自己如何适合他们的公司？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0149",
              "question": "首先，提及您在项目工作时遇到的困难和解决问题的方法，以及您所做的任何战术及战略。请务必强调您和您的团队如何合作克服这些障碍并产生良好的结果？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            },
            {
              "id": "web3_0150",
              "question": "（DEX） ，与传统交易所有何不同？",
              "variants": [],
              "sources": [
                "web3_questions_only_clean.json"
              ]
            }
          ]
        }
      ]
    }
  ],
  "category_counts": {
    "LLM": 2225,
    "General Tech": 1406,
    "Machine Learning": 501,
    "Deep Learning": 330,
    "Reinforcement Learning": 228,
    "Transformer": 184,
    "Web3": 150,
    "NLP": 129,
    "RAG": 95,
    "SLAM/Robotics": 80,
    "Database": 69,
    "Security": 41,
    "Python": 10,
    "Data Analysis": 5,
    "Multimodal": 5
  }
}
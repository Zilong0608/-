[
  "AI 目前已在以下哪些领域或方向得到应用？",
  "手写softmax公式，手写BN公式，softmax层的label 是什么 ？",
  "Softmax+Cross Entropy 如何反向求导？",
  "如何基于 n-gram 频率训练小型草案模型并设置接受阈值？",
  "如何撰写中国 AI 专利并加快审查？",
  "Colossal-AI 有1D/2D/2.5D/3D，是什么情况？",
  "下列哪种词嵌入支持上下文建模（Context Modeling）？",
  "下列哪种词嵌入可以自定义训练特定主题？",
  "相同的词可以通过___________来实现多个词嵌入？",
  "为什么要做softmax 标准化",
  "为什么后续有不少工作尝试对softmax 进行替换",
  "Word2vec训练流程",
  "LR 和SVM 的区别",
  "tfidf 的原理",
  "## SVM 的原理",
  "SVM 为什么采用间隔最大化？",
  "SVM 有哪些可以调节的参数？",
  "常用的SVM 核函数？",
  "svm 能介绍下么？",
  "## 从词袋模型到word2vec，以及Word2vec 参数更新，两种加速方式，为什么",
  "SVM 原理",
  "word2vec 和fastText 对比有什么区别？",
  "glove 和word2vec 对比有什么区别？",
  "## svm 经过核函数变换后，最重要的是做什么",
  "请你简单介绍一下SVM？",
  "可以简单介绍一些word2vec 吗？",
  "请简述CBOW 和FastText 之间的区别？",
  "softmax 和sigmod 区别？",
  "svm 不同核方法区别？",
  "word2vec 伪代码推倒？",
  "fasttext 和word2vec 区别",
  "softmax 是否可以作为激活函数，为什么",
  "## 说说你知道的SVM 核函数有哪些",
  "word2vec 有哪些优化方法？",
  "SVM 为什么使用拉格朗日乘子法？软间隔怎么做？",
  "## 关于word2vec 上下文词怎么表示中心词，输入是什么，上下文词权重，中心词怎么",
  "1）SVM 原理",
  "对于一个取值较多的类别变量在不能进行onehot 的情况下如何使用？",
  "推荐系统中，如何进行负采样？",
  "word2vec的原理，怎么训练的？",
  "- 2.5 基于批内负采样的对比学习方法",
  "## 基于批内负采样的对比学习方法",
  "八、softmax和交叉熵损失怎么计算，二值交叉熵呢？",
  "九、如果softmax的e次方超过float的值了怎么办？",
  "softmax和交叉熵损失怎么计算，二值交叉熵呢？",
  "如果softmax的e次方超过float的值了怎么办？",
  "MLP+softmax层 介绍？",
  "什么是n元语法？为什么要用n-gram？",
  "tf-idf高意味着什么？",
  "softmax函数是什么？",
  "softmax函数怎么求导？",
  "2 拼接 一个 softmax 层",
  "Q： 知识表示相对于one-hot表示的优势是什么？",
  "Q：word2vec与LDA模型之间的区别和联系？",
  "fastText 是什么?",
  "fastText 的结构是什么样?",
  "fastText 词内的n-gram信息 的 训练过程?",
  "fastText 词内的n-gram信息 存在问题?",
  "fastText 存在问题？",
  "为什么有 one-hot ？",
  "one-hot 是什么?",
  "one-hot 有什么特点?",
  "one-hot 存在哪些问题?",
  "什么是 TF-IDF？",
  "TF-IDF 如何评估词的重要程度？",
  "TF-IDF 的思想是什么？",
  "TF-IDF 的计算公式是什么？",
  "TF-IDF 怎么描述？",
  "TF-IDF 的应用？",
  "Wordvec 中 CBOW 指什么?",
  "Word2vec 中 霍夫曼树 是什么？",
  "Word2vec 中 为什么要使用 霍夫曼树？",
  "Word2vec 中使用 霍夫曼树 的好处？",
  "为什么 Word2vec 中会用到 负采样？",
  "Word2vec 中会用到 负采样 是什么样？",
  "Word2vec 中 负采样 的采样方式？",
  "word2vec和tf-idf 在相似度计算时的区别？",
  "word2vec训练trick，window设置多大？",
  "【演变史】one-hot 存在问题?",
  "【演变史】fastText 存在问题?",
  "【对比】word2vec 为什么解决不了多义词问题？",
  "LR/SVM/softmax/Adaboost 损失函数之间的差别？",
  "SVM 的原理是什么？",
  "43LR和SVM 区别",
  "为什么SVM 要引入核函数？",
  "为什么SVM 对缺失数据敏感？",
  "## SVM 核函数之间的区别",
  "为什么要将求解SVM 的原始问题转换为对偶问题？",
  "GBDT 对标量特征要不要one-hot 编码？",
  "sigmoid 和softmax 的区别？softmax 的公式？",
  "改进的softmax 损失函数有哪些？",
  "sigmoid和softmax的区别？softmax的公式？",
  "改进的softmax损失函数有哪些？",
  "^(t+1)=softmax(–U–K*m^(t))",
  "SVM的原理是什么？",
  "为什么 SVM 要引入核函数？",
  "为什么SVM对缺失数据敏感？",
  "为什么要将求解 SVM 的原始问题转换为对偶问题？",
  "GBDT对标量特征要不要one-hot编码？"
]